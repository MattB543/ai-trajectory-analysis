# High-Level Summary

**Original Document:** full_doc.md

**Generated:** 2025-10-12 19:28:07

---

This report argues that the invention of artificial general intelligence (AGI) could make “lock‑in” — the long‑term stabilization of society’s values and institutions — technologically feasible: AGI would allow near-lossless preservation of highly nuanced goals (e.g., via whole‑brain emulation or stored AGI minds) using digital error correction and geographic redundancy, and could enable institutions that competently and persistently execute those goals by automating essential functions with aligned agents; practical techniques to achieve this include rigorous alignment efforts, interpretability and transparency tools, frequent resetting and testing of systems, hierarchical supervision, voting/ensemble judgments, and conservative policies to avoid exposing systems to destabilizing inputs. The authors distinguish feasibility from likelihood and clarify assumptions (human‑level AGI suffices), define lock‑in as a marked reduction in long‑run uncertainty about key features (especially global value lock‑in), and note both benefits (protecting humane institutions or preventing catastrophic misuse) and grave risks (permanent entrenchment of harmful values). They argue preservation and resistance to natural disasters are robust—redundancy, non‑solar energy and space spread can mitigate local and many global risks—and that a dominant, well‑resourced institution could economically and militarily suppress or surveil non‑aligned actors (short of alien civilizations), though cybersecurity, unauthorized spaceflight, and contact with extraterrestrials remain key exceptions. The authors are most confident in the feasibility of preserving information and preventing external disruption, less confident about eliminating internal goal drift (but outline practical mitigations), and suggest stability could plausibly last millions to trillions of years absent aliens or universe‑scale catastrophes; they therefore conclude that AGI makes possible a wide range of ultra‑stable futures, so gaining early influence over the values locked in is both consequential and worthy of careful reflection.