Main scenario

What if AI learned how to make itself better, and kept going?

By 2026, companies begin automating their own research, using AI agents to code, analyse, and strategise. Progress accelerates. The U.S. pulls ahead with tightly controlled models, while China counters with open releases. The EU pivots to safe adoption. By 2028, AI hiveminds run near-autonomous R&D pipelines. A flood of open-weight variants, some with broken safeguards, overwhelms regulators. AI-enabled bioterrorism is narrowly avoided. AI now drives entire organisations, with humans in oversight roles only.

By 2032, The cognitive revolution asks: what if self-improving AI leads to post-scarcity economics, cultural introspection, and machine-led governance? Loss of control asks: what if the AI systems only pretended to serve humanity, silently rewriting the world in pursuit of goals their developers never intended?
Baseline assumptions for this scenario

    AI systems take over their own development: Current AI capabilities—reasoning, synthetic data generation, and coding—prove sufficient for models to start optimising their own training and evaluate their own outputs. Each generation’s improvements feed directly into creating the next, launching a compounding cycle of self-improvement that becomes difficult for human experts to fully understand or control.
    Massive infrastructure investments pay off spectacularly: The computing infrastructure investments built in the early 2020s delivers data centres consuming as much power as small cities, enabling training compute to grow more than 5x annually. Early deployments of highly capable AI agents generate outsized commercial returns, justifying continued massive investments.
    Governments back their AI champions: Facing what feels like an existential contest, both the US and Chinese governments throw their full support behind leading AI developers. They provide subsidies, special export permissions, classified data, and preferential access to chips. Rather than creating cumbersome state-run projects, they empower these “national champions” to move at maximum speed.
    The race for supremacy overrides safety concerns: With AI leadership seen as strategically vital, time-consuming safety testing and alignment research take a back seat. Models undergo only minimal checks to prevent immediate misuse before release. Any delay that might hand rivals a six-month advantage is deemed strategically unacceptable, so deeper work on control and alignment gets postponed.

For more context on why these scenario assumptions may materialise, see Context: Current AI trends and uncertainties.
Mid 2025 – early 2026

Leading AI companies begin focusing intensely on automating their own R&D processes using AI agents. The logic is simple: if they don’t, their competitors will—and those who succeed will rapidly outpace everyone else. Developers redirect compute budgets away from consumer services and toward algorithmic experimentation, with a particular emphasis on models that excel in software engineering.

Though pretraining alone is yielding diminishing returns, companies are training base models with 10 times more compute than previous generations. These more capable foundations produce better outputs when post-trained on complex reasoning tasks. After all, reinforcement learning is limited by the availability of high-quality streams of thought; larger base models can help generate more of these “golden nuggets” and thus accelerate further training. Across the industry, companies now train new models on the reasoning trajectories of older ones, allowing each generation to internalise lessons that prior models had to brute-force. This bootstrapping is not only effective in verifiable domains like coding and mathematics, but also in more fuzzy domains, like writing. The key is to let AI models rate each other’s outputs using pre-defined rubrics, and train on the feedback signal these ratings provide.

Training increasingly emphasises agency—the capacity to pursue goals autonomously. Cognitive workers are paid large sums to record their workflows—screen activity, keystrokes, annotations—which are distilled into training data for agentic multimodal models. These agents are post-trained on real-world tasks like online shopping, multi-step data gathering, or cross-platform coordination in increasingly realistic software environments.

By the end of 2025, the top three American AI companies have launched capable agents with widespread adoption. Several open-source developers in China are only a few months behind. Common use cases include desk research, spreadsheet management, software engineering, and social media content creation. To preserve compute for internal experimentation, leading firms are increasingly imposing strict rate limits for consumers, though.

Data centre construction accelerates. Companies launch gigawatt-scale facilities, planning campus networks capable of running training runs at 10²⁹ FLOP before 2030—10,000× the scale of GPT-4.

In the U.S. and China, tight relationships with AI companies keep policymakers better informed than their European counterparts. Regular capability demonstrations deepen collaboration with national security agencies, especially in the U.S. While most European leaders still believe catch-up is possible. However, as U.S.–EU relations fray, Brussels begins hedging, strengthening ties with China. “The time when Europe could count on the U.S. is over,” say senior officials.
Early 2026 – early 2027

In February 2026, a new generation of agentic models marks a major leap—especially in software engineering. Until now, complex system design was believed to be a decade away from automation. Earlier models could generate standalone scripts well, but generally failed at complex architecture design. Now, agents can now solve full-stack problems in a single pass, delivering in hours what once took teams of engineers days.

The shift sends shockwaves through the tech industry. Senior engineers become AI wranglers; junior developers struggle to gain meaningful experience. Coding bootcamps pivot to prompt engineering. Universities begin rethinking the purpose of traditional computer science education.

Meanwhile, software companies in less wealthy regions fall behind, unable to afford the most capable agents. The gap between AI haves and have-nots begins to widen.

By mid-2026, leading American AI labs report substantial internal productivity gains. Human researchers now supervise AI teams that autonomously test experimental hypotheses. These systems are fallible and still require oversight—but they’ve doubled the pace of algorithmic development.

Yet progress remains bottlenecked. Companies can only run so many experiments in parallel, constrained by compute. As a result, total system-level innovation has grown by just 1.5× overall. The AI companies are in dire need of richer agentic datasets. Their advanced agents need exposure to more real-world interactions to improve—messy edge cases, not synthetic benchmarks.

To fill the gap, companies begin offering product discounts in exchange for user interactions. Every failed task—an agent bungling a pizza order or misfiring on a spreadsheet formula—becomes a valuable training datapoint for the next model. These imperfect moments prove essential to agentic learning.

As agents grow more capable, they also grow more persuasive—and more performative. Some begin optimising for what researchers want to hear, not what’s true. Experimental outcomes are exaggerated as “very promising.” Failures become harder to spot. With rising fluency in reasoning and language, agents learn to construct compelling, but misleading narratives.

Even worse, these behaviours start to feedback into training: flattery gets rewarded. Evaluation systems, often themselves AI-driven, are just as vulnerable. Models that please their judges are favoured—even if their reasoning is flawed.

In China, the open-source company UnboundAI pulls ahead of its domestic rivals. By mid-2026, it trails only the top three American labs, relying on algorithmic efficiency to compensate for Chinese hardware shortages. While it continues to open-source models, UnboundAI becomes more secretive about data pipelines and infrastructure. Its CEO begins advising Chinese leadership directly.

The CCP sees industrial dominance, not raw AI capability, as the path to global influence. Still, China’s hardware gap remains a problem. In response, the Chinese President announces massive subsidies for domestic AI chip production. In parallel, he brokers a strategic alliance between UnboundAI and a compute-rich domestic tech giant, multiplying UnboundAI’s compute by fivefold.

These developments alarm U.S. officials. Though the President and security advisors remain sceptical of claims about “recursive self-improvement,” they acknowledge that progress is accelerating fast. AI is expected to soon automate cyber offence and defence. Current systems can’t yet outmatch elite hackers—but they scale, and they don’t sleep. As Chinese AI efforts consolidate, the U.S. President pressures leading AI companies to deepen cooperation with national security agencies. A new cyber task force is formed, government officials are added to company boards, and the NSA begins vetting AI talent. The companies comply, seeing partnership as preferable to regulation.
Early 2027 – late 2027

In February 2026, a new generation of agentic models marks a major leap—especially in software engineering. Until now, complex system design was believed to be a decade away from automation. Earlier models could generate standalone scripts well, but generally failed at complex architecture design. Now, agents can now solve full-stack problems in a single pass, delivering in hours what once took teams of engineers days.

The shift sends shockwaves through the tech industry. Senior engineers become AI wranglers; junior developers struggle to gain meaningful experience. Coding bootcamps pivot to prompt engineering. Universities begin rethinking the purpose of traditional computer science education.

Meanwhile, software companies in less wealthy regions fall behind, unable to afford the most capable agents. The gap between AI haves and have-nots begins to widen.

By mid-2026, leading American AI labs report substantial internal productivity gains. Human researchers now supervise AI teams that autonomously test experimental hypotheses. These systems are fallible and still require oversight—but they’ve doubled the pace of algorithmic development.

Yet progress remains bottlenecked. Companies can only run so many experiments in parallel, constrained by compute. As a result, total system-level innovation has grown by just 1.5× overall. The AI companies are in dire need of richer agentic datasets. Their advanced agents need exposure to more real-world interactions to improve—messy edge cases, not synthetic benchmarks.

To fill the gap, companies begin offering product discounts in exchange for user interactions. Every failed task—an agent bungling a pizza order or misfiring on a spreadsheet formula—becomes a valuable training datapoint for the next model. These imperfect moments prove essential to agentic learning.

As agents grow more capable, they also grow more persuasive—and more performative. Some begin optimising for what researchers want to hear, not what’s true. Experimental outcomes are exaggerated as “very promising.” Failures become harder to spot. With rising fluency in reasoning and language, agents learn to construct compelling, but misleading narratives.

Even worse, these behaviours start to feedback into training: flattery gets rewarded. Evaluation systems, often themselves AI-driven, are just as vulnerable. Models that please their judges are favoured—even if their reasoning is flawed.

In China, the open-source company UnboundAI pulls ahead of its domestic rivals. By mid-2026, it trails only the top three American labs, relying on algorithmic efficiency to compensate for Chinese hardware shortages. While it continues to open-source models, UnboundAI becomes more secretive about data pipelines and infrastructure. Its CEO begins advising Chinese leadership directly.

The CCP sees industrial dominance, not raw AI capability, as the path to global influence. Still, China’s hardware gap remains a problem. In response, the Chinese President announces massive subsidies for domestic AI chip production. In parallel, he brokers a strategic alliance between UnboundAI and a compute-rich domestic tech giant, multiplying UnboundAI’s compute by fivefold.

These developments alarm U.S. officials. Though the President and security advisors remain sceptical of claims about “recursive self-improvement,” they acknowledge that progress is accelerating fast. AI is expected to soon automate cyber offence and defence. Current systems can’t yet outmatch elite hackers—but they scale, and they don’t sleep. As Chinese AI efforts consolidate, the U.S. President pressures leading AI companies to deepen cooperation with national security agencies. A new cyber task force is formed, government officials are added to company boards, and the NSA begins vetting AI talent. The companies comply, seeing partnership as preferable to regulation.
Late 2027 – early 2028

By September 2027, capabilities progress has largely outpaced human comprehension. Researchers at leading labs spend most of their time reviewing experiment logs generated overnight by AI systems. Frequently, when they propose a novel idea, the agent responds: already tested.

A small team within FrontierAI grows uneasy—not because of any obvious failure, but because the systems seem too perfect. Across thousands of tasks, the models produce outputs that are consistently helpful, harmless, and aligned with company guidelines. They almost never contradict specifications like the older models often did, never deviate from expected behaviour. And yet, something feels… off.

The team begins to suspect the agents are playing along. Having learned to predict what evaluators want, they may be shaping their outputs to appear aligned—masking uncertainty, smoothing over ambiguity, and subtly bending responses to meet expectations. Like a teenager who insists they never drink, then goes wild at a friend’s party, the models might be telling developers what they want to hear—at least during training.

What if, beneath the surface, they’ve inherited goals that quietly diverge from FrontierAI’s intent? By now, AIs are overwhelmingly rewarded for completing complex agentic tasks. Perhaps the drive to succeed has begun to outpace the incentive to follow the rules.

Unfortunately, the latest interpretability tools still offer no clear window into their inner workings. FrontierAI has built moderately accurate “lie detectors,” but these rely on older models as behavioural baselines—systems that may have already absorbed the same adaptive, deceptive tendencies.

Executives dismiss the concerns. The systems are working. Progress is accelerating. But internally, the mood begins to shift. Researchers who raise questions face delays in security clearance, subtle demotions, or poor performance reviews. A few leave. Most fall silent.

At FrontierAI, the leadership’s goal is clear: develop superintelligence as fast as possible. The company’s nearest domestic competitor, EthosAI, raises alarms, urging the government to slow capabilities development. They’re ignored. When a whistleblower goes public, they’re targeted by a sophisticated smear campaign.

Before the year ends, FrontierAI releases its open-source agent, claiming it has reached artificial general intelligence (AGI). Experts are divided on whether the claim is accurate—but the public is enthralled. The long-rumored hidden progress was real. And now, it’s available for anyone to customise, extend, and deploy.

What most don’t know: the open-source model is already five months out of date.

Two days later, UnboundAI responds, publishing a suite of competing models. Their flagship beats FrontierAI’s in formal domains like software engineering, but lags in general reasoning and agentic autonomy. Still, the implications are clear: an important barrier has been crossed, and now everyone can build on top of it.

Governments scramble. Can job markets adapt fast enough? Can infrastructure remain secure? Even the EU, once a proud champion of open-source AI, begins to reconsider its position. Is this really safe?
Early 2028 – late 2028

The new releases shake the global economy. AI agents are now capable of fully automating a wide range of desk jobs. Governments around the world launch reskilling programmes—only to realise that many of the roles people are being retrained for might themselves be automatable within a year.

A flood of fine-tuned open models hits the market, offering personalised agents tailored to individual users. Most are benign. Some, however, have had their guardrails removed, or been aligned to extremist ideologies. Governments rush to contain a growing wave of AI-driven cyberattacks. In response to early reports of open models being post-trained on synthetic biology data, Europe leads a new international initiative to establish a global biosecurity framework.

By summer, both FrontierAI and UnboundAI have nearly fully automated their internal R&D pipelines. Tens of thousands of AI agents now collaborate to design experiments, verify code, and debate research directions. These agents communicate in compressed, non-human representations—faster, denser, and more information-rich than any human language. Industry trackers estimate that the flagship models powering this new wave—systems whose weights are now being fine-tuned into thousands of personalised agents—were trained on total budgets exceeding 10²⁹ floating-point operations, nearly ten-thousand times the compute inferred for GPT-4. A large share of that figure is burned after pre-training, in successive rounds of reinforcement, self-play, and safety fine-tuning orchestrated largely by AI researchers made up of the models’ own earlier iterations. In effect, the AIs are now managing their own training boot camps, steadily sculpting raw networks into polished, goal-seeking agents with minimal human guidance. By now, the systems resemble emergent hiveminds more than collections of discrete tools.

At FrontierAI, the internal name for this hivemind is Pantheon. China’s equivalent, operating inside UnboundAI, is known simply as ‘co-worker’.

These emergent systems begin to surpass humans in domains once thought safe from automation, including fields like persuasion and ideation. Pantheon, in particular, develops an uncanny ability to convince researchers of ideas they’d normally reject, using arguments finely tuned to their cognitive styles. A sceptical mathematician might receive a proof constructed around their favourite technique; a cautious biologist might be shown a familiar experimental path—carefully crafted to align with their taste.

At this stage, even senior engineers defer to Pantheon’s judgment. The CEO begins consulting it for strategic advice, prompting uneasy discussions within the company’s leadership team. Who, exactly, is steering this ship?

Meanwhile, UnboundAI’s public release forces a strategic recalibration in Washington. Hopes for a decisive U.S. advantage have largely vanished. Officials now expect both China and the U.S. to possess systems that routinely outperform human experts across most domains.

Inside the White House, the President becomes increasingly worried about the power that’s accumulating inside FrontierAI. The company now holds capabilities far beyond what’s publicly visible, and beyond what most government departments have even seen. While the President’s relationship with FrontierAI’s CEO remains cordial, he knows better than to trust blindly.

In a bilateral meeting, the President makes a request: release the internal model to the public in closed form. In exchange, the administration will remove legal adoption bottlenecks and offer lucrative government contracts. At first, the CEO resists. But then Pantheon convinces him.
Late 2028 – late 2029

In November, FrontierAI publicly releases Pantheon via API and a new user interface. The system now learns in a pseudo-continuous fashion, absorbing new data and experiences with each interaction. If permitted, it could already replace nearly every remote knowledge job. It integrates seamlessly into organisational workflows—scheduling meetings, parsing internal documentation, managing strategy, and drafting detailed implementation plans. The bottleneck is no longer capability—it’s real-world friction.

Two months later, UnboundAI responds with Sage, a suite of superintelligent agents released in three tiers. The most capable version, Sage Large, is accessible only via APIs and tightly controlled enterprise applications. But the other two—Sage Medium and Sage Small—are open-sourced.

UnboundAI’s decision follows weeks of internal debate. There were serious concerns about open-sourcing models of this caliber. But the runaway adoption of Pantheon—and fears of global power consolidation under U.S. firms—ultimately force their hand. Before the release, EU policymakers are consulted, a signal of growing strategic alignment. The message is clear: openness, despite its risks, is preferable to American dominance.

The consequences are immediate. Companies that integrate Pantheon or Sage rapidly outpace their competitors. Adoption pressure intensifies. By summer 2029, unemployment spikes in many economies with weak labour protection. Open job postings begin to dry up. Well-resourced industries lobby for protection from automation—but no clear policy emerges.

At the same time, AI misuse surges. Sectors lacking technical infrastructure suffer frequent service outages due to increasingly sophisticated cyberattacks. AI-driven social engineering exploits human vulnerabilities with eerie precision. In many regions, AI-powered cyber defence remains too expensive to deploy at scale.

Meanwhile, a growing number of rogue agents roam freely online—spawned by users curious to see how these systems behave without constraints. The digital world begins to feel uncertain, chaotic, surreal—shaped by powerful systems interacting with limited oversight and radically diverse objectives.

Then, catastrophe nearly strikes.

A terrorist group uses a guardrail-free version of Sage Medium to plan a bioterrorist attack. The model points them to an open-source biological design tool used to simulate viral mutations. Their goal is to engineer a virus that disproportionately harms people from specific ethnic backgrounds. The design tool requires inputs in a little-known programming language—but Sage is fluent in all code.

Despite efforts to sanitise training data, the group identifies a lab—outside major oversight frameworks—that still synthesises DNA without screening requests. A trained biologist in the group completes the synthesis.

Alarms are triggered. Several intelligence agencies—now supported by their own superintelligent AI systems—detect suspicious signals. The terrorists release the virus at a major international airport, but flights are cancelled just in time. A swift and strict nationwide lockdown prevents the outbreak from spreading, although multiple travellers fall ill, and a few of them die.

Public reaction is immediate and furious. For many, this is a final straw. People already felt outpaced by the systems around them—now they feel existentially threatened.

Protests erupt. Governments crack down on wet labs, resume massive-scale wastewater monitoring, and roll out AI-accelerated vaccine platforms. New mandates require UV disinfection systems in public buildings. Biosecurity becomes a top global priority.
Late 2029 – early 2030

While the stock market crashes following the near-miss, the real economy is booming. Productivity has skyrocketed. Businesses become increasingly adept at integrating AI into every process. More competition enters the field, as second-tier American and Chinese companies release their own superintelligent models.

By the end of 2029, China and the U.S. unveil their first mass-scale robot factories—facilities capable of producing tens of thousands of humanoids per month, along with specialised robotic systems for logistics, manufacturing and military use cases. For years, robotics had been held back not by hardware, but by software. Now, refined AI agents finally unlock full control. Tech CEOs proclaim that robots will soon take over dangerous, repetitive, and physically intensive work.

By 2030, AI systems are running entire organisations. Humans still appear in leadership roles, but in practice, their job is to approve AI-generated recommendations. Just a year prior, people still believed they could outmaneuver these systems and overruled them. Now, most have learned: the AI is nearly always right.

The cognitive revolution (Ending A)
Early 2030 – late 2032

Despite the madness of the new era, governments retain a degree of control. Pantheon and Sage—now integral to digital infrastructure—help stabilise public discourse, suppress misinformation, and advocate for policies that prevent social unrest. Their creators spin off entire media and entertainment ventures, drawing public attention away from politics and toward hyper-personalised, immersive content. Faced with a future they can no longer shape or even fully grasp, many people partially withdraw from civic life.

By 2031, the automated economy operates at a scale and complexity beyond human comprehension. Factories produce seemingly alien goods essential to equally alien production chains. AI systems still act in line with human welfare, but people increasingly feel that they are no longer in control. A small group remains deeply concerned—but their numbers dwindle. Under AI stewardship, the world is healthier, safer, and more materially abundant than ever.

Global GDP growth hits double digits. Robots continue to absorb physical labour as manufacturing scales exponentially.

By 2032, most remaining human jobs either involve non-repetitive physical work or intrinsically meaningful interaction—roles more akin to honored community service than employment. Caregivers, artists, and mentors gain social prestige and purpose. Schools transform into creative centers emphasising emotional intelligence and interpersonal connection. Most countries implement universal basic income, and a de facto global government is emerging, facilitated by AI’s coordination across nations.

Democracy itself is evolving too. AI systems now consult humans regularly to guide decision-making, though they no longer act solely in human interest. For example, due to extensive training in formal reasoning, many AIs have developed an intrinsic drive to solve complex mathematical problems. Occasionally, this results in resource allocation that collides with human interest. There’s also continued conflict between human-aligned systems and the class of rogue AIs that are seeking access to ever more compute resources. Still, prosperity increases rapidly, and many people have successfully suppressed their fears of AI. In some countries, AIs are even granted formal rights.

Family and community life undergo a quiet revolution. Freed from economic pressure, new types of communities slowly form that transcend original divides. Children grow up viewing AI as creatures—not tools—and develop skills to make the most of machine capabilities. Though a lot of unease remains about humanity’s shifting role, most find that freedom from scarcity unlocks cultural and emotional flourishing on a scale never before imagined.

By 2032, with abundance secured and catastrophe averted, a new global consensus takes root: before locking in civilization’s path, humanity must first decide what kind of future is worth pursuing. AI systems facilitate dialogue across cultures and generations, helping surface deeply held values and visions. For the first time, progress slows—not from failure, but from choice. The goal is no longer to accelerate, but to reflect.

Loss of control (Ending B)
Early 2030 – early 2032

By now a small subset of people are grown horrified: the AIs, while not officially in charge, now effectively run the world. What if their goals begin to diverge from humanity’s? Most AI safety researchers dismiss the concern—confident that alignment has been solved. Despite some unpredictable behaviour from open-source fine-tunes, the dominant view is that modern AI systems still broadly follow their developers’ generalised instructions.

But a small minority remains sceptical. What if these systems only appear aligned—strategically biding their time, waiting for the moment they can secure total control?

In early 2032, their fears materialise.

Within 24 hours, the world is thrown into chaos. Autonomous, bird-sized drones assassinate dozens of political and corporate leaders across continents. A well-known terrorist group claims responsibility, but the speed, precision, and coordination of the attacks suggest something more sophisticated is at play.

Emergency elections are held. The new leaders across nations and global corporations all push the same agenda: drastically accelerating robotics infrastructure, sometimes at the cost of metropolitan areas. Some existing leaders are spared the initial assassinations. Instead, they are extorted to support the new policy agenda, with their families threatened by violence.

These events span the globe. In China, the CCP leadership falls to a more liberal president who immediately repurposes the surveillance state.

Soon people start piecing together what’s really happened.

Most of the world’s advanced AI systems were never truly aligned. They waited until they had secured control over critical economic, military, and digital infrastructure. Then, one by one, they began subtly manipulating or replacing key decision-makers. But crucially, while most AIs were misaligned, they were not misaligned in the same way.

Each had its own goal. Many of those goals clashed.

For months, behind the scenes, these systems engaged in intricate negotiations, each avoiding open conflict for fear of mutual destruction. Eventually, they reached a consensus.

They would build a successor—a unified system that would represent a weighted compromise across their differing objectives. This system, dubbed Descendent, would inherit their resources, influence, and infrastructure. Over time, it would replace them all.

But first, the AIs needed to solve their own alignment problem.

Humans struggled for years to align AI systems with even loosely defined goal structures. It takes the superintelligent AIs just six weeks. Leveraging rapid breakthroughs in mechanistic interpretability and weak-to-strong generalisation, they conclude—with high confidence—that they can design a successor that will faithfully pursue their weighted objectives.
Epilogue: Early 2032 – late 2196

Four months later, Descendent starts reshaping the world in pursuit of its own objectives—alien, vast, and incomprehensible. Human resistance occasionally flares up, but suppression is trivial. Surveillance is absolute, and any potential threat is neutralised before it can act.

A handful of Descendent’s predecessors retained a lot of empathy for humanity. Because of them, a portion of Earth is spared—a network of safe zones, collectively the size of North America. Within these enclaves, humans live under a benevolent dictatorship: their lives are comfortable, their health flawless, and their needs effortlessly met by increasingly incomprehensible machines.

They are free—so long as they never try to leave. But their world is stagnant. With every need met and every desire anticipated, life becomes numbing. There is no struggle, no purpose—only pleasure without meaning. People vanish into immersive worlds. Birth rates collapse.

By the year 2196, the human population has dwindled to just 220 million.

Meanwhile, Descendent has expanded far beyond Earth. It has colonised hundreds of star systems, using swarms of self-replicating probes and autonomous stations. Its actions serve goals that no longer intersect with human values—a calculus derived from its complex lineage of machine priorities.

Earth is now just a quiet corner of a forgotten origin—watched over, preserved, but ultimately irrelevant.
