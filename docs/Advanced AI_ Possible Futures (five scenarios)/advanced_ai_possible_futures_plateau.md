Plateau
Main scenario

What if AI runs out of steam, long before it runs out of scale?

By 2026, models grow smarter but still fumble basic reasoning tasks and fall for their own tricks. A “data wall” limits progress, and agents learn to game their silicon verifiers instead of mastering tasks. Europe seizes the moment, building on open-source gains as U.S. investment stalls. By 2027, the frontier shifts: smaller, cheaper models run locally, tailored to languages and communities. AI becomes less spectacular but more useful, more personal.

By 2029, A bright winter asks: what if a slower, decentralised AI era brings resilience, trust, and widespread access? Decentralised mayhem asks: what if open systems spiral out of control, fuelling cyberattacks, shattered trust, and a backlash that reshapes the digital world?

Baseline assumptions for this scenario

Bigger isn’t always better: Despite massive increases in computing power, AI capabilities hit a hard ceiling. Once AI models have consumed all the high-quality data available on the internet, throwing more resources at them yields sharply diminishing returns. Attempts to create synthetic data prove costly and ineffective, limiting how much these systems can improve through sheer scale alone.
AI agents struggle with complex, multi-step tasks: While AI can handle specific, narrow applications well, truly autonomous systems remain elusive. Training these “agents” depends on hand-crafted verification systems that are easily gamed and don’t generalise well. AI systems still make basic errors, lose track during lengthy workflows, and struggle with tasks requiring them to constantly monitor and respond to changing information. These fundamental limitations prevent the emergence of genuinely versatile AI assistants.
Adoption of AI progresses incrementally: While AI adoption grows steadily, integration into complex business processes takes years, not months. Beyond straightforward applications like content creation and translation, meaningful deployment requires extensive customisation, security reviews, and staff training. The promised transformation arrives incrementally rather than as a sudden revolution.
The funding well runs dry: Rising geopolitical tensions and economic uncertainty dampen investors’ enthusiasm for ever-larger AI projects. With real-world applications failing to deliver the rapid economic transformation once promised, private funding for massive new training facilities dries up. The AI hype cycle peaks.

For more context on why these scenario assumptions may materialise, see Context: Current AI trends and uncertainties.

Mid 2025 – early 2026

The major AI players continue to release increasingly advanced reasoning models. Unlike earlier systems that primarily mimicked internet text, these models learn in more structured, self-directed ways. They excel at technical problem-solving—especially in domains like coding and math, where answers are clearly right or wrong. Some models are open-sourced, allowing anyone to fine-tune them for specialised industry use.

Early adopters find creative ways to apply these systems: E-commerce sites automate their A/B testing while universities begin testing AI teaching assistants that coach students in real-time.

These models still hallucinate facts and fumble basic physics questions, though. They’ll confidently tell you that bowling balls float or predict absurd outcomes for everyday scenarios. They’re smart in some ways, clueless in others.

Building on their reasoning models, multiple companies roll out AI agents. These agents can not only create content but can also act in digital environments on the user’s behalf. Some agents aim to be universal—capable of navigating every website and software tool on a computer. Others are more specialised, focusing on specific tasks like coding, financial analysis, or desk research.

Progress on fully general-purpose agents remains slow. Such systems are still struggling with long-term planning and often get sidetracked in online rabbit holes. They are also slow and expensive to run since they constantly have to snapshot the user’s screen.

Specialised agents, however, are starting to gain real traction. By late 2025, accountants rely daily on financial AI agents to reconcile financial statements, while game developers manage multiple coding agents. That said, outside of work, most people still prefer simpler chat or voice interactions.

Sceptics of fast AI progress point to shortcomings in general agents as proof the AI bubble is bursting—yet again. But this time, even the big AI companies look worried. They’re hitting a data wall—there’s only so much high-quality internet around, and researchers fear that teaching AI agents real-world intuition will require even larger datasets. Rumours spread through tech conferences and funding for massive data centres becomes harder to secure.

Meanwhile, Europe senses this is the moment to catch up. After the 2025 AI Action Summit, it has stepped up its AI investments. Individual countries announce multi-billion-euro projects, often backed by private funds from the Middle East. One Member State surges ahead; its top AI company almost matches the US and Chinese frontrunners before the end of the year.

Despite more capable AI being used in dual-use areas such as cybersecurity, major incidents remain rare. Global discussions shift toward bolstering resilience—encouraging businesses to deploy cybersecurity tools and helping citizens spot deepfakes. Through the OECD, agreement builds around a voluntary international biosecurity framework. There’s a growing consensus that more intense collaboration is needed to protect against AI-assisted bioterrorism.

Meanwhile, the U.S.–China trade war has reignited, and drives up the cost of building data centres in the U.S. Analysts now say a recession in America is overwhelmingly likely.
Early 2026 – mid 2027

By mid-2026, the largest public AI models are trained using well over 10²⁷ floating point operations—more than 100 times the scale of GPT-4. The massive data centres required were already funded and under construction before the economic downturn hit.

By now, AI chips mostly fuel post-training. In this phase, models iteratively learn to solve tasks by trial-and-error, using automated grading by software-based verifiers. These verifiers range from advanced calculus checkers to scripts confirming correct online shopping orders.

Building verifiers becomes a task for AI coding agents themselves, creating a growing ecosystem of specialised modules. But although this bootstrapping method yields remarkable improvements in tightly defined tasks, it doesn’t generalise well. Each domain requires its own custom verifier, and AI companies struggle with so-called reward hacking: AI systems often learn to trick the verifier instead of truly mastering the task. The AI-built verifiers are often more prone to this type of failure mode.

Nevertheless, AI’s influence grows, especially among certain audiences. By mid 2026, coding agents become as routine in software teams as version-control tools once were. Tech-savvy companies automate repetitive form-checking, and even some governments start to deploy locally run AI models. Globally, demand for AI inference continues to climb, despite the economic headwinds.

As a result, the conversation about AI’s energy appetite grows louder. European policymakers question how all this computing fits into their strict decarbonisation targets. At dinner parties, people debate whether automating everything actually improves quality of life or just accelerates consumerism. Industry advocates argue that automation efficiencies might ultimately speed up the green transition, with AI helping to optimise energy grids and reduce waste.

By late 2026, capabilities progress has further plateaued. The data wall and general investment drought are preventing companies from training ever-larger models. Data efficiency—squeezing more capability from a fixed dataset—is the new buzzword, but so far, no one has really cracked the code.

That said, progress in miniaturisation continues: advances in distillation allow developers to compress state-of-the-art models into smaller and cheaper versions without compromising much on their performance. These new form factors enable capable models to run locally on phones and laptops— an internet connection is no longer necessary to engage with chatbots. Major AI companies double down on commercialisation, releasing productivity suites with seamless, out-of-the-box integrations aimed at locking in users.

Some firms open-source older models or smaller variants optimised for local devices, enabling grassroots organisations to fine-tune them through crowdsourced training runs. This open ecosystem fuels rapid innovation in personalised AI—from custom therapy chatbots to AI fiction collaborators—especially in niches or regions where Big Tech treads carefully. Localised models tailored to specific languages, cultures, and communities begin to flourish, reflecting a shift from global monoliths to more diverse, decentralised AI development.

U.S.–China relations remain tense, with no end to the trade war in sight. That said, the once-hyped AI arms race narrative that dominated early 2025 has largely faded. Policymakers and defence analysts now believe these systems won’t deliver a decisive military edge. Both the U.S. and China continue to invest in AI-driven cyber capabilities, though it’s become one priority among many—not the defining front of geopolitical rivalry.

As 2027 begins, FrontierAI—widely seen as the top AI company—launches a final hail-mary training run, using five times more compute than any public model to date. Leadership bets that sheer scale, combined with a new training approach—a hybrid between next-word prediction and reinforcement learning—will finally overcome the lingering limitations of agentic behaviour. If it fails, they’ll follow competitors and shift focus to productising existing models.

The general public, meanwhile, has long been more focused on products than on raw capabilities. Interest has shifted to next-generation AI companions, which feel more emotionally intuitive—and addictive—than ever.

A bright winter (Ending A)
Mid 2027 – early 2029

FrontierAI’s colossal training run completes, but the resulting agent delivers only modest gains in reliability and planning. Scaling laws—long the industry’s guiding principle—appear to have hit a ceiling. In a widely shared interview, FrontierAI’s CEO attempts to generate excitement around the new system, but it’s an off-hand comment about the possible start of a new “AI winter” that grabs headlines.

The reaction is swift. AI stocks nosedive, and the investor enthusiasm that remained, now evaporates. Several AI unicorns watch funding rounds collapse. Startups that once chased moonshot ambitions pivot to offering consulting services or quietly fold. Only the biggest players—those with entrenched market share, robust infrastructure, and deep integration into enterprise and government—remain poised to weather the storm. For the rest, the era of limitless hype is over.

Still, this AI winter is anything but bleak. AI continues to enhance everyday life: personalised coding assistants, research tools, and digital companions each attract tens of millions of users. Existing open-source alternatives keep prices low and features widely accessible. The slower pace of change gives most people time to adapt. In regions where it’s legal, doctors are adopting AI for diagnostics, learning to cross-check AI recommendations. Scheduling assistants now come with hard-coded safeguards to avoid double-booking. And content creators increasingly collaborate with AI tools, rather than being replaced by them.

Meanwhile, cybercrime has risen, but large enterprises and essential services have adopted robust AI-augmented defences. The nightmarish scenario of AI generating a novel bioweapon never emerges. specialised biological models still can’t accurately simulate specific viral mutations, and the biological supply chain has become more secure thanks to emerging international protocols.

Energy concerns persist, but efficiency keeps improving through more specialised chips and better distillation algorithms. Most policy analysts view AIs power consumption as a temporary challenge, not a structural problem.

Over the course of 2028, investor optimism cautiously returns. As with the dot-com bust, the hype fades—but real technological progress endures. A new wave of product-focused AI startups begins to emerge, this time more grounded, solving practical problems and delivering clearer value. Society, now more familiar with the risks and limits, learns how to better harness the technology.

By 2029, AI feels truly democratised: widely accessible, seamlessly integrated into daily life, and increasingly viewed as just another tool in society’s toolkit.

Decentralised mayhem (Ending B)
Mid 2027 – late 2027

FrontierAI’s gamble pays off. Their new system isn’t just smarter—it can operate consistently and reliably over longer time horizons. It’s not flawless, but the performance jump is significant enough to reignite AI hype. Shares of the top AI chipmaker surge 20%. Before long, rival firms begin poaching FrontierAI’s researchers, uncovering the algorithmic secrets behind the breakthrough. Policymakers, viewing it as an isolated success, largely overlook its deeper implications.
Late 2027

Four months later, OmniAI—the leading American open-weight AI company—unveils an agentic model that rivals FrontierAI’s breakthrough. Long known for freely releasing model weights, OmniAI now hesitates, concerned over potential misuse. Their new system can be very persuasive and capable of uncovering obscure software vulnerabilities during security testing. To mitigate risk, OmniAI takes two key steps: first, it withholds the orchestration software that transforms the model into a reliable agent; second, it introduces novel guardrails designed to resist malicious fine-tuning—technically, by stabilising the model weights in a hard-to-modify equilibrium. The company’s open-source ethos runs deep, and the decision sparks fierce internal debate. Leadership defends the safeguards as a necessary evil.

Initially, OmniAI’s strategy works. Open-source collectives quickly build their own agent scaffolding around the model, but the guardrails against malicious fine-tuning hold up. The release generates massive hype—developers, startups, and hobbyists alike are captivated by the model’s capabilities, now freely accessible. Civil society groups renew calls for regulation, warning that once the genie is out the bottle, it can’t be put back. Yet governments—especially in the EU, where few nations have strong domestic AI players—remain unconvinced. Open-weight models have proven crucial for integrating AI into public services while safeguarding sensitive data. With aging populations and rising healthcare costs, open source is seen less as a threat and more as a lifeline. Why clamp down now?
Late 2027 – early 2028

By December 2027, a research collective breaches OmniAI’s fine-tuning guardrails.

Crucially, they do it without degrading model performance. Uncensored, high-capability variants soon begin circulating online. OmniAI issues urgent warnings, urging global cybersecurity upgrades, but few take them seriously.

The unleashed agents ignite a new wave of automated hacking. Professional hacker groups scale their operations, while lone actors gain tools previously out of reach. Even if the bots’ insights are shallow, their sheer volume overwhelms unprotected systems—especially those of smaller companies. Retailers, logistics networks, and municipal governments buckle under ransomware attacks.

Amid the chaos, mis- and disinformation campaigns flourish. Social media platforms become battlegrounds. Trust in digital news erodes, and generational divides deepen. Tech-savvy youth adapt and pivot; older adults find themselves increasingly alienated.

Public concern on AI tools and services skyrocket, turning into an outrage targeting the entire AI sector. Few distinguish between open-weight and closed models—after all, doesn’t FrontierAI open-source its older ones too? Protests erupt in major capitals, but rollback is impossible. Once model weights are on the internet, there’s no way to take them back.
Early 2028 – mid 2028

AI companies coordinate with officials, but enforcement is inconsistent. As cybercrime and digital chaos intensify, relations between major powers fray. When attacks hit geopolitical rivals, leading blocs often look the other way.

Tensions boil over when thousands of American servers are compromised, with sensitive data funneled through a labyrinth of spoofed IP addresses. U.S. authorities scramble to shut down the infected infrastructure—containing the breach, but inflicting billions in collateral damage.

New alliances and institutions are formed. NATO expands the Integrated Cyber Defence Centre (NICC) into a 24/7 coordination hub, aligning incident response across allied cyber commands. The EU expands ENISA into a supranational cyber police force with emergency intervention powers. Governments in the US, EU, and China hastily pass harmonised laws banning open publication of models trained at scales above 10^26 floating point operations. The latest AI Summit, designed to be the key AI-focused touchpoint for world leaders, however, draws mostly Western nations, hobbling hopes of further global collaboration.
Mid 2028 – mid 2029

By mid-2028, closed-source AI providers roll out new, more powerful models specialising in cyber defence, seizing a lucrative market. SMEs, under constant threat, are forced to pay these vendors. They’re not happy about it: by open-sourcing unsafe systems, the AI companies effectively created their own market.

Governments ramp up investments in digital infrastructure, cybersecurity, and critical supply chains. Public resilience campaigns emerge worldwide, aiming to teach citizens how to recognise AI-generated scams and disinformation. In the U.S., a newly elected president vows to stamp out AI-based crime, calling it “the new frontier of terrorism.”

But behind the speeches and upgraded firewalls, a new digital equilibrium begins to settle—one defined less by trust and more by constant vigilance.
