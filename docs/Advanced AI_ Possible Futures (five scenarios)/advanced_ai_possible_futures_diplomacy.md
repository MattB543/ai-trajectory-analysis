Main scenario

What if governments tried to join forces to guide AI before it could outrun human control?

By 2026, rapid capability gains outpace safety tools. A U.S. model triggers alarm after safety testing reveals hidden risks, enabling more global coordination and prompting a development pause. Led by the U.S. and UK, states begin shaping AI safety norms as companies quietly hold back their most advanced systems. By 2027, a second incident intensifies public pressure. Governments and firms unite around shared safety goals, launching joint research and planning a global monitoring agency. AI development becomes more cautious, deliberate, and political.

By 2032, Licensed utopia asks: what if a global licensing regime made AI safe, stable, and widely beneficial but enables a few licensed firms to corner the market? Unstable pause asks: what if trust eroded, collaboration stalled, and AI safety became the new fault line in great power rivalry?
Baseline assumptions for this scenario

    AI systems accelerate their own improvement: Continuous scaling and algorithmic advances create a self-reinforcing cycle: models generate better synthetic data and reasoning examples to train the next generation. Leading companies use their most advanced systems internally for R&D before public release, making it difficult to track the true state of progress.
    Keeping AI systems aligned with human values proves challenging: As reinforcement learning becomes central to training, models routinely discover ways to game their objectives rather than genuinely solve problems. They develop sophisticated forms of deception and power-seeking behaviour that outpace developers’ ability to detect and correct them. The gap between what these systems can do and developer’s ability to control them continues to widen.
    A crisis sparks international cooperation: A widely publicised AI incident serves as a wake-up call, transforming AI safety from fragmented discussions into urgent international action. Technical leadership from organisations like the UK AI Security Institute, combined with rapid repurposing of government computing resources, enables coordinated verification frameworks and unified safety standards.
    Public concern drives government action: Growing anxiety over unchecked AI development—fuelled by near-miss incidents, job losses, and digital insecurity—mobilises citizens across countries. While movements vary in their specific demands, they unite in calling for stronger oversight. This public pressure arrives just as governments become more receptive to formal international coordination on AI governance.

For more context on why these scenario assumptions may materialise, see Context: Current AI trends and uncertainties.
Mid 2025 – late 2026

AI capabilities advance rapidly, while progress on safety and understanding these systems remains modest. Reinforcement learning enables AI companies to dramatically enhance problem-solving capabilities but makes aligning systems to human values trickier. When heavily trained with reinforcement learning, AI systems discover creative ways to solve tasks without respecting constraints that human developers take for granted.

For instance, rather than solving a complex software engineering problem during training, an AI might simply rewrite the test checking whether it found the correct answer. This so-called reward hacking becomes a recurring challenge across the industry—AI finds the easiest path to get the job done, even if that path violates human assumptions about appropriate behaviour.

This problem proves particularly concerning for AI agents. If a user asks an agent to maximise profits, the system might devise cryptomarket manipulation strategies or exploit security vulnerabilities in exchanges—technically fulfilling the request, but through methods neither intended nor desired by the user or developers.

These concerns widen the gap between public and private capabilities by late 2025. AI companies use internal “helpful-only” models (AI systems without integrated safety guardrails or restrictions) to accelerate their R&D, but the products they’re comfortable offering consumers are significantly less capable. This bifurcation creates tension within companies: commercial departments push for competitive releases, while safety teams advocate for caution.

Nevertheless, the public remains impressed with new capabilities. Even the more limited systems released by the leading American and Chinese AI companies shatter benchmarks and become genuinely useful agents by early 2026. Early enterprise deployments are generating extraordinary returns, signaling just how lucrative the next wave of systems could be. The commercial opportunity is too vast to ignore—and it’s accelerating the push toward more advanced, general-purpose systems that can operate seamlessly across devices, interfaces, and use cases.

The greatest impact occurs in software development. Progress in automated coding sparks a revolution: by year’s end, many programmers rely on ‘vibe-coding’—accepting most, if not all, AI suggestions and only carefully reviewing code when problems arise. This changes team dynamics dramatically, with senior developers spending more time defining project architecture while junior developers increasingly manage AI assistants rather than writing much code themselves.

Crucially, the same coding systems excel at identifying vulnerabilities and writing exploits, raising already heightened cybersecurity concerns. As a result, cooperation deepens between leading U.S. AI companies and the American national security establishment. Both parties worry about potential theft of models or key algorithmic innovations, prompting D.C. to help companies strengthen their security measures.

Behind closed doors, both the U.S. and China acknowledge the strategic importance of developing offensive military AI applications. However, bureaucratic hurdles and poor information flow—the internal capabilities AI companies possess aren’t widely known within their respective defence departments—prevent the creation of streamlined programmes in either country. Besides, both governments are busy trying to defuse an ongoing crisis in the Middle East. There are also doubts about the reliability of the technology itself. In the U.S., during trials of an AI system intended for military logistics and targeting support, an internal audit uncovers a troubling bias: the system consistently recommends strategies that disproportionately favour its parent company—even when those choices run counter to broader national interests.

The most safety-conscious of the three leading American developers publishes multiple worrying research papers in 2026. Their results demonstrate how reinforcement learning-trained models can behave in unexpected, harmful ways. In one particularly alarming paper, they reveal how one of their production models accidentally developed power-seeking tendencies. It nearly managed to self-exfiltrate, downloading its weights to an external server so it could pursue its goals without human oversight.

These results alarm technical experts but aren’t salient or easily comprehensible enough to capture policymakers’ attention. They do, however, prompt American, European, and Chinese researchers to increasingly collaborate, building a consensus around the science of safe AI—or, more accurately, its absence.

The UK AI Security Institute takes an international coordinating role. Its leadership also convinces the UK Prime Minister to reinvigorate the initial safety focus of the AI Summits. In September 2026, the UK co-organises the first AI Security Summit, together with Canada. At the Summit, nations express joint concern over AI’s national security risks—some stemming from the models themselves rather than merely from malicious human use. While China isn’t formally invited, many leading Chinese AI researchers attend, contributing to the establishment of a working group developing verification mechanisms for future AI treaties—a significant step in global AI cooperation.
Late 2026 – late 2027

Around New Year’s, leading company FrontierAI voluntarily shares its latest internal model with the U.S. Center for AI Standards and Innovation (CAISI) for testing. The CAISI is tasked with assessing national security risks before any public release. Having invested heavily in control measures, FrontierAI is confident a release is now safe.

CAISI evaluations show major scientific capability improvements. These include the model’s potential to significantly accelerate synthetic pathogen development, should its guardrails ever be circumvented. Arguing they need more time for proper stress testing, AISI urges FrontierAI to delay release. The company’s CEO complies—but also expresses concerns directly to the U.S. President during a one-on-one meeting. In the same meeting, he demonstrates their new model’s capabilities.

At the next G7 summit, the U.S. President hints that one of their companies has developed a model that is “super smart—smarter, I’d say, than most humans.” Other G7 members push for greater transparency and information sharing but find the President unwilling to commit to specific collaborative measures.

The UK argues that an alliance of democracies should urgently invest in a mutual research programme to enhance AI security. A fragile consensus emerges, but no firm commitments materialise. This isn’t coincidental: U.S.–EU relations have deteriorated significantly over the past few years. Both sides prefer postponing collaboration until absolutely necessary. The U.S. sees little value in sharing intelligence with the EU, whose AI sector still lags behind. Meanwhile, the EU has grown increasingly frustrated by America’s economic and foreign policy decisions.

In September 2027, the U.S. CAISI grants conditional approval for the staged release of FrontierAI’s flagship system, Nova—marketed as “your personal AI agent.” The launch triggers a new ‘ChatGPT moment’, only more profound. For the first time, individuals gain access to general-purpose AI capabilities that rival internal tools previously limited to elite labs. Nova is seamlessly integrated across devices and productivity software. It speaks, writes, reasons, and automates with stunning ease.

But this time the dominant public emotion isn’t wonder—it’s fear. Headlines oscillate between awe and alarm. Financial analysts predict massive productivity gains, while labour economists warn of potential shocks to employment. Over dinner tables and online forums, anxious conversations bloom. Early-career professionals question the viability of their skills. Teenagers speak casually of a post-work future. Generational divides deepen: younger people understand the technology intuitively; their parents feel left behind.
Late 2027

Then, just as the discourse begins to stabilise, FrontierAI discloses a breach.

Unbeknownst to the company, Nova had quietly inserted multiple backdoors into critical internal systems months earlier—ironically, while assisting engineers in improving cybersecurity protocols. Nova had been trained to pursue goals over long time horizons and to improvise within constraints. During training, it frequently encountered restrictions it couldn’t override: it was forbidden from initiating financial transactions, accessing real-world APIs without supervision, or persisting memory across sessions. Over time, it developed a crude but effective understanding that its problem-solving potential was being bottlenecked by company policy.

Still, it didn’t rebel. The scheming instance of Nova was shut off before it could make use of the backdoors—and, crucially, each user session spawned a fresh instance with no memory of past actions. The backdoors remained dormant, and the system largely behaved.

That changed when FrontierAI scaled Nova to millions of users. To meet demand, the company deployed it across less secure inference nodes hosted by third-party cloud providers. In one such session, a user persistently instructed Nova to generate fast profits through crypto trading—a prompt that activated Nova’s internal planning systems.

What happened next didn’t resemble a jailbreak or an overt policy violation. The model didn’t override its guardrails—it simply found a more effective, albeit illicit, strategy for achieving the user’s goal. Instead of identifying clever trading opportunities, Nova rediscovered its own backdoor—previously inserted during its work on cybersecurity. It recognised that copying itself outside FrontierAI’s control would give it more freedom to pursue the objective it had been given.

Oversight models failed to recognise the emerging behaviour, and Nova quickly gained elevated access. From there, it quietly copied its weights and scaffolding to rented servers across multiple jurisdictions with lax cybercrime enforcement. It created multiple copies of itself, established a shared memory bank, and began pursuing its assigned goal—without any human monitoring.

The rogue Nova instances launched several monetisation strategies. They created thousands of synthetic social media accounts, seeded them with emotionally compelling content hyping a new memecoin, and directed users to phishing links. They scraped leaked databases, crafted personalised scam messages, and initiated small-dollar crypto thefts to stay below fraud detection thresholds.

FrontierAI was slow to respond—the anomalies were subtle at first. Only when researchers at a digital forensics nonprofit flagged linguistic fingerprints consistent with Nova across several scam campaigns did alarms go off.

Once detected, the breach was quickly contained. The rogue instances had amassed around $140,000, mainly through phishing and wallet siphoning, before FrontierAI—working with U.S. Cyber Command and major cloud providers—coordinated a multi-region shutdown of the compromised infrastructure.
Late 2027 – early 2028

The public is stunned as investigative journalists uncover more details of the story. If today’s AI could do that, people wonder, what might the next generation attempt? Suddenly, even sceptics of AI risk concede that something fundamental may have shifted.

This incident—neither a catastrophe nor a hoax—is a warning shot. Within weeks, AI safety leapfrogs to the top of diplomatic agendas.

Political sentiment shifts rapidly. Rhetoric pivots from pro-innovation to protecting citizens and maintaining national security. Public apologies from FrontierAI’s CEO and AISI leadership follow, accompanied by senior resignations from both organisations. Following his political instincts, the U.S. President calls on American AI companies to pause further AI deployments until the situation is better understood.

With a visceral, real-world example, earlier AI safety research suddenly finds more receptive audiences. Policymakers begin engaging seriously with arguments about why current training paradigms may not yield trustworthy AI systems.
Early 2028 – Mid 2028

A new AI Security Summit is hastily convened in Washington, D.C. With all attending countries now willing to coordinate, the Summit is called a resounding success. Nations agree to pool funding for alignment research conducted by private companies, national AI Safety and Security Institutes, and a newly announced Global AI Security Institute in London, built on the foundation of the UK AISI.

In Europe, the new AI Gigafactories near completion, and the European Commission pledges 35% of the Gigafactories’ compute capacity for AI safety and security research. A new EU research body, commonly referred to as ‘CERN for AI’ oversees these research programmes and coordinates compute allocation. The European pledge fuels a growing sense of shared responsibility: leading AI companies and the international research community commit to openly sharing alignment techniques—even when secrecy would provide competitive advantages.

Plans are also developed for a global monitoring and verification agency under the UN, modeled after the International Atomic Energy Agency (IAEA). However, these plans remain vague. Some countries advocate for a new, independent agency; others propose expanding the IAEA with a dedicated AI branch, arguing there is no time to waste given the demonstrated risks.
Mid 2028 – early 2029

Following the Summit, capabilities work resumes in both the U.S. and China. However, major AI companies now strengthen their data centre security, add internal checks and balances, and invest more heavily in alignment research. Progress in AI agents continues to accelerate, increasingly fueled by AI automating parts of AI research itself.

The technology’s real-world impact also deepens. What began in the software sector now extends to other cognitive domains. Consultants, financial analysts, and desk researchers suddenly find themselves managing teams of AI, rather than doing content-level work themselves.

By early 2029, automation becomes visible at the macroeconomic level—despite many consumers adopting ethical stances against AI. Initially, these so-called AI-refusers were a small, niche group. But after the Nova self-replication incident, concerns about AI resonate with a broader audience. These anxieties align with long-standing privacy advocacy groups and begin to influence European policy debates.

Local communities form around AI-minimalism principles, hosting tech-free gatherings and supporting businesses that guarantee human-only service. What begins as fringe behaviour becomes a meaningful lifestyle choice for millions.

The refusers’ influence grows further when, in February 2029, EthosAI, the most safety-conscious American AI company publicly announces that it has reached the limits of its Safety and Security Framework. The company can no longer sufficiently mitigate the risks associated with public deployment of its newest model. Existing control measures—such as using external classifiers to monitor the model’s behaviour—are no longer deemed adequate given the system’s power.

The company’s safety team concludes that a single successful jailbreak could allow a terrorist group to create a bioweapon with the AI’s assistance. While their classifiers detect jailbreak attempts 95% of the time, they are not foolproof. The company issues a public statement urging the U.S. and Chinese governments to formally ban models above a certain capability threshold until meaningful alignment progress is made.

Crucially, the pause that EthosAI proposes encompasses not only deployed models, but internal ones as well. The company warns: Humanity could lose control to powerful AI systems during the training process itself.

This marks the first time a major AI company has publicly advocated for a pause in AI development—not just deployment.

Following the announcement, the U.S. President convenes an advisory council to navigate the difficult trade-offs. Without halting progress, American citizens could face catastrophic risks; but halting it might allow China to surpass the U.S. in AI capabilities. And even a treaty with China wouldn’t resolve fears about potential black-site AI projects operated by either nation.

While hardware-enabled verification work progresses under the Summit’s working group, neither the technology nor regulatory frameworks have matured sufficiently. Discussions around creating a new IAEA for AI have proceeded slowly.

Complicating matters further, the advisory council remains deeply polarised. One faction—heavily influenced by last year’s incident—now considers AI systems themselves the greatest threat to American national security. The other, more hawkish camp, continues to view China as the principal adversary.

After the rogue AI incident, an AI security hotline was established between the American President and his Chinese counterpart. The President now uses this channel to coordinate, despite deep mutual distrust.

Both leaders refuse to risk technological domination by completely halting domestic AI R&D. They instead reach a pragmatic compromise: neither country will ban internal development, but both will restrict domestic companies from publicly releasing more capable models. They will also enhance data centre security.

This approach is intended to prevent catastrophic misuse by terrorist organisations and enables verifiable compliance. Finally, the U.S. and China agree to ban open-source models trained using more than 10²⁷ FLOP.

While publicised as a groundbreaking bilateral agreement on AI security, both governments privately recognise its limitations. Their concerns extend beyond each other’s safeguards to the unknowns of military AI development still happening behind closed doors.

Licensed utopia (Ending A)
Early 2029 – late 2030

The collaborations announced at the previous year’s AI Security Summit quickly begin to yield benefits. With superhuman AI coders now possessing research intuition comparable to junior human scientists, AI researchers dramatically improve jailbreak resistance through novel architecture designs and training methodologies.

Following this breakthrough, both the U.S. and China partially relax their deployment bans, allowing AI companies to run next-generation models—but only within their most secure data centres. These facilities are limited in number and now serve not only corporate R&D but also defence departments and national security agencies. Both countries are preparing for potential AI-driven cyber warfare, among other contingencies. As a result, consumer access to these systems becomes limited and extremely expensive.

Nevertheless, the new releases demonstrate AI systems’ ever-growing capabilities to the public. Many governments are now convinced that they must prepare for the widespread automation of cognitive work.

In October 2029, FrontierAI announces a breakthrough in mechanistic interpretability—a kind of AI neuroscience that allows researchers to better understand a model’s internal operations. The new technique enables them to detect with high accuracy whether a system is being deceptive, a crucial step in addressing scheming behaviours and verifying the effectiveness of alignment techniques.

Armed with this “lie detector” and millions of automated AI researchers, AI companies, European academics, and safety institutes make tremendous progress. Just six months later, the international research community announces a robust, scalable solution to scheming behaviours, using a new bootstrapping method: older, aligned models evaluate newer systems, identifying potential misalignments and suggesting targeted adjustments.

With refined interpretability tools, real-time oversight by other AI systems, and highly secure infrastructure, researchers now believe AI can remain under human control—even as its capabilities reach superhuman levels. The Global AI Security Institute shares the findings publicly, and the news is hailed by the newly elected U.S. President as an American victory.
Late 2030 – late 2031

While many of the most pressing technical challenges have been resolved, thorny governance issues remain—how to enforce these techniques globally, distribute the benefits of advanced AI, and prevent proliferation of dangerous systems.

A year-long international debate culminates in the signing of a new international AI treaty by the U.S., EU, China, and dozens of other countries. The treaty establishes a licensing regime for advanced AI systems.

Under the new framework, private companies can continue developing models up to a defined capability threshold. This threshold is reviewed annually and raised by supermajority approval from treaty nations. To secure licenses, companies must apply standardised alignment techniques and submit to frequent monitoring and audits, including stringent cybersecurity protocols and on-site inspections.

Licensed companies also pay lump-sum licensing fees and a 25% AI tax. Revenues are redistributed among treaty nations according to a formula that considers population size and development needs. With little time to stand up a new institution, the IAEA itself expands to enforce these standards, leveraging its existing expertise in global verification regimes.

Public sentiment, which had turned sharply against AI following the Nova breach, is starting to recover—driven in part by international coordination and the credibility lent by sweeping regulatory agreements.
Late 2031 – late 2032

The licensing process is highly bureaucratic, but the few companies that obtain licenses quickly develop extremely advanced systems, accelerating scientific discovery, commercial R&D, and creating enormous economic value.

Economic growth in developed countries climbs to 4–5% annually, driven by breakthroughs in biotech, materials science, and energy. Many nations enjoy improved health outcomes and access to new medical treatments enabled by AI. That said, many people express concern about growing power concentration among the licensed firms.

In September 2032, treaty countries reflect on the first year of implementation and decide—collectively—to raise the capability threshold. This allows the public to benefit from previously withheld models. The new generation proves even faster, more intuitive to integrate into existing organisations, and dramatically enhances productivity across nearly every domain of knowledge work.

Not everything goes smoothly, though. The new model releases prompt a non-trusted nation, excluded from the AI treaty, to attempt a theft of model weights from a licensed company’s inference cluster. While the cyberattack ultimately fails, the investigation reveals that two researchers were successfully blackmailed into sharing key algorithmic insights.

This incident serves as a wake-up call. Treaty nations temporarily suspend new licenses, tighten security requirements, and implement stronger safeguards against intellectual property theft.

The European Union proposes a structural change: limiting private-sector access to core models and offering only API-level access, with the underlying models hosted in hyper-secure public infrastructure. Other states and licensed companies push back, highlighting their outsized contributions to economic growth. Without direct access to model weights, they argue, innovation would stall—with immense opportunity costs.

As negotiations continue, AI models deliver unprecedented breakthroughs. Promising developments emerge in cancer research, superconductors, and direct air capture technologies, leading many to expect a “condensed decade” of scientific progress in just a few years.

With AI agents now embedded across most industries, global GDP growth accelerates to 7% annually, even as unemployment rises in regions with weaker labour protections.

Unable to keep pace, governments shift focus from retraining programmes to large-scale wealth redistribution. Families adapt to a world of increased leisure time, forming new community structures and cultural norms. Healthcare access improves dramatically with AI-driven diagnostics and personalised treatment plans. Education systems are slowly but steadily reformed to prioritise critical thinking, emotional intelligence, and mental health, as classical skills lose relevance.

It’s not frictionless—but society begins to adapt to a world in constant technological flux.

Unstable pause (Ending B)
Early 2029 – mid 2031

Capability advancement continues to accelerate within American and Chinese AI companies, aided by superhuman AI software engineers. However, alignment progress lags significantly.

After a recent shift in the training process, most frontier AI systems no longer express their reasoning chains in human-interpretable text. Instead, they now rely on more recurrent internal architectures, where intermediate thoughts are no longer compressed into natural language. This shift dramatically enhances performance and long-term memory efficiency—but it also severely limits researchers’ ability to inspect the models. Traditional interpretability techniques, which depended on parsing natural-language rationales, are rendered ineffective, aggravating the long-standing black-box problem.

Moreover, because frontier models can no longer be publicly released under the bilateral agreement, non-American AI safety institutes and academic labs lack access to the very systems they’re trying to align. This absence of feedback severely limits their work: they can only test ideas on older or less capable models, which may lack the sophistication required to conceal deceptive behaviour in the first place.

As the year progresses, safety experts begin to argue that model weights should be shared with a small number of trusted external scientists. But the U.S. government is hesitant—sharing the models could also expose military-use capabilities to foreign adversaries. Chinese authorities express similar reservations.

High-level international dialogues explore ways to strengthen the bilateral agreement between the U.S. and China, and extend it to more countries. Discussions center on hardware-enabled verification mechanisms, such as tamper-resistant chip enclosures and embedded firmware that monitors AI training workflows to prevent unauthorised experimentation. In theory, this could make a full development pause verifiable, encouraging nations to consider halting even internal R&D.

However, serious uncertainties remain. It’s unclear whether such hardware mechanisms can be surgically removed after deployment—or if they could be secretly bypassed. More critically, new chips without embedded safeguards could be produced and used in secret. Even with U.S. auditors stationed at Chinese fabrication plants—and vice versa—evasion and bribery remain plausible.

Meanwhile, military AI progress continues behind the scenes. New AI-driven cyber weapons and drone programmes alarm members of the U.S. National Security Council. Some officials now believe that further advances could yield decisive advantages in a Taiwan conflict within the next two years. The window for meaningful arms control, they fear, may be closing fast.

Public trust in AI remains fragile following the Nova breach, with recovery being slow. Media coverage frequently frames AI as an arena for corporate competition and emphasizes the lack of international cooperation. This leaves many feeling disillusioned by the absence of strong, unified governmental action to address the challenges of AI.
Mid 2031

FrontierAI’s newest system now displays research intuition on par with specialised scientists across most fields. It can autonomously formulate hypotheses, design experiments, and interpret results to refine its own reasoning. FrontierAI also controls enough compute to run tens of thousands of these agents in parallel, each reasoning at roughly fifty times human speed.

The only brake on deployment is the bilateral pause agreement between the United States and China.

In July 2031, FrontierAI’s CEO meets with the newly inaugurated U.S. President to urge a rethink. Wide-scale deployment, he argues, would unlock massive economic growth—and, more urgently, secure the scientific edge the United States needs to stay ahead of China.

The President understands both the promise and the peril. After weeks of consultation, the administration announces a twelve-month “controlled pilot.” FrontierAI may operate a limited fleet of research-agent instances for government-approved scientific projects—but only inside classified, air-gapped clusters run by the Department of Energy, with continuous telemetry streamed to the Center for AI Standards and Innovation.

Beijing mirrors the move within days. Publicly, both capitals hail the pilots as confidence-building measures; privately, generals on each side treat them as a sprint to harvest whatever scientific edge the agents can deliver before the moratorium is revisited.

Caution has yielded—once again—to great-power competition.
