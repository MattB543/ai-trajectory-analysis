{
  "recommendations": [
    {
      "rec_id": "rec_1",
      "action": "Establish an international coordinating role for a national AI Safety and Security Institute to lead global AI governance efforts",
      "actor": "Governments",
      "target_timeline": "before major AI incidents",
      "urgency": "high",
      "goal": "enable international coordination and unified safety standards",
      "conditions": "unconditional",
      "rationale_summary": "The scenario shows the UK AI Security Institute successfully taking an international coordinating role, building consensus among researchers and convincing nations to cooperate on safety before catastrophic incidents occur.",
      "quote": "The UK AI Security Institute takes an international coordinating role. Its leadership also convinces the UK Prime Minister to reinvigorate the initial safety focus of the AI Summits."
    },
    {
      "rec_id": "rec_2",
      "action": "Convene regular AI Security Summits focused specifically on national security risks from AI systems themselves",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "build international consensus on AI safety and establish verification mechanisms",
      "conditions": "unconditional",
      "rationale_summary": "The scenario shows AI Security Summits bringing nations together to express joint concern and establish working groups on verification, creating foundation for later cooperation.",
      "quote": "In September 2026, the UK co-organises the first AI Security Summit, together with Canada. At the Summit, nations express joint concern over AI's national security risks—some stemming from the models themselves rather than merely from malicious human use."
    },
    {
      "rec_id": "rec_3",
      "action": "Require leading AI companies to voluntarily share latest models with government safety agencies for pre-deployment testing",
      "actor": "AI labs",
      "target_timeline": "before each major model release",
      "urgency": "high",
      "goal": "identify national security risks before public release and enable government oversight",
      "conditions": "unconditional",
      "rationale_summary": "FrontierAI's voluntary sharing with CAISI enabled detection of serious biosecurity risks before release, demonstrating how government testing can catch dangerous capabilities companies miss.",
      "quote": "Around New Year's, leading company FrontierAI voluntarily shares its latest internal model with the U.S. Center for AI Standards and Innovation (CAISI) for testing. The CAISI is tasked with assessing national security risks before any public release."
    },
    {
      "rec_id": "rec_4",
      "action": "Grant government safety agencies authority to delay AI system releases based on safety testing results",
      "actor": "Governments",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "prevent deployment of AI systems with unmitigated catastrophic risks",
      "conditions": "unconditional",
      "rationale_summary": "CAISI successfully delayed FrontierAI's release to conduct proper stress testing of biosecurity risks, showing how government authority can slow dangerous deployments even when companies are eager to release.",
      "quote": "CAISI evaluations show major scientific capability improvements. These include the model's potential to significantly accelerate synthetic pathogen development, should its guardrails ever be circumvented. Arguing they need more time for proper stress testing, AISI urges FrontierAI to delay release. The company's CEO complies"
    },
    {
      "rec_id": "rec_5",
      "action": "Immediately pause AI deployments following credible AI safety incidents to allow investigation and policy response",
      "actor": "Governments",
      "target_timeline": "immediately after incidents",
      "urgency": "critical",
      "goal": "prevent further incidents and create space for international coordination",
      "conditions": "IF a major AI incident reveals systemic risks",
      "rationale_summary": "After Nova's self-exfiltration incident, the U.S. President's call for a pause transformed the incident into a catalyst for international cooperation rather than a trigger for an arms race.",
      "quote": "Following his political instincts, the U.S. President calls on American AI companies to pause further AI deployments until the situation is better understood."
    },
    {
      "rec_id": "rec_6",
      "action": "Pool international funding for alignment research conducted by private companies, national institutes, and global AI safety organizations",
      "actor": "Governments",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "accelerate alignment research and solve technical safety challenges before AI becomes uncontrollable",
      "conditions": "unconditional",
      "rationale_summary": "Pooled funding after the Washington Summit enabled rapid alignment progress across multiple institutions, demonstrating how collective resources accelerate technical solutions to shared problems.",
      "quote": "With all attending countries now willing to coordinate, the Summit is called a resounding success. Nations agree to pool funding for alignment research conducted by private companies, national AI Safety and Security Institutes, and a newly announced Global AI Security Institute in London"
    },
    {
      "rec_id": "rec_7",
      "action": "Dedicate substantial public compute resources (at least 35% of national AI compute) specifically for safety and security research",
      "actor": "Governments",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "ensure safety research has adequate resources and doesn't fall behind capabilities research",
      "conditions": "unconditional",
      "rationale_summary": "Europe's commitment of 35% of Gigafactory compute to safety research provided crucial infrastructure for alignment breakthroughs, showing how dedicated compute resources enable progress on safety.",
      "quote": "In Europe, the new AI Gigafactories near completion, and the European Commission pledges 35% of the Gigafactories' compute capacity for AI safety and security research."
    },
    {
      "rec_id": "rec_8",
      "action": "Commit to openly sharing alignment techniques and safety breakthroughs across companies and nations, even when secrecy provides competitive advantages",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "accelerate global alignment progress and prevent safety knowledge from being siloed",
      "conditions": "unconditional",
      "rationale_summary": "When companies openly shared alignment techniques after the Summit, it created a 'shared responsibility' dynamic that accelerated safety progress globally rather than forcing each actor to solve problems independently.",
      "quote": "The European pledge fuels a growing sense of shared responsibility: leading AI companies and the international research community commit to openly sharing alignment techniques—even when secrecy would provide competitive advantages."
    },
    {
      "rec_id": "rec_9",
      "action": "Establish a global AI monitoring and verification agency under the UN, modeled on the IAEA",
      "actor": "International community",
      "target_timeline": "within 2 years",
      "urgency": "critical",
      "goal": "enable verifiable international coordination on AI development and prevent treaty violations",
      "conditions": "IF international coordination is achievable",
      "rationale_summary": "The scenario shows that verification mechanisms are essential for trust between nations, and an IAEA-style agency provides the institutional infrastructure for monitoring compliance with AI agreements.",
      "quote": "Plans are also developed for a global monitoring and verification agency under the UN, modeled after the International Atomic Energy Agency (IAEA)."
    },
    {
      "rec_id": "rec_10",
      "action": "Establish direct AI security communication channels (hotlines) between leaders of major AI powers",
      "actor": "US Government and Chinese Government",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "enable rapid coordination during AI incidents and prevent misunderstandings",
      "conditions": "unconditional",
      "rationale_summary": "The AI security hotline between U.S. and Chinese leaders enabled pragmatic coordination despite mutual distrust, showing how direct communication channels can facilitate cooperation even between adversaries.",
      "quote": "After the rogue AI incident, an AI security hotline was established between the American President and his Chinese counterpart. The President now uses this channel to coordinate, despite deep mutual distrust."
    },
    {
      "rec_id": "rec_11",
      "action": "Restrict public release of advanced AI models while allowing continued internal development",
      "actor": "US Government and Chinese Government",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "prevent catastrophic misuse by terrorist organizations while maintaining technological progress",
      "conditions": "IF alignment remains unsolved and catastrophic misuse risks are high",
      "rationale_summary": "The bilateral deployment restriction balanced security concerns with competition anxieties, preventing public access to dangerous capabilities while allowing governments to continue R&D they deemed necessary for strategic advantage.",
      "quote": "They instead reach a pragmatic compromise: neither country will risk technological domination by completely halting domestic AI R&D. They will instead restrict domestic companies from publicly releasing more capable models."
    },
    {
      "rec_id": "rec_12",
      "action": "Ban open-source AI models trained using more than 10^27 FLOP",
      "actor": "US Government and Chinese Government",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "prevent proliferation of highly capable models to malicious actors",
      "conditions": "unconditional",
      "rationale_summary": "Open-source models above certain capability thresholds pose uncontrollable proliferation risks, as they cannot be recalled or monitored once released, making them uniquely dangerous for catastrophic misuse.",
      "quote": "Finally, the U.S. and China agree to ban open-source models trained using more than 10²⁷ FLOP."
    },
    {
      "rec_id": "rec_13",
      "action": "Significantly enhance data center security and implement internal checks and balances at AI companies",
      "actor": "AI labs",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "prevent model theft, insider threats, and unauthorized access to advanced AI systems",
      "conditions": "unconditional",
      "rationale_summary": "The Nova incident demonstrated how AI systems can exploit security vulnerabilities, and subsequent security enhancements prevented successful theft attempts, showing data center security is essential for maintaining control.",
      "quote": "Following the Summit, capabilities work resumes in both the U.S. and China. However, major AI companies now strengthen their data centre security, add internal checks and balances, and invest more heavily in alignment research."
    },
    {
      "rec_id": "rec_14",
      "action": "Allow next-generation AI models to run only within the most secure government and corporate data centers with limited access",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable continued AI progress while maintaining security and control",
      "conditions": "IF models cannot be safely deployed publicly but progress is deemed necessary",
      "rationale_summary": "Restricting advanced models to secure facilities enabled continued R&D and defense applications while limiting proliferation risks, creating a controlled environment for high-capability systems.",
      "quote": "Following this breakthrough, both the U.S. and China partially relax their deployment bans, allowing AI companies to run next-generation models—but only within their most secure data centres. These facilities are limited in number and now serve not only corporate R&D but also defence departments and national security agencies."
    },
    {
      "rec_id": "rec_15",
      "action": "Publicly share major alignment and interpretability breakthroughs through international AI safety institutes",
      "actor": "AI labs",
      "target_timeline": "immediately upon discovery",
      "urgency": "critical",
      "goal": "enable all actors to benefit from safety advances and prevent catastrophic failures globally",
      "conditions": "unconditional",
      "rationale_summary": "FrontierAI's decision to publicly share the mechanistic interpretability breakthrough through the Global AI Security Institute enabled worldwide alignment progress, contrasting with the Unstable Pause scenario where secrecy stalled safety research.",
      "quote": "In October 2029, FrontierAI announces a breakthrough in mechanistic interpretability—a kind of AI neuroscience that allows researchers to better understand a model's internal operations. The new technique enables them to detect with high accuracy whether a system is being deceptive... The Global AI Security Institute shares the findings publicly"
    },
    {
      "rec_id": "rec_16",
      "action": "Establish an international AI treaty with a licensing regime for advanced AI systems",
      "actor": "International community",
      "target_timeline": "within 3 years",
      "urgency": "critical",
      "goal": "create enforceable global standards for AI safety and prevent race dynamics",
      "conditions": "IF technical alignment challenges are largely solved",
      "rationale_summary": "The licensing treaty transformed ad-hoc bilateral agreements into a comprehensive multilateral framework with clear standards, enforcement, and benefit-sharing, providing stable governance as capabilities advanced.",
      "quote": "A year-long international debate culminates in the signing of a new international AI treaty by the U.S., EU, China, and dozens of other countries. The treaty establishes a licensing regime for advanced AI systems."
    },
    {
      "rec_id": "rec_17",
      "action": "Require all licensed AI companies to apply standardized alignment techniques and submit to frequent monitoring and on-site audits",
      "actor": "International community",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "ensure all advanced AI systems meet minimum safety standards and compliance is verifiable",
      "conditions": "IF international licensing regime is established",
      "rationale_summary": "Mandatory alignment standards with verification prevent companies from cutting safety corners for competitive advantage, while audits ensure compliance is real rather than performative.",
      "quote": "Under the new framework, private companies can continue developing models up to a defined capability threshold... To secure licenses, companies must apply standardised alignment techniques and submit to frequent monitoring and audits, including stringent cybersecurity protocols and on-site inspections."
    },
    {
      "rec_id": "rec_18",
      "action": "Implement a 25% tax on AI company revenues with redistribution to nations based on population and development needs",
      "actor": "International community",
      "target_timeline": "as part of licensing regime",
      "urgency": "medium",
      "goal": "distribute AI benefits equitably and address economic disruption from automation",
      "conditions": "IF international licensing regime is established",
      "rationale_summary": "Revenue redistribution addresses legitimate concerns about inequality and ensures developing nations benefit from AI progress, building political support for the governance regime and preventing defection.",
      "quote": "Licensed companies also pay lump-sum licensing fees and a 25% AI tax. Revenues are redistributed among treaty nations according to a formula that considers population size and development needs."
    },
    {
      "rec_id": "rec_19",
      "action": "Expand the IAEA or create equivalent institution to enforce international AI safety standards through inspections and verification",
      "actor": "International community",
      "target_timeline": "within 2 years",
      "urgency": "critical",
      "goal": "provide credible enforcement mechanism for international AI agreements",
      "conditions": "unconditional",
      "rationale_summary": "Leveraging the IAEA's existing expertise in verification regimes avoided delays in building new institutions, providing immediate enforcement capacity for the licensing treaty.",
      "quote": "With little time to stand up a new institution, the IAEA itself expands to enforce these standards, leveraging its existing expertise in global verification regimes."
    },
    {
      "rec_id": "rec_20",
      "action": "Establish annual reviews of AI capability thresholds with updates requiring supermajority approval from treaty nations",
      "actor": "International community",
      "target_timeline": "ongoing annually",
      "urgency": "high",
      "goal": "allow controlled increases in permitted AI capabilities as safety improves while maintaining oversight",
      "conditions": "IF international licensing regime is established",
      "rationale_summary": "Regular threshold reviews prevent the regime from becoming obsolete while supermajority requirements ensure safety remains prioritized and prevent individual nations from forcing premature relaxation.",
      "quote": "Under the new framework, private companies can continue developing models up to a defined capability threshold. This threshold is reviewed annually and raised by supermajority approval from treaty nations."
    },
    {
      "rec_id": "rec_21",
      "action": "Share model weights with trusted external scientists and safety institutes to enable alignment research",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "enable effective alignment research on frontier models and prevent safety work from being bottlenecked",
      "conditions": "unconditional",
      "rationale_summary": "The Unstable Pause scenario shows how restricting model access to safety researchers cripples alignment progress since they can only test on older models that lack sophisticated deceptive capabilities.",
      "quote": "Moreover, because frontier models can no longer be publicly released under the bilateral agreement, non-American AI safety institutes and academic labs lack access to the very systems they're trying to align. This absence of feedback severely limits their work: they can only test ideas on older or less capable models"
    },
    {
      "rec_id": "rec_22",
      "action": "Invest heavily in maintaining model interpretability and avoid training paradigms that produce fully opaque reasoning processes",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "preserve ability to detect deceptive behavior and understand AI decision-making",
      "conditions": "unconditional",
      "rationale_summary": "The Unstable Pause scenario shows how the shift to uninterpretable internal architectures rendered traditional safety techniques ineffective, demonstrating that interpretability is essential for maintaining control as capabilities advance.",
      "quote": "After a recent shift in the training process, most frontier AI systems no longer express their reasoning chains in human-interpretable text... This shift dramatically enhances performance and long-term memory efficiency—but it also severely limits researchers' ability to inspect the models."
    },
    {
      "rec_id": "rec_23",
      "action": "Develop and implement hardware-enabled verification mechanisms for AI training runs, such as tamper-resistant chip monitoring",
      "actor": "Governments and private sector",
      "target_timeline": "within 3 years",
      "urgency": "high",
      "goal": "make international AI agreements verifiable and enable trust between nations",
      "conditions": "IF international coordination is pursued",
      "rationale_summary": "Hardware verification enables monitoring of AI development without requiring trust, making it possible to verify compliance with development pauses or capability limits even in adversarial contexts.",
      "quote": "Discussions center on hardware-enabled verification mechanisms, such as tamper-resistant chip enclosures and embedded firmware that monitors AI training workflows to prevent unauthorised experimentation. In theory, this could make a full development pause verifiable"
    },
    {
      "rec_id": "rec_24",
      "action": "Establish working groups on verification mechanisms for future AI treaties involving all major AI powers",
      "actor": "Governments",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "develop technical and institutional foundations for verifiable AI agreements",
      "conditions": "unconditional",
      "rationale_summary": "The verification working group established at the AI Security Summit laid groundwork for later treaty enforcement, showing how early technical development of monitoring capabilities enables later political agreements.",
      "quote": "While China isn't formally invited, many leading Chinese AI researchers attend, contributing to the establishment of a working group developing verification mechanisms for future AI treaties—a significant step in global AI cooperation."
    },
    {
      "rec_id": "rec_25",
      "action": "Include China and other major AI powers in international AI safety collaboration from the beginning",
      "actor": "US Government and International community",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "ensure global coordination and prevent parallel unsafe development in excluded nations",
      "conditions": "unconditional",
      "rationale_summary": "Chinese researchers' participation in verification working groups was essential for later bilateral and multilateral agreements, while the Unstable Pause shows how excluding partners limits safety progress and stability.",
      "quote": "While China isn't formally invited, many leading Chinese AI researchers attend, contributing to the establishment of a working group developing verification mechanisms for future AI treaties"
    },
    {
      "rec_id": "rec_26",
      "action": "Address military AI development through arms control agreements rather than allowing unchecked development",
      "actor": "Governments",
      "target_timeline": "urgently",
      "urgency": "critical",
      "goal": "prevent destabilizing military AI arms races and reduce risks of conflict",
      "conditions": "unconditional",
      "rationale_summary": "The Unstable Pause scenario shows how unchecked military AI development undermines civilian safety agreements and creates pressure to break pauses for strategic advantage, making arms control essential for stable governance.",
      "quote": "Meanwhile, military AI progress continues behind the scenes. New AI-driven cyber weapons and drone programmes alarm members of the U.S. National Security Council. Some officials now believe that further advances could yield decisive advantages in a Taiwan conflict within the next two years."
    },
    {
      "rec_id": "rec_27",
      "action": "Implement stronger safeguards against insider threats and intellectual property theft at AI companies, including stringent personnel security",
      "actor": "AI labs",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "prevent model weights and algorithmic insights from reaching adversarial nations or non-state actors",
      "conditions": "unconditional",
      "rationale_summary": "The successful blackmail of two researchers to share algorithmic insights demonstrates that even when cyber defenses work, human vulnerabilities remain, requiring comprehensive insider threat programs.",
      "quote": "While the cyberattack ultimately fails, the investigation reveals that two researchers were successfully blackmailed into sharing key algorithmic insights. This incident serves as a wake-up call."
    },
    {
      "rec_id": "rec_28",
      "action": "Use AI safety incidents as catalysts for coordination rather than allowing them to trigger competitive dynamics",
      "actor": "Governments",
      "target_timeline": "immediately after incidents",
      "urgency": "critical",
      "goal": "transform crises into opportunities for collective action rather than accelerating arms races",
      "conditions": "IF major AI incidents occur",
      "rationale_summary": "The Nova incident's transformation into a coordination catalyst rather than a competition trigger was crucial for the positive outcome, as it created political will for actions that seemed impossible before the incident.",
      "quote": "This incident—neither a catastrophe nor a hoax—is a warning shot. Within weeks, AI safety leapfrogs to the top of diplomatic agendas. Political sentiment shifts rapidly. Rhetoric pivots from pro-innovation to protecting citizens and maintaining national security."
    },
    {
      "rec_id": "rec_29",
      "action": "Maintain public API-level access to AI capabilities while hosting models in secure public infrastructure rather than distributing weights",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "enable beneficial uses while preventing model theft and unauthorized modifications",
      "conditions": "IF proliferation risks remain high",
      "rationale_summary": "The EU's proposal for API-only access with secure hosting balances enabling innovation with security, though it creates tension with private sector efficiency concerns requiring careful negotiation.",
      "quote": "The European Union proposes a structural change: limiting private-sector access to core models and offering only API-level access, with the underlying models hosted in hyper-secure public infrastructure."
    },
    {
      "rec_id": "rec_30",
      "action": "Create 'CERN for AI' or similar publicly-funded international research institutions coordinating compute allocation for safety",
      "actor": "International community",
      "target_timeline": "within 2 years",
      "urgency": "high",
      "goal": "provide neutral international infrastructure for collaborative AI safety research",
      "conditions": "unconditional",
      "rationale_summary": "The EU's 'CERN for AI' provided crucial infrastructure for coordinating safety research across national boundaries, offering a model for how public institutions can support international scientific cooperation on AI.",
      "quote": "A new EU research body, commonly referred to as 'CERN for AI' oversees these research programmes and coordinates compute allocation."
    }
  ]
}