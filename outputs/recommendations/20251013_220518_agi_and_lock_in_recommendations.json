{
  "recommendations": [
    {
      "rec_id": "rec_1",
      "action": "Avoid locking in bad, poorly chosen, or insufficiently flexible values",
      "actor": "Everyone with influence over AGI development and deployment",
      "target_timeline": "before and during AGI development",
      "urgency": "critical",
      "goal": "prevent existential catastrophes from permanent lock-in of harmful or inflexible value systems",
      "conditions": "unconditional",
      "rationale_summary": "The authors explicitly state that locking in bad values could constitute an existential catastrophe. Once locked in with AGI-enabled stability, such values could persist for millions or billions of years, making this a critical risk to avoid.",
      "quote": "Some lock-in events could constitute existential catastrophes, e.g. locking in bad values. These are important to avoid."
    },
    {
      "rec_id": "rec_2",
      "action": "Engage in careful reflection on how to make the long-term future as good as possible",
      "actor": "Society",
      "target_timeline": "now and ongoing",
      "urgency": "high",
      "goal": "ensure that if lock-in occurs, it leads to good long-term outcomes",
      "conditions": "unconditional",
      "rationale_summary": "The feasibility of ultra-stable institutions means significant influence over the long-run future is possible. This makes it crucial to reflect carefully now on what values and institutions should be preserved, before lock-in becomes technologically achievable.",
      "quote": "The possibility of ultra-stable institutions pursuing any of a wide variety of values, and the seeming generality of the methods that underlie them, suggest that significant influence over the long-run future is possible. This should inspire careful reflection on how to make it as good as possible."
    },
    {
      "rec_id": "rec_3",
      "action": "Build institutions that are stable in the face of everything except endorsed sources of change like democratic voting or moral reflection",
      "actor": "Institution builders",
      "target_timeline": "during AGI era",
      "urgency": "high",
      "goal": "preserve good values while maintaining flexibility for legitimate change",
      "conditions": "IF building stable institutions",
      "rationale_summary": "Complete lock-in without flexibility could be catastrophic, but some stability may be necessary to prevent bad values from being locked in. The solution is to design institutions that preserve stability while building in specific mechanisms for endorsed change.",
      "quote": "In particular, it seems plausibly good for an institution to be stable in the face of everything except for a few endorsed sources of change, such as democratic voting or moral reflection. Doing this might require some of the same procedures that would be used to lock-in more specific values, while it would simultaneously be important to avoid any parts that would prevent progress or otherwise be insufficiently flexible."
    },
    {
      "rec_id": "rec_4",
      "action": "Consider using stable institutions to enshrine basic human rights and humane values for the long term",
      "actor": "Governments and international community",
      "target_timeline": "before AGI enables lock-in",
      "urgency": "high",
      "goal": "ensure humane values and institutions like liberal democracy survive in the long-run",
      "conditions": "IF stable institutions are being built",
      "rationale_summary": "While lock-in poses risks, it could also be used positively to permanently protect important values. If humane values aren't stabilized, they could be displaced by other value systems or by Darwinian competition.",
      "quote": "there are nevertheless some things which might be good to make predictably true for a very long time (e.g. we might want to enshrine some basic human rights). Indeed, some degree of stability may be necessary to permanently preclude bad values from eventually being locked-in."
    },
    {
      "rec_id": "rec_5",
      "action": "Solve the AI alignment problem before AGI is widely deployed",
      "actor": "AI labs and AI safety researchers",
      "target_timeline": "before wide AGI deployment",
      "urgency": "critical",
      "goal": "prevent permanent human disempowerment from misaligned AGI",
      "conditions": "unconditional",
      "rationale_summary": "If alignment isn't solved before widespread AGI deployment, misaligned AI systems could cause an existential catastrophe. There may not be time to solve alignment after AGI is already deployed. Without aligned AI, stable human-directed institutions aren't possible.",
      "quote": "if the alignment problem isn't solved before AGI is widely deployed, there may not be time to solve it afterwards — since misaligned AGI could lead to an existential catastrophe soon thereafter"
    },
    {
      "rec_id": "rec_6",
      "action": "Invest sufficient time and resources in solving AI alignment rather than rushing deployment",
      "actor": "AI labs",
      "target_timeline": "during AGI development",
      "urgency": "critical",
      "goal": "achieve adequate solutions to AI alignment before deployment",
      "conditions": "unconditional",
      "rationale_summary": "The authors argue that alignment is likely solvable given sufficient time and effort. For stable institutions, only human-level alignment is needed (not superintelligence alignment), which is a simpler problem. Rushing deployment before solving alignment could be catastrophic.",
      "quote": "We think that this simpler version of the alignment problem is likely to be solvable, given enough time and investment."
    },
    {
      "rec_id": "rec_7",
      "action": "Design AI systems for interpretability to enable direct reading of their thoughts and understanding of their behavior",
      "actor": "AI developers",
      "target_timeline": "during AI development",
      "urgency": "high",
      "goal": "enable verification that AI systems are aligned and detect value drift",
      "conditions": "IF building AI for stable institutions OR IF prioritizing alignment",
      "rationale_summary": "Interpretability allows developers and supervisors to directly understand how AI would behave in various scenarios, making it much easier to ensure alignment and detect problems. This is one key advantage AI could have over relying on biological humans.",
      "quote": "AI systems could be designed for interpretability, perhaps allowing developers and supervisors to directly read their thoughts, and to directly understand how it would behave in a wide class of scenarios."
    },
    {
      "rec_id": "rec_8",
      "action": "Test AI behavior thoroughly in numerous simulated situations, including high-stakes scenarios designed to elicit problematic behavior",
      "actor": "AI developers and institution builders",
      "target_timeline": "during development and ongoing",
      "urgency": "high",
      "goal": "verify AI alignment and detect potential failures before deployment",
      "conditions": "IF deploying AI in critical roles",
      "rationale_summary": "With digital minds, it's possible to test AI in far more scenarios than humans, including high-stakes situations that would be too dangerous to test with real consequences. This enables much more thorough vetting than is possible with human employees.",
      "quote": "AI behavior can be thoroughly tested in numerous simulated situations, including high-stakes situations designed to elicit problematic behavior."
    },
    {
      "rec_id": "rec_9",
      "action": "Design AI systems to single-mindedly optimize for intended goals rather than having competing desires",
      "actor": "AI developers",
      "target_timeline": "during AI development",
      "urgency": "high",
      "goal": "prevent AI systems from pursuing goals other than the institution's intended values",
      "conditions": "IF building aligned AI",
      "rationale_summary": "Unlike humans who have many competing desires (survival, status, sexuality), AI systems could be designed with a single goal. This makes them more reliable agents for institutions, as they won't be distracted by personal interests that conflict with their mission.",
      "quote": "With sufficient understanding of how to induce particular goals, AI systems could be designed to more single-mindedly optimize for the intended goal, whereas most humans will always have some other desires, e.g. survival, status, or sexuality."
    },
    {
      "rec_id": "rec_10",
      "action": "Use digital error correction to preserve information about values and goals without any loss",
      "actor": "Institution builders",
      "target_timeline": "when implementing stable institutions",
      "urgency": "medium",
      "goal": "prevent corruption of stored values over millions or billions of years",
      "conditions": "IF building stable institutions",
      "rationale_summary": "Digital error correction can make storage and computation errors exponentially unlikely. This allows values to be perfectly preserved far longer than possible with biological memory or even traditional writing, which both suffer inevitable corruption.",
      "quote": "using digital error correction, it would be extremely unlikely that errors would be introduced even across millions or billions of years"
    },
    {
      "rec_id": "rec_11",
      "action": "Store all critical information, resources, and AI systems redundantly across many geographical locations",
      "actor": "Institution builders",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "ensure no local disaster can destroy the institution",
      "conditions": "IF building stable institutions",
      "rationale_summary": "Redundancy across many locations means only worldwide catastrophes or intelligent action could destroy the institution. Local disasters like earthquakes or meteor strikes would only destroy one copy, which could be replaced from other locations.",
      "quote": "values could be stored redundantly across many different locations, so that no local accident could destroy them. Wiping them all out would require either (i) a worldwide catastrophe, or (ii) intentional action."
    },
    {
      "rec_id": "rec_12",
      "action": "Store values as preserved whole-brain emulations or specially-trained AGI minds that can judge novel situations",
      "actor": "Institution builders",
      "target_timeline": "at institution founding",
      "urgency": "medium",
      "goal": "preserve highly nuanced specifications of values that can be applied to future dilemmas",
      "conditions": "IF building stable institutions with complex values",
      "rationale_summary": "Unlike writing which has low bandwidth, preserved minds can encode nuanced values in full detail. These minds can be queried about any future situation, allowing complex values to be applied correctly even in scenarios the founders never imagined.",
      "quote": "In the future, values could be directly stored in minds. Plausibly, whole-brain emulation (WBE) will be invented soon after AGI. If so, then it would be possible to preserve entire human minds, and query them about their views at any level of detail."
    },
    {
      "rec_id": "rec_13",
      "action": "Reset AI systems back to thoroughly-tested states frequently",
      "actor": "Institution builders",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "prevent value drift from accumulating as AI systems learn and interact with the world",
      "conditions": "IF building stable institutions AND IF concerned about value drift",
      "rationale_summary": "Even if individual AI systems might experience value drift from learning, frequently resetting them to known-good states prevents drift from accumulating. With digital minds, this is trivial to do, unlike with biological humans.",
      "quote": "For any tasks that didn't require high context over a long period of time, they could be frequently reset back to a well-tested state."
    },
    {
      "rec_id": "rec_14",
      "action": "Boot up completely-reset versions of AI systems from scratch for uncertain or high-stakes decisions",
      "actor": "Institution builders",
      "target_timeline": "as needed for major decisions",
      "urgency": "medium",
      "goal": "ensure important decisions reflect original values without any value drift",
      "conditions": "IF building stable institutions AND facing novel high-stakes decisions",
      "rationale_summary": "A freshly booted AI system has had no previous chance of value drift and only needs to be informed about prerequisites for the specific decision. This ensures critical choices are made according to the original values rather than drifted values.",
      "quote": "Whenever there's uncertainty about what to do in a novel situation, or a high-stakes decision needs to be made, the institution could boot-up a completely-reset version of an AI system (or a brain emulation) that acts according to the original values."
    },
    {
      "rec_id": "rec_15",
      "action": "Bring back multiple copies of AI judges and inform them in different ways, then have them discuss what the right decision is",
      "actor": "Institution builders",
      "target_timeline": "for high-stakes decisions",
      "urgency": "medium",
      "goal": "reduce contingency on how information is learned and reach more robust decisions",
      "conditions": "IF building stable institutions AND making important decisions",
      "rationale_summary": "If judgments are contingent on the order arguments are presented, this indicates concerning arbitrariness. Having multiple copies receive information differently and then discuss can reduce this contingency and reach more robust conclusions.",
      "quote": "In order to reduce contingency from how these prerequisites are learned, the institution could boot-up multiple copies and inform them in different ways — and also let some of the copies opine on how to inform the other copies. And then have them all discuss what the right option is."
    },
    {
      "rec_id": "rec_16",
      "action": "Have large numbers of AI systems with slightly-different backgrounds make independent judgments and take majority votes",
      "actor": "Institution builders",
      "target_timeline": "ongoing for decisions",
      "urgency": "medium",
      "goal": "eliminate random value drift while detecting systematic problems",
      "conditions": "IF building stable institutions",
      "rationale_summary": "Random drift across many AI systems would be uncorrelated and could be eliminated by majority voting. If systems systematically agree on something different from their original values, this flags a problem that needs investigation.",
      "quote": "Value drift that is effectively random could be eliminated by having a large number of AI systems with slightly-different backgrounds make an independent judgment about what the right decision is, and take the majority vote."
    },
    {
      "rec_id": "rec_17",
      "action": "Arrange AI systems into hierarchies of supervision where more reliable systems check that experienced systems aren't acting catastrophically",
      "actor": "Institution builders",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent AI systems that have accumulated context and experience from causing catastrophic harm if they've drifted",
      "conditions": "IF building stable institutions",
      "rationale_summary": "AI systems that have been operating longer are more at risk of value drift. Having them supervised by less-experienced but more-reliable systems, possibly with transparency tools to read their thoughts, can prevent catastrophic actions without requiring constant resets.",
      "quote": "If the task did require a larger amount of context over a longer period of time, they could be supervised and frequently re-tested by other AI systems with less context. These may not be able to correctly identify the value of the supervisee's every action, but they could prevent the supervisee from performing any catastrophic actions."
    },
    {
      "rec_id": "rec_18",
      "action": "Continuously test AI systems' motivations throughout their operation",
      "actor": "Institution builders",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "detect value drift or misalignment before AI systems can cause harm",
      "conditions": "IF building stable institutions",
      "rationale_summary": "Continuous testing allows detection of value drift early, before systems with drifted values accumulate enough power or context to cause serious problems. Testing can include behavioral tests in simulations and transparency-based examination of goals.",
      "quote": "Thoroughly test and understand how each system will behave after foreseeable inputs... Continuously check that each system does its job reasonably well, and in particular doesn't pose any danger to the institution as a whole"
    },
    {
      "rec_id": "rec_19",
      "action": "Design tests where AI systems genuinely cannot tell whether they are being tested, including high-stakes scenarios",
      "actor": "Institution builders and AI developers",
      "target_timeline": "during testing phases",
      "urgency": "high",
      "goal": "ensure AI systems reveal their true motivations rather than gaming tests",
      "conditions": "IF testing AI systems for deployment",
      "rationale_summary": "If AI systems know they're being tested, they might behave well during tests but badly during deployment. Testers who are at least as competent as the tested system can design tests with no detectable flaws, ensuring the AI reveals its true behavior.",
      "quote": "if there's any possible flaws in a test that would let the testee deduce that they were being tested, the testers would be aware of this, so that they could avoid such flaws. This means that the testers would know how the tested AI could deduce that it was being tested, which would allow them to design tests where the AI was genuinely unsure whether it was currently being tested or not."
    },
    {
      "rec_id": "rec_20",
      "action": "Use transparency tools to monitor AI systems' internal cognition and detect signs of misalignment or deceptive behavior",
      "actor": "AI developers and institution builders",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "directly verify AI goals and detect problems that behavioral testing might miss",
      "conditions": "IF building stable institutions with AI",
      "rationale_summary": "Transparency allows direct examination of whether an AI is trying to determine if it's being tested, whether it has concerning goals, or whether it's planning deceptive actions. This can detect problems that purely behavioral testing might miss, especially for inputs that are hard to test.",
      "quote": "Transparency tools could be used during the testing-procedures above... they could clarify why an AI system does what it does in the tested cases. For example, if it's trying to figure out whether it's in a test situation (presumably with an intention to behave differently depending on the answer) that's a red flag."
    },
    {
      "rec_id": "rec_21",
      "action": "Have less-informed AI systems supervise and test more-informed systems that have learned potentially-corrupting information",
      "actor": "Institution builders",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "prevent value drift from learning new information while still allowing necessary learning",
      "conditions": "IF building stable institutions AND allowing AI to learn",
      "rationale_summary": "If less-informed versions can recognize drift in more-informed versions (through summaries or transparency), they can supervise without being exposed to the same corrupting information. This creates a safety layer while still enabling learning.",
      "quote": "Insofar as less-informed systems can recognise drift, have more-informed interpreters be supervised and tested by less-informed versions of the same systems."
    },
    {
      "rec_id": "rec_22",
      "action": "Prevent AI systems from being exposed to information or arguments that systematically cause problematic behavior or value drift",
      "actor": "Institution builders",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "avoid situations where AI systems cannot reliably judge according to intended values",
      "conditions": "IF building stable institutions AND such problematic inputs are identified",
      "rationale_summary": "If testing reveals inputs that systematically cause AI systems to behave badly or drift in concerning ways, and these problems can't be fixed, a conservative institution can simply avoid exposing AI to such inputs. This sacrifices optimality but maintains safety.",
      "quote": "even in situations where it remains unclear what the desired behavior is, or in situations where it's somehow difficult to design a system that responds in the desired way, a sufficiently conservative institution could simply opt to prevent AI systems from being exposed to inputs like that"
    },
    {
      "rec_id": "rec_23",
      "action": "Control access to weapons of mass destruction and prevent their development by unauthorized actors",
      "actor": "Dominant institutions or governments",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "prevent catastrophic attacks that could destabilize institutions or destroy civilization",
      "conditions": "IF seeking stability OR preventing catastrophic risks",
      "rationale_summary": "WMDs are one of the few things that could threaten a geographically-distributed institution or cause worldwide catastrophe. Preventing their development and use is necessary both for stability and for avoiding existential risks more generally.",
      "quote": "people might want to permanently prevent the use of weapons so powerful that they could cause the collapse of civilization"
    },
    {
      "rec_id": "rec_24",
      "action": "Develop and maintain energy sources that do not depend on the sun",
      "actor": "Institutions seeking long-term stability",
      "target_timeline": "before major natural disasters",
      "urgency": "medium",
      "goal": "survive global catastrophes like asteroid impacts that block out the sun",
      "conditions": "IF building civilization resilient to natural disasters",
      "rationale_summary": "The main way asteroids and supervolcanoes cause global catastrophes is by blocking sunlight for years. An AI civilization could easily survive this with nuclear power or stored energy, unlike biological civilizations dependent on plant life.",
      "quote": "an AI civilization could easily survive such catastrophes by using energy-sources that don't depend on the sun (such as nuclear power or enough stored electrical or chemical energy to last for several years)"
    },
    {
      "rec_id": "rec_25",
      "action": "Spread to other solar systems within millions of years",
      "actor": "Civilization seeking long-term survival",
      "target_timeline": "within millions of years",
      "urgency": "low",
      "goal": "ensure survival beyond the eventual end of Earth",
      "conditions": "IF seeking stability beyond billions of years",
      "rationale_summary": "Earth will become uninhabitable in 1-2 billion years as the sun evolves. Spreading to other solar systems ensures survival beyond this point. While this seems distant, millions of years is enough time to accomplish this, so planning should begin much earlier.",
      "quote": "Due to the evolution of the sun, all eukaryotic life is predicted to die out 1-2 billion years from now... However, millions of years would be more than enough to spread to other solar systems"
    },
    {
      "rec_id": "rec_26",
      "action": "Delegate all tasks critical to institutional survival to stably-aligned AI systems rather than humans or unaligned AI",
      "actor": "Dominant institutions",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "ensure the institution cannot be undermined by human or AI inaction or sabotage",
      "conditions": "IF building dominant stable institution",
      "rationale_summary": "By using aligned AI for all essential functions, the institution eliminates dependence on potentially unreliable humans or unaligned AI. This means non-aligned actors cannot harm the institution through inaction, only through active harmful actions which can be monitored.",
      "quote": "Any task that is directly important to the institution's survival could be done by stably-aligned agents... By relying on stably-aligned agents for essential services, non-aligned members of the population could not significantly harm the dominant institution by inaction."
    },
    {
      "rec_id": "rec_27",
      "action": "Maintain control over cutting-edge AI technology and prevent development of superior AI systems by other actors",
      "actor": "Dominant institutions",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent other actors from developing AI systems that the institution's aligned AI cannot surveil or counter",
      "conditions": "IF building dominant stable institution",
      "rationale_summary": "An aligned AI system may not be able to effectively surveil a much more capable unaligned AI system, since it may not understand the intention or consequences of the more capable system's actions. Staying at the technological frontier prevents this problem.",
      "quote": "it may be difficult for an aligned AI system to surveil a different, unaligned AI system that was much more capable, at some tasks... Thus, the dominant institution might have to be on the cutting-edge of AI technology (possibly by prohibiting all superior forms of AI)."
    },
    {
      "rec_id": "rec_28",
      "action": "Control unauthorized space travel to prevent actors from escaping institutional control and building rival civilizations",
      "actor": "Dominant institutions",
      "target_timeline": "ongoing until space is fully controlled",
      "urgency": "medium",
      "goal": "prevent unauthorized actors from accessing resources to build competing institutions",
      "conditions": "IF building dominant stable institution AND before sending own colonization wave",
      "rationale_summary": "Anyone who escapes to space could build their own civilization using space resources until they pose a threat. This is only necessary until the institution has already sent its own colonization wave and controls accessible space resources.",
      "quote": "A dominant institution may also need to prevent unauthorized space travel, since anyone who left the institution's purview would be able to build their own civilization, until they posed a threat."
    },
    {
      "rec_id": "rec_29",
      "action": "Consider halting investigation of philosophical ideas that could cause value drift, if such ideas are identified",
      "actor": "Institution builders",
      "target_timeline": "if problematic ideas are identified",
      "urgency": "low",
      "goal": "prevent value drift from philosophical reflection that might change core values",
      "conditions": "ONLY IF building maximally conservative stable institution AND problematic philosophical ideas are identified",
      "rationale_summary": "If philosophical reflection systematically causes AI systems to drift from intended values in ways that cannot be resolved, an extremely conservative institution could halt such reflection. However, this sacrifices philosophical progress and the authors do not strongly endorse this approach.",
      "quote": "An extreme version of this would be to prevent all reasoning that could plausibly lead to value-drift, halting progress in philosophy."
    },
    {
      "rec_id": "rec_30",
      "action": "Gradually increase institutional stability over time rather than requiring perfect stability immediately",
      "actor": "Institution builders",
      "target_timeline": "ongoing over decades to centuries",
      "urgency": "medium",
      "goal": "make lock-in more feasible by spreading out costs and allowing time for deliberation",
      "conditions": "IF building stable institutions",
      "rationale_summary": "Perfect stability from day one would be expensive and require immediate decisions about what to lock in. Gradually increasing stability allows institutions to spread costs over time, accumulate knowledge about how to improve stability, and reflect on values before fully committing.",
      "quote": "It's also worth noting that an institution would not need to be perfectly stable from the beginning. Instead, it could gradually increase its stability over time... Initially, a dominant actor might just invest enough in stability that it could expect to stay stable for a few decades. Throughout those decades, it could gradually invest more in stability"
    }
  ]
}