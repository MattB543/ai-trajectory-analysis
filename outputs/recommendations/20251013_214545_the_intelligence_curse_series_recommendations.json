{
  "recommendations": [
    {
      "rec_id": "rec_1",
      "action": "Implement KYC (know-your-customer) and purchase-tracking tools for biological materials to provide oversight on purchases of potentially dangerous materials",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent AI-enabled bioterrorism and engineered pandemics",
      "conditions": "unconditional",
      "rationale_summary": "AI might make it easier to engineer pandemics. KYC infrastructure similar to anti-money laundering systems would provide visibility into who is purchasing dangerous biological materials, preventing bad actors from acquiring them.",
      "quote": "Methods for stopping pandemics from starting include: KYC (know-your-customer) and purchase-tracking tools to bring a high level of oversight to the purchase of potentially dangerous biological materials, similar to anti-money laundering infrastructure."
    },
    {
      "rec_id": "rec_2",
      "action": "Expand screening of orders from DNA synthesis providers to include AI estimation of pandemic potential for novel pathogens, not just known pathogens",
      "actor": "DNA synthesis providers",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent AI-enabled bioterrorism and engineered pandemics",
      "conditions": "unconditional",
      "rationale_summary": "Current screening is voluntary and focuses only on known pathogens. AI can estimate pandemic potential of novel sequences, catching dangerous orders that wouldn't be flagged under current systems.",
      "quote": "Screening of orders from DNA synthesis providers, which is currently a voluntary standard and mostly focused on known pathogens, but should expand to include AI estimation of the pandemic potential that would catch even novel pathogens."
    },
    {
      "rec_id": "rec_3",
      "action": "Deploy wastewater monitoring systems to detect pathogens that are increasing quickly",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "enable early detection and response to pandemic threats",
      "conditions": "unconditional",
      "rationale_summary": "Wastewater monitoring can detect pathogen spread early in a pandemic, providing critical time for response. This creates a biosecurity layer that doesn't depend on preventing attacks but can limit their damage.",
      "quote": "Wastewater monitoring to detect any pathogen that is increasing quickly."
    },
    {
      "rec_id": "rec_4",
      "action": "Install UV-C lighting in HVAC systems to kill airborne pathogens",
      "actor": "Private sector",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "slow pandemic spread and potentially eliminate infectious disease",
      "conditions": "unconditional",
      "rationale_summary": "UV-C in HVAC can kill pathogens circulating in air, similar to how filtration and chlorination ended waterborne disease epidemics. This provides a passive defense layer that works against any airborne pandemic.",
      "quote": "UV-C lighting in HVAC systems to kill pathogens that are circulating in the air, doing for air what filtration and chlorination did for the water supply in the late 1800s and early 1900s (ending typhoid, cholera, and dysentery epidemics)."
    },
    {
      "rec_id": "rec_5",
      "action": "Deploy triethylene glycol (haze) in high-risk sites like hospitals and ports to kill airborne pathogens",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "slow pandemic spread at critical chokepoints",
      "conditions": "IF a pandemic begins spreading",
      "rationale_summary": "Triethylene glycol is safe to breathe, kills pathogens effectively, and could be rapidly mass-produced since it's already a common chemical precursor. Deploying at high-risk sites could slow pandemic spread significantly.",
      "quote": "Haze (triethylene glycol) is safe to breathe and kills pathogens, potentially even more effectively than UV-C. It is a chemical precursor in a lot of supply chains, so it could be easy to mass-produce quickly in the event of a spreading pandemic if work on distribution is done ahead of time. Deploying it in high-risk sites like hospitals or ports could slow the spread of a pathogen."
    },
    {
      "rec_id": "rec_6",
      "action": "Implement formal verification of code at scale using AI",
      "actor": "AI labs",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "reduce cybersecurity vulnerabilities and prevent AI-enabled cyberattacks",
      "conditions": "unconditional",
      "rationale_summary": "Formal verification currently requires expensive bespoke mathematical work, but AI might make it feasible at scale. This would dramatically reduce technical vulnerabilities, especially in critical infrastructure code.",
      "quote": "Formal verification of code currently requires lots of bespoke mathematical work, but AI might make this feasible at scale."
    },
    {
      "rec_id": "rec_7",
      "action": "Use AI to rewrite legacy codebases from the ground up to be more secure and maintainable in 'The Great Refactor'",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "reduce cybersecurity vulnerabilities in legacy systems before AI-powered attacks exploit them",
      "conditions": "unconditional",
      "rationale_summary": "Legacy code maintained by organizations without deep technical competence is the biggest cyber risk from AI offense. Rewriting these codebases proactively using AI removes vulnerabilities before they can be exploited at scale.",
      "quote": "'The Great Refactor': using AI to rewrite many existing codebases from the ground up to be more secure and maintainable."
    },
    {
      "rec_id": "rec_8",
      "action": "Deploy AI-powered vulnerability detection methods including static analysis, fuzzing, and penetration testing",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "identify and patch vulnerabilities before AI-powered attacks can exploit them",
      "conditions": "unconditional",
      "rationale_summary": "AI can bring down the cost of classic vulnerability detection methods while adding human-like flexibility. Scaling these methods helps defenders keep pace with AI-powered offense.",
      "quote": "AI might bring down the cost of human-like flexibility in classic vulnerability detection methods like static analysis, fuzzing, and penetration testing."
    },
    {
      "rec_id": "rec_9",
      "action": "Develop and deploy tamper-proof chip enclosures to improve hardware security",
      "actor": "Private sector",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "protect against hardware-level attacks on AI systems and critical infrastructure",
      "conditions": "IF software vulnerabilities are patched OR nation-states target AI hardware",
      "rationale_summary": "As software becomes more secure, attacks will increasingly target hardware. Nation-states may also want to damage or spy on competing nations' AI hardware, making physical security more critical.",
      "quote": "Hardware security will matter more, as there will be more incentive to attack chips (especially if software vulnerabilities are patched through the above, or nation-state-level actors want to damage or spy on AI hardware of competing nations). Tamper-proof chip enclosures are one approach."
    },
    {
      "rec_id": "rec_10",
      "action": "Deploy LLM scanning of incoming messages to detect and prevent spear-phishing attacks",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "prevent AI-enabled social engineering attacks",
      "conditions": "unconditional",
      "rationale_summary": "Automated social engineering is already responsible for major cyber incidents. LLMs can scan incoming messages for signs of phishing at scale, defending against AI-powered social engineering offense.",
      "quote": "LLM scanning of incoming messages will help against spear-phishing."
    },
    {
      "rec_id": "rec_11",
      "action": "Use LLMs to monitor system logs for signs of cyberattack",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "enable faster detection of and response to cyberattacks",
      "conditions": "unconditional",
      "rationale_summary": "LLMs can make monitoring logs for attack signatures much easier and cheaper, enabling organizations to detect breaches faster and respond before major damage occurs.",
      "quote": "LLMs will make monitoring logs for signs of attack easier."
    },
    {
      "rec_id": "rec_12",
      "action": "Deploy AI to assist with fine-grained permission management in high-security organizations",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "improve both security and productivity in security-conscious organizations",
      "conditions": "unconditional",
      "rationale_summary": "Fine-grained permissions are currently a major complexity burden in high-security IT. AI can reduce this complexity while improving security, helping organizations like intelligence agencies and AI labs manage access better.",
      "quote": "AI can also help with fine-grained permission management, which is currently a major source of complexity in high security IT, improving both productivity and security at the most security-conscious organisations (e.g. intelligence agencies, the military, and hopefully AI labs)."
    },
    {
      "rec_id": "rec_13",
      "action": "Advance scalable oversight techniques for AI alignment including RLHF, weak-to-strong generalization, and AI safety via debate",
      "actor": "AI safety researchers",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "ensure AIs pursue the goals their creators give them and avoid rogue AI",
      "conditions": "unconditional",
      "rationale_summary": "Scalable oversight addresses how to give accurate feedback to powerful models, preventing incorrect or duplicitous behavior. This is foundational to ensuring AI systems remain aligned as they become more capable.",
      "quote": "Scalable oversight is about figuring out how to give accurate feedback to powerful models, to avoid incorrectly rewarding incorrect or duplicitous behavior from models. RLHF is an example; other work strands include weak-to-strong generalization and AI safety via debate."
    },
    {
      "rec_id": "rec_14",
      "action": "Advance interpretability research including mechanistic and developmental interpretability",
      "actor": "AI safety researchers",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "understand and verify AI behavior to prevent misalignment",
      "conditions": "unconditional",
      "rationale_summary": "Understanding what neural networks are doing internally could let us verify and steer model behavior. This provides a technical foundation for ensuring AI systems are actually aligned, not just appearing to be.",
      "quote": "Interpretability aims to understand what neural networks are doing, in hopes that this then lets us verify and/or steer model behavior. Mechanistic interpretability aims to understand the final trained models, while developmental interpretability studies how capabilities emerge during training."
    },
    {
      "rec_id": "rec_15",
      "action": "Develop automated alignment research capabilities to scale alignment work using AI",
      "actor": "AI safety researchers",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "solve alignment faster by using AI systems to help solve alignment",
      "conditions": "unconditional",
      "rationale_summary": "Automated alignment research aims to delegate alignment problems to AI systems themselves. This could dramatically accelerate alignment progress, though it requires solving the problem of trusting AI systems to do safety research.",
      "quote": "Automated alignment research aims to punt the above problems to the AIs."
    },
    {
      "rec_id": "rec_16",
      "action": "Implement AI control measures to ensure even misaligned AIs cannot cause harm",
      "actor": "AI labs",
      "target_timeline": "starting now",
      "urgency": "critical",
      "goal": "provide defense-in-depth against rogue AI by not assuming alignment",
      "conditions": "unconditional",
      "rationale_summary": "AI control takes a security mindset: assume minimal guarantees about AI behavior and build systems that maintain safety anyway. This is the correct stance until we have strong evidence that alignment works reliably.",
      "quote": "AI control aims to make sure that even misaligned AIs cannot cause havoc. It is in-line with a standard security mindset where you want security to hold even if you're making minimal assumptions about a system. This should be our stance towards AIs until we have good evidence on alignment."
    },
    {
      "rec_id": "rec_17",
      "action": "Establish government-supported moonshot projects for risk-reducing technologies modeled after Operation Warp Speed",
      "actor": "US Government",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "rapidly develop and deploy technologies that reduce catastrophic AI risks without requiring centralization",
      "conditions": "unconditional",
      "rationale_summary": "Many catastrophic risk mitigation technologies require coordinated development and deployment at scale. Government moonshots can accelerate this work, enabling decentralized approaches to AI safety rather than forcing centralization after a warning shot.",
      "quote": "Our key policy ask is for government-supported moonshot projects for the risk-reducing tech we outline above, modeled after Operation Warp Speed."
    },
    {
      "rec_id": "rec_18",
      "action": "Mandate KYC (know-your-customer) rules for DNA synthesis providers",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent AI-enabled bioterrorism",
      "conditions": "unconditional",
      "rationale_summary": "DNA synthesis is a key chokepoint for engineered pandemics. Government mandates can ensure universal compliance with KYC rules, whereas voluntary standards leave gaps that bad actors can exploit.",
      "quote": "This is especially true for biosecurity threats, which are the hardest for the private sector alone to solve. In particular, governments should mandate KYC (know-your-customer) rules for DNA synthesis providers."
    },
    {
      "rec_id": "rec_19",
      "action": "Fund wastewater monitoring systems for pathogen detection",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "enable early detection of pandemic threats",
      "conditions": "unconditional",
      "rationale_summary": "Wastewater monitoring provides early warning for pandemic spread but requires public infrastructure investment. Government funding can ensure comprehensive coverage that private actors won't provide.",
      "quote": "governments should mandate KYC (know-your-customer) rules for DNA synthesis providers, fund wastewater monitoring for pathogens, and ban gain-of-function research"
    },
    {
      "rec_id": "rec_20",
      "action": "Ban gain-of-function research that creates pandemic-potential pathogens in labs",
      "actor": "Governments",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "prevent lab-created pandemics",
      "conditions": "unconditional",
      "rationale_summary": "Gain-of-function research creates pandemic-potential pathogens for dubious information gain. Banning this research eliminates a major source of pandemic risk at minimal cost to beneficial research.",
      "quote": "governments should mandate KYC (know-your-customer) rules for DNA synthesis providers, fund wastewater monitoring for pathogens, and ban gain-of-function research—the creation of pandemic-potential pathogens in the lab for dubious information gain."
    },
    {
      "rec_id": "rec_21",
      "action": "Build AI user interfaces with Steve Jobs-level product design insight focused on human usability rather than agent autonomy",
      "actor": "AI labs",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "keep humans economically relevant by making human-AI collaboration more effective than pure automation",
      "conditions": "unconditional",
      "rationale_summary": "Current AI development focuses on autonomous agents rather than tools that augment humans. Better user interfaces would make human-AI teams more competitive, extending the period where humans remain economically relevant.",
      "quote": "Pro-human user interfaces. We have not yet seen Steve Jobs-level product insight and design applied to any AI tool. Effort is increasingly spent on developing AI agents rather than AI tools. This should change."
    },
    {
      "rec_id": "rec_22",
      "action": "Develop augmented reality tools that allow humans to receive high-bandwidth information from AIs while making decisions",
      "actor": "Private sector",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "increase AI-human bandwidth to make symbiotic human-AI systems more competitive",
      "conditions": "unconditional",
      "rationale_summary": "Higher bandwidth communication lets humans direct AIs faster and more carefully, making human-in-the-loop systems more competitive. AR provides visual information overlay without requiring humans to context-switch to screens.",
      "quote": "Increasing AI-human bandwidth and decreasing latency. This lets humans incorporate AIs more solidly into their workflows and direct them faster and more carefully, making symbiotic human-AI systems more competitive. Augmented reality tools could help humans make decisions and take actions while receiving information at a high rate from AIs."
    },
    {
      "rec_id": "rec_23",
      "action": "Develop non-invasive brain-computer interfaces (BCIs) to enable instantaneous human-to-AI feedback",
      "actor": "Private sector",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "allow humans to be effective overseers and managers of AIs through tight integration",
      "conditions": "unconditional",
      "rationale_summary": "BCIs enable instantaneous feedback between humans and AIs, allowing humans to oversee and direct AI systems much more effectively. Non-invasive BCIs reduce adoption barriers while still providing substantial bandwidth improvements.",
      "quote": "Brain-computer interfaces (BCIs). Instantaneous human-to-AI feedback via BCI allows humans to be effective overseers and managers of AIs, and integrate more tightly into human+AI systems. BCIs should be noninvasive to reduce adoption barriers."
    },
    {
      "rec_id": "rec_24",
      "action": "Make AI model finetuning easy so individuals and small businesses can create finetunes embodying their local knowledge, judgment, and taste",
      "actor": "AI labs",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "decentralize AI capabilities and keep humans economically relevant through their unique local knowledge",
      "conditions": "unconditional",
      "rationale_summary": "Local and personal knowledge/taste may remain important even as AI advances. Easy finetuning lets non-lab actors scale their local knowledge with AI productivity, keeping them relevant and preventing centralization in the labs.",
      "quote": "Easy finetuning of AI models so that people, small businesses, and startups can create AI finetunes that embody their local knowledge as well their personal judgement, taste, and sense of direction. To the extent that local and personal knowledge/taste is important, this will help non-lab actors stay relevant and competitive, by scaling their local knowledge and taste with AI productivity"
    },
    {
      "rec_id": "rec_25",
      "action": "Advance open-source robotics hardware and base models with easy task-specific finetuning",
      "actor": "AI safety researchers",
      "target_timeline": "before general robotics",
      "urgency": "medium",
      "goal": "decentralize robotics capabilities to prevent concentration of power in the physical world",
      "conditions": "IF robotics remains bottlenecked on task-specific data and finetuning",
      "rationale_summary": "Data is a major robotics bottleneck, and task-specific finetuning may remain necessary even with advanced AI. Open-source robotics with easy finetuning would distribute control over physical automation rather than centralizing it.",
      "quote": "Decentralized robotics. Data is a major bottleneck in robotics, and Moravec's paradox suggests that robotics might lag behind other AI capabilities. Robotics might remain based on task-specific finetuning, as is currently the case even for state-of-the-art deep learning-based robotics. This might create a world where the data and task-specific finetunes for manufacturing robots are distributed across many actors, rather than centralized into a small number of large companies, especially if we can push open-source robotics hardware and base models, and make robotics fine-tuning easy."
    },
    {
      "rec_id": "rec_26",
      "action": "Develop tools to help individuals and small businesses collect, manage, and protect their own data from centralized AIs",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "prevent data centralization and help individuals maintain data moats that keep them economically relevant",
      "conditions": "unconditional",
      "rationale_summary": "As AI handles information processing, economic value increasingly comes from controlling unique data rather than processing ability. Tools that help people control and protect their data prevent centralization and let them profit from their data assets.",
      "quote": "Helping humans own & control local data. If AI can cheaply do any valuable processing or deduction work when given some data, the ability to do intellectually valuable work will increasingly be bottlenecked by whether you can physically and legally give the required inputs to the AI, rather than by the information processing itself. Helping individuals and small businesses collect & manage their own data, and then protect that data from centralised AIs [...] would help the balance of power."
    },
    {
      "rec_id": "rec_27",
      "action": "Develop distributed training run infrastructure to allow decentralized groups to train AI models",
      "actor": "AI labs",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "decentralize AI development to prevent power concentration in a few labs",
      "conditions": "unconditional",
      "rationale_summary": "Currently only a handful of organizations can afford to train frontier AI models. Distributed training infrastructure could allow decentralized groups to pool resources and train competitive models, preventing lab oligopoly.",
      "quote": "Distributed training runs, such as what Prime Intellect is doing, might allow decentralized groups to train AI models."
    },
    {
      "rec_id": "rec_28",
      "action": "Develop confidential computing technologies and tools for distributed infrastructure to give individuals control over their compute",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "reduce cost and access barriers to controlling your own compute",
      "conditions": "WHILE GPUs remain expensive",
      "rationale_summary": "GPUs are expensive and efficient inference requires pooling many users. Confidential computing lets people use data centers with privacy guarantees, while distributed infrastructure tools let more players control their compute, reducing barriers to participation.",
      "quote": "Local compute for running powerful models. Much of this is downstream of GPU prices, and eventually we might hope for a GPU in every home, much as computers went from unaffordable to everyone owning one [...]. However, in the meantime performant GPUs are very expensive, and while some are trying, LLM inference is made cheap through maintaining high throughput by pooling requests from many users. Confidential computing technologies could let you run workloads on data centers with attestable security and privacy guarantees. Better tools for distributed infrastructure would allow a larger number of players to spin up their own compute clusters that they control, and reduce the cost barrier to controlling your compute."
    },
    {
      "rec_id": "rec_29",
      "action": "Accelerate development of cheap AI and especially open-source AI",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent pricing structures that benefit large organizations while making AI unaffordable for individuals",
      "conditions": "unconditional",
      "rationale_summary": "If capable AI costs $20,000/year, companies can replace employees but individuals can't benefit. Open-source AI puts price pressure on labs, preventing this disadvantageous pricing window from lasting long and keeping AI accessible.",
      "quote": "Cheap AI in general, especially open-source AI. A bad outcome is if, say, a system that can mostly substitute for some high-skill job costs ~$20,000/year—an amount that lets a company replace an employee, while making it hard for an individual human to benefit from it [...]. Open-weights and open-source AI in particular helps put price pressure on AI labs that prevents this state of affairs from lasting long."
    },
    {
      "rec_id": "rec_30",
      "action": "Develop AI alignment techniques that align models to individual users rather than generic human values",
      "actor": "AI safety researchers",
      "target_timeline": "before labor-replacing AI",
      "urgency": "high",
      "goal": "keep humans in the economic loop by ensuring AI agents earn income for specific people",
      "conditions": "unconditional",
      "rationale_summary": "Generic alignment to human values doesn't keep individuals economically relevant. User-specific alignment creates an economy of agents tied to individual people, where agents' activities earn income for their users and rely on user judgment and knowledge.",
      "quote": "Alignment to the user. Most alignment work prioritizes aligning to some generic concept of human values (or—and this is much more likely to happen by default—a corporate statement or political compromise). It assumes that instruction-following on behalf of the models is all the per-user specialization needed. However, we expect that for models to successfully act on users' behalf in most functions of the economy and the world will require their high-granularity, detailed alignment to each individual user. This could create an economy of agents, each of which is directly tied to one person."
    },
    {
      "rec_id": "rec_31",
      "action": "Deploy AI tutors to enable rapid upskilling and job transitions for workers",
      "actor": "Governments",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "help humans remain economically relevant by quickly learning skills AI hasn't automated",
      "conditions": "unconditional",
      "rationale_summary": "Economic changes from AI will come at unprecedented speed. AI tutors can enable faster upskilling than traditional education, helping humans stay ahead in the race between human retraining and AI capability expansion.",
      "quote": "Upskilling humans in the areas which will bottleneck the AI economy. AI systems are likely to have uneven capability profiles compared to humans, excelling in tasks with easy verification, low time horizons, and a lack of interfacing with the physical world. Naturally, these will create bottlenecks which humans will be able to fill to stay relevant in the economy. There is a race between human upskilling and retraining on one hand, and AI labs smoothing over the jagged performance frontier on the other. AI tutors for job changes. We expect that the changes to the economy will come at historically unprecedented speeds, and require faster upskilling than in the past."
    },
    {
      "rec_id": "rec_32",
      "action": "Develop and train humans in techniques for effective AI oversight",
      "actor": "Governments",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "keep humans economically relevant as AI overseers and managers",
      "conditions": "unconditional",
      "rationale_summary": "As AI handles more direct work, human value may increasingly come from oversight and direction. Finding and teaching good oversight techniques helps humans remain effective in these roles even as AIs become more capable.",
      "quote": "Finding good techniques for AI oversight and training humans in them."
    },
    {
      "rec_id": "rec_33",
      "action": "Fund educational experiments including new types of schools and educational programs",
      "actor": "Governments",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "prepare future generations for an AI-enabled economy",
      "conditions": "unconditional",
      "rationale_summary": "Current education teaches short-horizon, easily-gradable tasks—exactly what AI automates best. New educational approaches are needed to prepare humans for the skills that will remain valuable in an AI economy.",
      "quote": "Educational experiments, like new types of schools and educational programs. The current education system, which focuses on short-horizon, easily-gradable tasks, teaches exactly what AI automates."
    },
    {
      "rec_id": "rec_34",
      "action": "Improve forecasting of AI capabilities and their bottlenecks to predict what skills the economy will need",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable proactive rather than reactive workforce adaptation",
      "conditions": "unconditional",
      "rationale_summary": "Better forecasts of what AI can and cannot do would let people and institutions prepare for changes before they arrive, rather than scrambling to adapt after jobs are already automated.",
      "quote": "Better forecasting of AI capabilities and their bottlenecks. We need better forecasts and understanding of what the economy will need and is bottlenecked on."
    },
    {
      "rec_id": "rec_35",
      "action": "Ban AI systems from owning assets, serving as C-suite executives, sitting on boards of directors, or owning shares",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "ensure humans remain at the top of the economic hierarchy",
      "conditions": "unconditional",
      "rationale_summary": "Enshrining the principle that humans own the top of the economic funnel now, before AI systems are capable enough for companies to try delegating these roles, prevents a future where AIs control economic production independent of human ownership.",
      "quote": "Policymakers should ban AI systems from owning any assets, serving as a C-Suite member of a company, servicing on a board of directors, or owning shares. This sounds silly now, but it's important to enshrine a principle that humans own the top of the funnel now before systems are good enough for companies to try to delegate these roles."
    },
    {
      "rec_id": "rec_36",
      "action": "Build digital advocates that allow policymakers to assess the values and opinions of populations using user-aligned AI models",
      "actor": "Private sector",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "strengthen democratic institutions by giving policymakers deeper understanding of citizen preferences",
      "conditions": "unconditional",
      "rationale_summary": "Digital advocates based on individually-aligned AI models can give policymakers Tocqueville-level insight into voter preferences and conditions on any question, far beyond simple polling, strengthening the connection between citizens and government.",
      "quote": "Digital advocates (proposed by Kulveit & Douglas et al) that allow policymakers to assess the values and opinions of a given population. Models aligned to individual users in detail, as discussed in the diffusion section, naturally enable digital advocates."
    },
    {
      "rec_id": "rec_37",
      "action": "Develop large-scale feedback collection systems to give policymakers fine-grained qualitative data on citizen preferences",
      "actor": "Private sector",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "strengthen democratic institutions by improving government understanding of citizen needs",
      "conditions": "unconditional",
      "rationale_summary": "Current opinion polling provides only simple numerical data. Large-scale qualitative feedback collection using AI can give much richer understanding of citizen preferences, helping democratic institutions stay anchored to public needs as AI advances.",
      "quote": "Large-scale feedback collection that allows policymakers to get more fine-grained and qualitative data about citizens' preferences than current simple numerical opinion polling does. The AI Objectives Institute's Talk to the City project is an early example. Imagine a politician who can sit down with an AI, and, on any question, get a level of understanding about voters' preferences and conditions that was as if Tocqueville had spent a year travelling among the voters and then writing up an analysis."
    },
    {
      "rec_id": "rec_38",
      "action": "Deploy anonymized biometric verification tokens to enable human verification without compromising biometric privacy",
      "actor": "Private sector",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "enable gatekeeping of services and benefits to humans in a world of AI-generated personas",
      "conditions": "unconditional",
      "rationale_summary": "As AI makes impersonation and fraud cheap, human verification becomes essential for distributing government benefits, gatekeeping services, and distinguishing humans from AIs—but this must preserve privacy to avoid authoritarianism risks.",
      "quote": "Human verification is a useful primitive for many things, including gatekeeping services from online forums to company registration to humans, and distributing government benefits to citizens amid the sea of impersonation and fraud that AI will make cheap. For example, anonymized biometric verification tokens (like the World Network, formerly Worldcoin), aim to prove someone's humanity without passing on their biometrics."
    },
    {
      "rec_id": "rec_39",
      "action": "Develop AI systems as trusted third-party auditors with verifiable privacy and integrity guarantees",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "enable trusted verification and coordination without concentrating power in human auditors",
      "conditions": "unconditional",
      "rationale_summary": "Human auditors are expensive and hard to fully trust with sensitive information. AI auditors with verifiable privacy and integrity could enable everything from government transparency to corporate coordination to international arms control without requiring trust in specific humans.",
      "quote": "AI systems as trusted third-party auditors. It is difficult to trust a human auditor with sensitive information, and human auditors are expensive. AI auditors could have superhuman speed, cheapness, and reliability, and we might be able to have both verifiable privacy of the information they audit as well as of the auditor's integrity. Imagine for example being able to verifiably run a specific auditing program (in the simple case, an LLM prompt) against verifiably private information. This could help with anything from governments giving assurances to citizens, to companies coordinating with each other, to the verification of international arms-control treaties."
    },
    {
      "rec_id": "rec_40",
      "action": "Deploy AI systems as trusted third-party advisers that provide perspective without personal bias",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "low",
      "goal": "provide fair-minded arbitration in contexts where human advisers are seen as biased",
      "conditions": "unconditional",
      "rationale_summary": "Human advisers are often correctly seen as biased or self-serving. LLMs trained on humanity's collective texts provide something like a 'point-of-view from nowhere' that can serve as a more trusted adviser in contexts requiring fairness.",
      "quote": "AI systems as trusted third-party advisers. An issue with human advisers is that their perspective is often (correctly) seen as biased or self-serving. With LLMs, we have something like a 'point-of-view from nowhere'—an intelligence trained on the collected texts of humanity, without a personal agenda. 'ChatGPT said so' is already sometimes used as a proxy for a fair-minded arbiter."
    },
    {
      "rec_id": "rec_41",
      "action": "Build AI-powered platforms that automatically track and analyze government activities to uncover corruption",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "democratize oversight of government by giving citizens intelligence-agency-level analysis capabilities",
      "conditions": "unconditional",
      "rationale_summary": "AI can democratize the ability to track actors with intelligence-agency-level analysis. While this poses privacy risks to individuals, it can be used to track government actions and uncover corruption, strengthening democratic accountability.",
      "quote": "AI-powered tracking of government activities. AI could democratize the ability to have intelligence agency-level analysis and insight into a chosen actor. While this poses many privacy risks to individuals, society could use this to track government actions and uncover corruption. For example, imagine a platform on which AIs automatically collate information about which companies have lobbied for a bill, and what changes they're likely pushing for."
    },
    {
      "rec_id": "rec_42",
      "action": "Use AI to assist in contract negotiation between parties that previously found coordination too expensive",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "low",
      "goal": "enable coordination between parties that couldn't previously afford to negotiate",
      "conditions": "unconditional",
      "rationale_summary": "Complex multi-party contract negotiation is time-consuming and expensive. AI assistance could make previously prohibitive negotiations feasible, enabling coordination that improves welfare but wouldn't happen otherwise.",
      "quote": "Contract negotiation is time-consuming, especially when the matter is complex and there are multiple parties involved. AI could help parties that previously would've found it too time-consuming and expensive to coordinate to negotiate a contract."
    },
    {
      "rec_id": "rec_43",
      "action": "Develop automated AI-based enforcement of contracts where appropriate to help actors commit to actions",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "low",
      "goal": "reduce enforcement costs and enable commitments that improve coordination",
      "conditions": "unconditional",
      "rationale_summary": "Automated enforcement based on AI judgments can reduce transaction costs and help parties make credible commitments, enabling beneficial coordination that wouldn't happen with expensive human enforcement.",
      "quote": "Automated AI-based enforcement of contracts could be used—thoughtfully—to help actors commit to actions. Simple examples include bets resolving automatically based on AI judgements, or payments to a contractor triggering automatically on the satisfactory delivery of work."
    },
    {
      "rec_id": "rec_44",
      "action": "Deploy distributed fact-checking systems like X's Community Notes at scale across the information ecosystem",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "improve the information environment by providing crowdsourced fact-checking",
      "conditions": "unconditional",
      "rationale_summary": "A functional information environment is critical for democracy and decision-making. Distributed fact-checking systems like Community Notes provide crowdsourced verification at scale, helping counter misinformation without centralized control of truth.",
      "quote": "Distributed fact-checking systems like X's Community Notes at scale."
    },
    {
      "rec_id": "rec_45",
      "action": "Create 'Internet gloves' that let users selectively pull information from platforms without addictive engagement mechanisms",
      "actor": "Private sector",
      "target_timeline": "starting now",
      "urgency": "low",
      "goal": "improve the information environment by reducing addictive platform design",
      "conditions": "unconditional",
      "rationale_summary": "Platform addiction damages decision-making and culture. AI-powered tools that let users extract information selectively, without being drawn into addictive engagement loops, could improve information consumption while preserving user agency.",
      "quote": "'Internet gloves' where users can use AIs to pull information from platforms in selective, non-addictive ways, without being sucked into the platform."
    },
    {
      "rec_id": "rec_46",
      "action": "Pass campaign finance reform to strengthen democracy against the intelligence curse",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "strengthen democratic institutions to withstand the pressures of AGI",
      "conditions": "unconditional",
      "rationale_summary": "Weak democracies will crumble under AGI. Campaign finance reform reduces the influence of concentrated wealth on politics, helping democracies stay anchored to citizens rather than capital as the intelligence curse increases wealth concentration.",
      "quote": "Alongside this, policymakers should take immediate action to strengthen democracies. Weak democracies will crumble under the weight of AGI. This would include: Passing campaign finance reform"
    },
    {
      "rec_id": "rec_47",
      "action": "Reform anti-corruption laws to strengthen democratic institutions",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "strengthen democratic institutions to withstand the pressures of AGI",
      "conditions": "unconditional",
      "rationale_summary": "Weak democracies will crumble under AGI. Stronger anti-corruption laws help keep government responsive to citizens rather than special interests, which becomes more critical as the intelligence curse shifts economic power away from regular people.",
      "quote": "Alongside this, policymakers should take immediate action to strengthen democracies. Weak democracies will crumble under the weight of AGI. This would include: Passing campaign finance reform, Reforming anti-corruption laws"
    },
    {
      "rec_id": "rec_48",
      "action": "Strengthen bureaucratic competence while reducing bloat in government institutions",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "medium",
      "goal": "ensure government can effectively implement policies in a fast-moving AI world",
      "conditions": "unconditional",
      "rationale_summary": "Governments need to be able to understand AI developments and implement effective policies quickly. Competent, efficient bureaucracy is essential for this, while bloat slows response and wastes resources.",
      "quote": "Strengthen bureaucratic competence while reducing bloat"
    },
    {
      "rec_id": "rec_49",
      "action": "Reform courts and legislatures to operate faster to keep pace with AI-enabled executive action",
      "actor": "Governments",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent executive branch from becoming sole unchecked arbiter due to speed advantages",
      "conditions": "unconditional",
      "rationale_summary": "AI will enable executives to act very quickly. If courts and legislatures remain slow while executives gain AI-powered speed, separation of powers breaks down and executive branches become effectively unchecked authorities.",
      "quote": "Governments should make courts and legislatures faster. Coordination around legislatures and the processing times of court cases might be glacial compared to the speed of either AI advances, or to the speed at which an AI-enabled executive can act. This creates a threat that the executive branch can become effectively the sole and unchecked arbiter."
    },
    {
      "rec_id": "rec_50",
      "action": "Establish sovereign wealth fund with public ownership stakes in AI companies or constitutional requirements to meet basic needs",
      "actor": "Governments",
      "target_timeline": "before widespread unemployment",
      "urgency": "high",
      "goal": "prepare to distribute AI benefits to citizens if they become economically irrelevant",
      "conditions": "IF humans lose economic relevance",
      "rationale_summary": "If the intelligence curse materializes and people lose economic value, governments need mechanisms ready to distribute AI benefits. Sovereign wealth funds or constitutional guarantees ensure humans share in abundance even without market income.",
      "quote": "Governments should preemptively prepare for a world where lots of regular people don't provide immediate economic value, even if that never materializes or if some people still do. If this comes to pass, they should be ready to implement a myriad of measures to distribute AI's economic benefits to the disenfranchised. This could be a sovereign wealth fund with public ownership stakes in highly automated companies, with requirements to distribute a set percentage directly to citizens. It could also look like constitutional requirements that governments meet basic needs."
    },
    {
      "rec_id": "rec_51",
      "action": "Forecast AI capabilities and develop solutions to the intelligence curse",
      "actor": "Governments",
      "target_timeline": "starting now",
      "urgency": "critical",
      "goal": "prepare for and prevent the disempowerment of humanity",
      "conditions": "unconditional",
      "rationale_summary": "Governments have the authority and resources to shape the AGI transition. They must understand what's coming and prepare institutional and policy responses, or they will be caught unprepared when labor displacement begins.",
      "quote": "If you are in governments, you should be forecasting AI capabilities and thinking through solutions to the intelligence curse."
    },
    {
      "rec_id": "rec_52",
      "action": "Develop concrete policies designed to prepare society for post-AGI world",
      "actor": "Think tanks",
      "target_timeline": "starting now",
      "urgency": "critical",
      "goal": "provide actionable policy solutions for the intelligence curse",
      "conditions": "unconditional",
      "rationale_summary": "Think tanks bridge research and policy. They need to turn analysis of the intelligence curse into specific, implementable policies that governments can adopt before the crisis hits.",
      "quote": "If you're at a think tank, start turning out policies designed to get us ready for a post-AGI world."
    },
    {
      "rec_id": "rec_53",
      "action": "Critically examine your AI organization's incentives and build better internal governance structures",
      "actor": "AI labs",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "ensure AI labs don't inadvertently cause the intelligence curse through misaligned incentives",
      "conditions": "unconditional",
      "rationale_summary": "AI labs face strong competitive pressures and profit incentives that may drive them toward labor-replacing rather than human-augmenting AI. Better internal governance can help labs resist these pressures and choose better paths.",
      "quote": "If you're at an AI lab, critically examine your organizations' incentives and help build better internal governance structures to overcome them."
    },
    {
      "rec_id": "rec_54",
      "action": "Start companies building technology that keeps humans economically relevant and spreads abundance",
      "actor": "Entrepreneurs",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "create technologies that break the intelligence curse and keep humans in the loop",
      "conditions": "unconditional",
      "rationale_summary": "Traditional prestige career paths are closing. Young people should pursue ambitious entrepreneurship focused on human-augmenting AI, differential robotics development, and other technologies outlined in the diffusion section.",
      "quote": "If you are young, get ambitious. The traditional prestige paths are closing anyways. Start companies trying to design tech that will keep humans economically relevant and spread abundance."
    },
    {
      "rec_id": "rec_55",
      "action": "Fund projects and products that keep humans in charge and economically relevant",
      "actor": "Venture capitalists",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "direct capital toward technologies that break the intelligence curse",
      "conditions": "unconditional",
      "rationale_summary": "VCs control large pools of capital and shape what technologies get built. By funding human-augmenting rather than human-replacing AI, they can help steer the future toward maintaining human agency and relevance.",
      "quote": "If you're a VC, fund projects and products that will keep humans in charge."
    }
  ]
}