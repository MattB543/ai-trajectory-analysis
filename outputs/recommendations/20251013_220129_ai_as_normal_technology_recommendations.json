{
  "recommendations": [
    {
      "rec_id": "rec_1",
      "action": "Regulate new areas where AI is used in highly consequential ways as they emerge",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent accidents in safety-critical AI applications",
      "conditions": "unconditional",
      "rationale_summary": "As AI capabilities expand into new consequential domains, regulation must keep pace to prevent accidents. Historical examples like the Flash Crash show that new regulations (like circuit breakers) can effectively address emerging risks from automated systems.",
      "quote": "As and when new areas arise in which AI can be used in highly consequential ways, we can and must regulate them. A good example is the Flash Crash of 2010, in which automated high-frequency trading is thought to have played a part. This led to new curbs on trading, such as circuit breakers."
    },
    {
      "rec_id": "rec_2",
      "action": "Shift AI safety defenses from model-level restrictions to downstream attack surfaces where risks materialize",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent misuse",
      "conditions": "unconditional",
      "rationale_summary": "Model alignment is inherently brittle because whether a capability is harmful depends on context that models lack. Effective defenses must focus on the points where AI is actually deployed and misused, similar to how email filtering defends against phishing rather than restricting email composition.",
      "quote": "The primary defenses against misuse must be located downstream of models...Model-level safety controls will either be too restrictive (preventing beneficial uses) or will be ineffective against adversaries who can repurpose seemingly benign capabilities for harmful ends."
    },
    {
      "rec_id": "rec_3",
      "action": "Strengthen existing vulnerability detection programs rather than restricting AI capabilities",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "defend against AI-enabled cyberattacks",
      "conditions": "unconditional",
      "rationale_summary": "Concerns about bioweapons, cyber threats, and other dual-use risks are better addressed by strengthening existing defenses rather than AI-specific restrictions. These are fundamentally existing risks that AI may amplify, and the same defenses that work against human attackers can be adapted for AI-enabled attacks.",
      "quote": "Defending against AI-enabled cyberthreats requires strengthening existing vulnerability detection programs rather than attempting to restrict AI capabilities at the source. Similarly, concerns about bio risks of AI are best addressed at the procurement and screening stages for creating bioweapons."
    },
    {
      "rec_id": "rec_4",
      "action": "Focus on measuring and improving offense-defense balance rather than only offensive AI capabilities",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "enable more accurate risk assessment and effective mitigation",
      "conditions": "unconditional",
      "rationale_summary": "Measuring only offensive capabilities gives a misleading picture of AI risk. In many domains like cybersecurity, AI can strengthen defensive capabilities through automated vulnerability detection, potentially favoring defenders over attackers.",
      "quote": "Rather than measuring AI risk solely in terms of offensive capabilities, we should focus on metrics like the offense-defense balance in each domain. Furthermore, we should recognize that we have the agency to shift this balance favorably."
    },
    {
      "rec_id": "rec_5",
      "action": "Invest in defensive AI applications rather than attempting to restrict AI technology itself",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "shift offense-defense balance in favor of defenders",
      "conditions": "unconditional",
      "rationale_summary": "Powerful AI tools help defenders systematically probe their own systems and find vulnerabilities before attackers exploit them. Restricting AI development could backfire by giving motivated adversaries access to powerful tools while defenders lose access to the same capabilities.",
      "quote": "We should recognize that we have the agency to shift this balance favorably, and can do so by investing in defensive applications rather than attempting to restrict the technology itself...If we align language models so that they are useless at these tasks (such as finding bugs in critical cyber infrastructure), defenders will lose access to these powerful systems."
    },
    {
      "rec_id": "rec_6",
      "action": "Maintain pressure on policymakers to avoid AI safety races domestically and internationally",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent corners being cut on safety due to competitive pressures",
      "conditions": "unconditional",
      "rationale_summary": "While great-power conflict rhetoric focuses on AI development speed, the real risk is policymakers rushing to adopt AI haphazardly. The safety community must ensure competitive pressures don't lead to unsafe deployment in pursuit of economic or strategic advantage.",
      "quote": "The safety community should keep up the pressure on policymakers to ensure that this does not change. International cooperation must also play an important role."
    },
    {
      "rec_id": "rec_7",
      "action": "Increase international cooperation on AI safety and governance",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent international AI arms races and improve collective safety",
      "conditions": "unconditional",
      "rationale_summary": "International cooperation can help prevent races to the bottom on safety standards and enable sharing of lessons about effective AI governance approaches across different regulatory frameworks.",
      "quote": "The safety community should keep up the pressure on policymakers to ensure that this does not change. International cooperation must also play an important role."
    },
    {
      "rec_id": "rec_8",
      "action": "Prioritize reducing uncertainty about AI risks and trajectories as a first-rate policy goal",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable better-informed and more robust policymaking",
      "conditions": "unconditional",
      "rationale_summary": "Deep uncertainty about AI's future trajectory makes traditional risk analysis unreliable. By actively working to reduce uncertainty through evidence gathering, monitoring, and research, policymakers can make better decisions and avoid premature interventions based on ungrounded assumptions.",
      "quote": "We advocate for reducing uncertainty as a first-rate policy goal and resilience as the overarching approach to catastrophic risks."
    },
    {
      "rec_id": "rec_9",
      "action": "Adopt value pluralism in AI policymaking by preferring policies acceptable to stakeholders with diverse values",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "maintain legitimacy and avoid restrictions that reasonable people can reject",
      "conditions": "unconditional",
      "rationale_summary": "Unavoidable differences in values and beliefs about AI mean policymakers must avoid imposing controversial premises. Policies should be justifiable to those with different worldviews, especially when restricting freedoms, as this is essential for legitimacy in liberal democracies.",
      "quote": "Unavoidable differences in values and beliefs mean that policymakers must adopt value pluralism, preferring policies that are acceptable to stakeholders with a wide range of values, and attempt to avoid restrictions on freedom that can reasonably be rejected by stakeholders."
    },
    {
      "rec_id": "rec_10",
      "action": "Prioritize robustness when selecting AI policies by choosing interventions that remain helpful under different futures",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "avoid policies that make things worse if key assumptions prove wrong",
      "conditions": "unconditional",
      "rationale_summary": "Given deep uncertainty about AI's trajectory, policies should be robust to being wrong about key assumptions. Some interventions that might help contain superintelligence could exacerbate risks if AI develops as normal technology by increasing concentration and reducing resilience.",
      "quote": "They must also prioritize robustness, preferring policies that remain helpful, or at least not harmful, if the key assumptions underpinning them turn out to be incorrect."
    },
    {
      "rec_id": "rec_11",
      "action": "Increase funding for AI safety research that addresses risks from the normal technology perspective",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "broaden understanding of AI risks beyond harmful capabilities focus",
      "conditions": "unconditional",
      "rationale_summary": "Current AI safety research focuses heavily on harmful capabilities and superintelligence scenarios. There is insufficient attention to downstream questions about how threat actors actually use AI, real-world adoption patterns, and socio-political disruption risks that arise from normal use.",
      "quote": "Current AI safety research focuses heavily on harmful capabilities and does not embrace the normal technology view. Insufficient attention has been paid to questions that are downstream of technical capabilities...we advocate for increased funding of research on risks (and benefits) that tackles questions that are more relevant under the normal technology view."
    },
    {
      "rec_id": "rec_12",
      "action": "Implement evidence-seeking policies to monitor AI use, risks, and failures in the wild",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "reduce uncertainty about AI impacts and enable evidence-based policymaking",
      "conditions": "unconditional",
      "rationale_summary": "While research funding can help monitor AI deployment, systematic evidence gathering requires regulation. Policies that surface information about how AI is actually being used, misused, and causing harm are essential for understanding real-world impacts.",
      "quote": "While research funding can help with monitoring AI in the wild, it might also require regulation and policy—that is, 'evidence-seeking policies.'"
    },
    {
      "rec_id": "rec_13",
      "action": "Establish whistleblower protections for those reporting AI safety issues",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "surface information about dangerous AI applications and practices",
      "conditions": "unconditional",
      "rationale_summary": "Insiders at AI companies may have knowledge of dangerous applications or safety shortcuts that they cannot bring to light without protection. Whistleblower protections have proven effective in other domains like food safety and worker safety.",
      "quote": "Whistleblower protection: Insiders may have knowledge of dangerous applications that they cannot bring to light. Examples (including non-AI domains): Whistleblower protections for various types of safety such as food safety and worker safety"
    },
    {
      "rec_id": "rec_14",
      "action": "Require transparency reporting from AI deployers about system usage and misuse",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "bring to light how AI systems are being misused in practice",
      "conditions": "unconditional",
      "rationale_summary": "Deployers of AI systems like chatbots have extensive log data showing how they are being misused in the wild. Transparency reporting requirements can make this information available to researchers and policymakers, similar to social media transparency requirements.",
      "quote": "Transparency reporting requirement for deployers: Deployers of technologies such as chatbots have a wealth of log data showing how they are being misused in the wild. Examples (including non-AI domains): Social media transparency reporting requirements to bring to light the distribution of harmful content"
    },
    {
      "rec_id": "rec_15",
      "action": "Mandate government AI use inventories to track public sector AI deployment",
      "actor": "US Government",
      "target_timeline": "near-term",
      "urgency": "medium",
      "goal": "improve government transparency and public trust",
      "conditions": "unconditional",
      "rationale_summary": "Transparency about government use of AI helps build public trust and enables oversight. The 2020 U.S. Executive Order provides a model for this type of requirement.",
      "quote": "Government use inventories: Transparency of government to improve trust. Examples (including non-AI domains): 2020 U.S. Executive Order"
    },
    {
      "rec_id": "rec_16",
      "action": "Implement product registration requirements for AI systems",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "medium",
      "goal": "track the rate of AI deployment across sectors",
      "conditions": "unconditional",
      "rationale_summary": "Registration requirements enable tracking of how quickly AI systems are being deployed, which is crucial for understanding whether adoption is happening faster than governance can adapt. The FAA drone registration provides a model.",
      "quote": "Product registration: Tracking the rate of deployment. Examples (including non-AI domains): FAA drone registration requirement"
    },
    {
      "rec_id": "rec_17",
      "action": "Require incident reporting for AI failures and harms",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "enable case studies and statistical analyses to improve safety knowledge",
      "conditions": "unconditional",
      "rationale_summary": "Systematic incident reporting enables learning from failures and developing better safety practices. Workplace and road accident reporting requirements have proven effective in other domains for improving safety over time.",
      "quote": "Incident reporting: Enabling case studies and statistical analyses to improve safety knowledge. Examples (including non-AI domains): Workplace or road accident reporting requirements"
    },
    {
      "rec_id": "rec_18",
      "action": "Create safe harbor protections for red teaming and security research on deployed AI systems",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "incentivize research on vulnerabilities in deployed systems",
      "conditions": "unconditional",
      "rationale_summary": "Legal protections for researchers who probe AI systems for vulnerabilities incentivizes important safety research. The DMCA safe harbor for cybersecurity research provides a proven model for balancing security research with other legal concerns.",
      "quote": "Safe harbor for red teaming of deployed systems: Incentivizes research on vulnerabilities in the wild. Examples (including non-AI domains): DMCA safe harbor for cybersecurity research"
    },
    {
      "rec_id": "rec_19",
      "action": "Provide guidance to researchers on what kinds of evidence are useful and actionable for policy",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "improve quality and relevance of AI safety research",
      "conditions": "unconditional",
      "rationale_summary": "Policymakers can help researchers focus on generating evidence that is actually useful for decision-making. For example, guidance on the marginal risk framework for open vs. proprietary models has helped orient research efforts productively.",
      "quote": "Guidance on the value of different kinds of evidence. Policymakers can provide the research community with a better understanding of what kinds of evidence are useful and actionable. For example, various policymakers and advisory bodies have indicated the usefulness of the 'marginal risk' framework for analyzing the relative risks of open-weight and proprietary models."
    },
    {
      "rec_id": "rec_20",
      "action": "Consider impact on evidence gathering as a factor when evaluating any AI policy",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "ensure policies enable rather than hinder learning about AI impacts",
      "conditions": "unconditional",
      "rationale_summary": "Beyond policies specifically designed to gather evidence, any policy choice affects our ability to learn about AI. For example, open-weight models may advance safety research, while proprietary models may enable easier surveillance of use—both trade-offs worth considering.",
      "quote": "Evidence gathering as a first-rate goal. So far, we have discussed actions that are specifically intended to generate better evidence or to reduce uncertainty. More broadly, the impact on evidence gathering can be considered to be a factor in evaluating any AI policy, alongside the impact on maximizing benefits and minimizing risks."
    },
    {
      "rec_id": "rec_21",
      "action": "Improve readiness to change course on AI policy if the trajectory of AI changes",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "maintain policy flexibility in the face of uncertainty",
      "conditions": "unconditional",
      "rationale_summary": "Given uncertainty about whether AI will develop as normal technology or something more dangerous, policymakers should pursue resilience-promoting interventions but remain prepared to shift course if evidence suggests the trajectory is changing.",
      "quote": "We recommend that, for now, policymakers should cautiously pursue interventions in the final category as well, but should also improve their readiness to change course if the trajectory of AI changes."
    },
    {
      "rec_id": "rec_22",
      "action": "Redouble efforts to protect foundations of democracy weakened by AI, especially free press and equitable labor markets",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "build societal resilience against AI and other shocks",
      "conditions": "unconditional",
      "rationale_summary": "Strengthening democratic institutions helps society withstand shocks regardless of their source. Since AI weakens institutions like journalism and fair labor markets, protecting these foundations is crucial for resilience and will help regardless of how AI develops.",
      "quote": "Societal resilience, broadly: It is important to redouble efforts to protect the foundations of democracy, especially those weakened by AI, such as the free press and equitable labor markets. Advances in AI are not the only shocks, or even the only technology shocks, that modern societies face, so these policies will help regardless of the future of AI."
    },
    {
      "rec_id": "rec_23",
      "action": "Require transparency from developers of high-stakes AI systems",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "enable effective technical defenses and policymaking",
      "conditions": "unconditional",
      "rationale_summary": "Transparency requirements for AI developers strengthen both technical and institutional capacity to manage risks. This is a prerequisite for effective governance that will help build capacity to mitigate AI risks regardless of the ultimate trajectory.",
      "quote": "Prerequisites for effective technical defenses and policymaking: These interventions enable those in the next category by strengthening technical and institutional capacity. Examples include funding more research on AI risks, transparency requirements for developers of high-stakes AI systems..."
    },
    {
      "rec_id": "rec_24",
      "action": "Build trust and reduce fragmentation in the AI safety community",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "improve collective ability to address AI risks",
      "conditions": "unconditional",
      "rationale_summary": "The AI safety discourse has become polarized between different worldviews. Reducing fragmentation and improving understanding across camps strengthens the community's overall capacity to respond to AI risks, regardless of which worldview proves more accurate.",
      "quote": "Prerequisites for effective technical defenses and policymaking: These interventions enable those in the next category by strengthening technical and institutional capacity. Examples include...building trust and reducing fragmentation in the AI community..."
    },
    {
      "rec_id": "rec_25",
      "action": "Increase technical AI expertise in government",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "improve government capacity for effective AI policymaking",
      "conditions": "unconditional",
      "rationale_summary": "Technical expertise is necessary for government to understand AI risks, evaluate evidence, and design effective policies. This capacity building will help regardless of how AI develops.",
      "quote": "Prerequisites for effective technical defenses and policymaking: These interventions enable those in the next category by strengthening technical and institutional capacity. Examples include...increasing technical expertise in government..."
    },
    {
      "rec_id": "rec_26",
      "action": "Improve AI literacy among the general public",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "enable informed public participation in AI governance",
      "conditions": "unconditional",
      "rationale_summary": "AI literacy helps the public understand both opportunities and risks, enabling more informed democratic participation in governance decisions. This capacity building is valuable regardless of AI's ultimate trajectory.",
      "quote": "Prerequisites for effective technical defenses and policymaking: These interventions enable those in the next category by strengthening technical and institutional capacity. Examples include...improving AI literacy."
    },
    {
      "rec_id": "rec_27",
      "action": "Develop early warning systems for emerging AI risks",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "enable rapid response to emerging threats",
      "conditions": "unconditional",
      "rationale_summary": "Early warning systems can detect emerging risks before they become catastrophic, enabling proactive rather than reactive governance. This approach will help identify and respond to problems regardless of whether AI develops as expected.",
      "quote": "Interventions that would help regardless of the future of AI: These include developing early warning systems, developing defenses against identified AI risks..."
    },
    {
      "rec_id": "rec_28",
      "action": "Develop technical defenses against identified AI risks",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "mitigate known risks from AI deployment",
      "conditions": "unconditional",
      "rationale_summary": "Technical defenses against known risks like bias, privacy violations, and security vulnerabilities will be valuable regardless of AI's future trajectory. Research on control techniques, auditing, monitoring, and other safety methods provides concrete tools for safer deployment.",
      "quote": "Interventions that would help regardless of the future of AI: These include...developing defenses against identified AI risks..."
    },
    {
      "rec_id": "rec_29",
      "action": "Incentivize defenders such as software developers to adopt AI for security",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "improve offense-defense balance",
      "conditions": "unconditional",
      "rationale_summary": "Defenders can use AI to systematically find and fix vulnerabilities before attackers exploit them. Incentivizing defensive AI adoption helps shift the offense-defense balance favorably and will be valuable regardless of AI's trajectory.",
      "quote": "Interventions that would help regardless of the future of AI: These include...incentivizing defenders (such as software developers in the context of cyberattacks) to adopt AI..."
    },
    {
      "rec_id": "rec_30",
      "action": "Create legal protections for AI safety researchers",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "enable critical safety research without legal risk",
      "conditions": "unconditional",
      "rationale_summary": "Researchers need legal protection to probe AI systems for vulnerabilities and safety issues. Such protections enable important work that will be valuable regardless of how AI develops.",
      "quote": "Interventions that would help regardless of the future of AI: These include...legal protections for researchers..."
    },
    {
      "rec_id": "rec_31",
      "action": "Mandate adverse event reporting requirements for AI systems",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "high",
      "goal": "systematically learn from AI failures",
      "conditions": "unconditional",
      "rationale_summary": "Systematic reporting of adverse events enables learning from failures and developing better safety practices over time. This will help improve AI safety regardless of the technology's ultimate trajectory.",
      "quote": "Interventions that would help regardless of the future of AI: These include...adverse event reporting requirements..."
    },
    {
      "rec_id": "rec_32",
      "action": "Promote competition in AI markets",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent concentration of power and increase resilience",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Competition prevents single points of failure and power concentration. If AI develops as normal technology, concentrated control increases risks from both malicious use and errors. However, this could complicate control of superintelligence, requiring readiness to change course.",
      "quote": "Resilience-promoting interventions that will help if AI is normal technology but which might make it harder to control a potential superintelligent AI, such as promoting competition, including through open model releases..."
    },
    {
      "rec_id": "rec_33",
      "action": "Ensure AI capabilities are widely available for defensive purposes",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable defenders to counter AI-enabled threats",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Restricting AI capabilities could backfire by giving attackers access to powerful tools while defenders lose them. If AI is normal technology, widespread availability helps defenders more than attackers, but this requires monitoring in case AI trajectory changes.",
      "quote": "Resilience-promoting interventions that will help if AI is normal technology but which might make it harder to control a potential superintelligent AI, such as...ensuring AI is widely available for defense..."
    },
    {
      "rec_id": "rec_34",
      "action": "Pursue polycentricity in AI regulation with multiple diverse regulators rather than central control",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "increase resilience through regulatory diversity",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Multiple regulators with different approaches create resilience through diversity and experimentation. This polycentric approach has worked well in domains like self-driving cars, but requires monitoring in case centralized control becomes necessary.",
      "quote": "Resilience-promoting interventions that will help if AI is normal technology but which might make it harder to control a potential superintelligent AI, such as...polycentricity, which calls for diversifying the set of regulators and ideally introducing competition among them rather than putting one regulator in charge of everything."
    },
    {
      "rec_id": "rec_35",
      "action": "Reject nonproliferation-based safety measures for AI",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "avoid creating single points of failure and brittleness",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Nonproliferation creates concentration and single points of failure, making systems brittle to shocks. It introduces new risks like monoculture vulnerabilities while being practically unenforceable given widespread AI knowledge. Such measures decrease resilience if AI is normal technology.",
      "quote": "With limited exceptions, we believe that nonproliferation-based safety measures decrease resilience and thus worsen AI risks in the long run. They lead to design and implementation choices that potentially enable superintelligence in the sense of power—increasing levels of autonomy, organizational ability, access to resources, and the like."
    },
    {
      "rec_id": "rec_36",
      "action": "Avoid licensing requirements for AI model development",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent concentration and maintain resilience",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Licensing is impractical to enforce given widespread technical knowledge and reduces competition. It creates brittleness by concentrating expertise and capabilities, while motivated adversaries will simply ignore requirements. This makes risks worse under the normal technology view.",
      "quote": "Nonproliferation policies seek to limit the number of actors who can obtain powerful AI capabilities. Examples include...requiring licenses to build or distribute powerful AI...Enforcing nonproliferation has serious practical challenges. Malicious actors can simply ignore licensing requirements."
    },
    {
      "rec_id": "rec_37",
      "action": "Avoid prohibiting open-weight AI models",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "maintain wide availability for defense and avoid concentration",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Prohibiting open-weight models concentrates power and reduces safety research capacity while being ineffective since motivated actors can train their own models. Open models enable defensive uses and broader safety research, improving resilience under normal technology assumptions.",
      "quote": "Nonproliferation policies seek to limit the number of actors who can obtain powerful AI capabilities. Examples include...prohibiting open-weight AI models (since their further proliferation cannot be controlled)."
    },
    {
      "rec_id": "rec_38",
      "action": "Avoid using 'forgetting' techniques to remove dual-use capabilities from AI models",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "maintain defensive capabilities and avoid false sense of security",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Removing capabilities from models represents a nonproliferation mindset that decreases resilience. It removes defensive capabilities while motivated adversaries can train their own uncensored models. This creates an illusion of safety while actually worsening the offense-defense balance.",
      "quote": "The following are examples of nonproliferation-based interventions: Removing dual-use capabilities from models through 'forgetting' techniques...With limited exceptions, we believe that nonproliferation-based safety measures decrease resilience and thus worsen AI risks in the long run."
    },
    {
      "rec_id": "rec_39",
      "action": "Avoid curbing the ability of downstream developers to fine-tune models",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "enable beneficial customization and avoid concentration of control",
      "conditions": "IF AI develops as normal technology",
      "rationale_summary": "Restricting fine-tuning represents centralized control that decreases resilience and innovation. It prevents beneficial customization while being ineffective against determined adversaries who can train models from scratch or use other techniques.",
      "quote": "The following are examples of nonproliferation-based interventions: Curbing the ability of downstream developers to fine-tune models...With limited exceptions, we believe that nonproliferation-based safety measures decrease resilience and thus worsen AI risks in the long run."
    },
    {
      "rec_id": "rec_40",
      "action": "Design regulation to be sensitive to AI adoption needs and avoid prematurely freezing categories",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable beneficial AI adoption while maintaining safety",
      "conditions": "unconditional",
      "rationale_summary": "Realizing AI's benefits requires experimentation and reconfiguration. Regulation that is insensitive to these needs or prematurely freezes categories risks stymying beneficial adoption. For example, categorizing entire domains as 'high-risk' misses that variation within domains may exceed variation across them.",
      "quote": "Realizing the benefits of AI will require experimentation and reconfiguration. Regulation that is insensitive to these needs risks stymying beneficial AI adoption. Regulation tends to create or reify categories, and might thus prematurely freeze business models, forms of organization, product categories, and so forth."
    },
    {
      "rec_id": "rec_41",
      "action": "Use regulation to enable and promote AI diffusion, not just restrict it",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "maximize societal benefits from AI",
      "conditions": "unconditional",
      "rationale_summary": "Regulation versus diffusion is a false tradeoff. Regulation can actively promote beneficial adoption by providing legal clarity, building trust, and establishing clear rules. The ESIGN Act promoting e-commerce shows how regulation can enable technology diffusion.",
      "quote": "To be clear, regulation versus diffusion is a false tradeoff, just as is regulation versus innovation. None of the above examples are arguments against regulation; they only illustrate the need for nuance and flexibility. Moreover, regulation has a crucial role to play in enabling diffusion."
    },
    {
      "rec_id": "rec_42",
      "action": "Implement mandatory negotiation frameworks between AI companies and content publishers with regulatory oversight",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "medium",
      "goal": "protect publisher interests and enable beneficial AI-journalism integration",
      "conditions": "unconditional",
      "rationale_summary": "Media organizations' justified wariness of AI companies limits beneficial integration of journalism into AI systems. Power asymmetries lead to exploitative deals. Mandatory negotiation with oversight can enable fair agreements while protecting both publishers and enabling AI diffusion.",
      "quote": "The incorporation of journalistic and media content into chatbots and other AI interfaces is limited by media organizations' justified wariness of AI companies...Various models for mandatory negotiation with regulatory oversight are possible."
    },
    {
      "rec_id": "rec_43",
      "action": "Provide regulatory clarity in areas of legal uncertainty around AI to promote adoption",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "medium",
      "goal": "enable beneficial AI adoption by reducing legal uncertainty",
      "conditions": "unconditional",
      "rationale_summary": "Legal uncertainty chills beneficial adoption. Clear rules and requirements can actually spur adoption by giving developers and users confidence. The FAA's 2016 drone regulations show how clarity can lead to rapid growth in registered devices and new use cases.",
      "quote": "In areas in which there is legal or regulatory uncertainty, regulation can promote diffusion. The application of liability laws to AI is often unclear...The resulting clarity spurred adoption and led to a rapid rise in the number of registered drones, certified pilots, and use cases across different industries."
    },
    {
      "rec_id": "rec_44",
      "action": "Invest in AI literacy and workforce training in both public and private sectors",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "promote AI diffusion by building human capital",
      "conditions": "unconditional",
      "rationale_summary": "As automation increases, complements to automation become more valuable. AI literacy and training are public goods that private sector will underinvest in. Government investment helps workers adapt and enables organizations to effectively use AI.",
      "quote": "One powerful strategy for promoting AI diffusion is investing in the complements of automation, which are things that become more valuable or necessary as automation increases. One example is promoting AI literacy as well as workforce training in both the public and the private sectors."
    },
    {
      "rec_id": "rec_45",
      "action": "Promote digitization and open government data to enable AI applications",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "enable AI users to benefit from previously inaccessible datasets",
      "conditions": "unconditional",
      "rationale_summary": "Open government data is a complement to automation that enables AI applications. It's a public good that benefits everyone, so private sector will underinvest. Government investment in digitization and open data platforms promotes beneficial AI adoption.",
      "quote": "Another example is digitization and open data, especially open government data, which can allow AI users to benefit from previously inaccessible datasets. The private sector will be likely to underinvest in these areas as they are public goods that everyone can benefit from."
    },
    {
      "rec_id": "rec_46",
      "action": "Improve energy infrastructure including grid reliability",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "promote both AI innovation and diffusion",
      "conditions": "unconditional",
      "rationale_summary": "Energy infrastructure is a complement to AI that helps with both training and inference. Improvements to grid reliability benefit AI development and deployment, making this a valuable investment for promoting AI's benefits.",
      "quote": "Improvements to energy infrastructure, such as the reliability of the grid, will promote both AI innovation and diffusion since it will help in both AI training and inference."
    },
    {
      "rec_id": "rec_47",
      "action": "Redistribute AI benefits to make them more equitable across society",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "reduce inequality and public anxiety about AI",
      "conditions": "unconditional",
      "rationale_summary": "If AI is normal technology, the main risks are socio-political disruption like inequality rather than existential threats. Government must actively redistribute benefits to counteract AI's tendency to concentrate gains, reducing inequality and public resistance.",
      "quote": "Governments also have an important role to play in redistributing the benefits of AI to make them more equitable and in compensating those who stand to lose as a result of automation."
    },
    {
      "rec_id": "rec_48",
      "action": "Compensate workers and sectors that lose from AI automation",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "address job losses and reduce social disruption",
      "conditions": "unconditional",
      "rationale_summary": "Some occupations and sectors will experience significant job losses from AI. Government should provide compensation and support for those displaced, both for equity and to reduce social disruption and backlash against beneficial AI adoption.",
      "quote": "Governments also have an important role to play in redistributing the benefits of AI to make them more equitable and in compensating those who stand to lose as a result of automation."
    },
    {
      "rec_id": "rec_49",
      "action": "Strengthen social safety nets to decrease public anxiety about AI",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "reduce public anxiety and enable beneficial AI adoption",
      "conditions": "unconditional",
      "rationale_summary": "High levels of public anxiety about AI in many countries stem from economic insecurity. Stronger social safety nets provide a buffer against job displacement, reducing anxiety and resistance to beneficial AI adoption.",
      "quote": "Strengthening social safety nets will help to decrease the currently high levels of public anxiety about AI in many countries."
    },
    {
      "rec_id": "rec_50",
      "action": "Fund arts and journalism through taxes on AI companies",
      "actor": "Governments",
      "target_timeline": "near-term",
      "urgency": "medium",
      "goal": "protect vital sectors harmed by AI",
      "conditions": "unconditional",
      "rationale_summary": "Arts and journalism are vital spheres of life that have been harmed by AI. These are public goods that merit government support. Taxing AI companies that benefit from these sectors and using funds to support them addresses both harm and funding needs.",
      "quote": "The arts and journalism are vital spheres of life that have been harmed by AI. Governments should consider funding them through taxes on AI companies."
    },
    {
      "rec_id": "rec_51",
      "action": "Balance speed of public sector AI adoption to maintain trust while capturing benefits",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable effective government services without losing legitimacy",
      "conditions": "unconditional",
      "rationale_summary": "Moving too quickly on AI adoption leads to failures that damage trust and legitimacy. But moving too slowly means basic functions get outsourced to less accountable private sector. Government must find a balance, avoiding both reckless adoption and excessive caution.",
      "quote": "Governments should strike a fine balance in terms of the public sector adoption of AI. Moving too quickly will lead to a loss of trust and legitimacy...But moving too slowly might mean that basic government functions are outsourced to the private sector where they are implemented with less accountability."
    },
    {
      "rec_id": "rec_52",
      "action": "Reduce overly cautious proceduralism in government AI deployment to avoid incompetent performance",
      "actor": "US Government",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "maintain government legitimacy and effectiveness",
      "conditions": "unconditional",
      "rationale_summary": "The administrative state's procedure fetish creates runaway bureaucracy that prevents beneficial AI use. This risks making government appear incompetent, undermining the very legitimacy that excessive caution seeks to protect. Balanced risk-taking is necessary for effective governance.",
      "quote": "The administrative state's approach to these risks is overly cautious and has been described by Nicholas Bagley as a 'procedure fetish,' potentially leading to a 'runaway bureaucracy.' In addition to losing out on the benefits of AI, Bagley cautioned that incompetent performance will lead to government agencies losing the very legitimacy that they seek to gain through their emphasis on procedure and accountability."
    },
    {
      "rec_id": "rec_53",
      "action": "Use uplift studies and economic indicators rather than benchmarks to measure AI progress and impact",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "accurately assess real-world AI utility and impact",
      "conditions": "unconditional",
      "rationale_summary": "Benchmarks measure progress in AI methods but have poor construct validity for real-world utility. They overemphasize what models are good at and underemphasize complex contextual work. Only building applications and testing with professionals in realistic scenarios reveals actual impact.",
      "quote": "The only sure way to measure real-world usefulness of a potential application is to actually build the application and to then test it with professionals in realistic scenarios (either substituting or augmenting their labor, depending on the intended use). Such 'uplift' studies generally do show that professionals in many occupations benefit from existing AI systems."
    },
    {
      "rec_id": "rec_54",
      "action": "Regulate AI adoption sector-specifically rather than through horizontal AI-only regulation",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "address safety races through effective regulation",
      "conditions": "unconditional",
      "rationale_summary": "AI arms races are sector-specific and should be addressed through sector-specific regulations that force companies to internalize safety costs. Self-driving cars, aviation, and social media show varied patterns requiring different approaches. Most high-risk sectors are already heavily regulated.",
      "quote": "In short, AI arms races might happen, but they are sector specific, and should be addressed through sector-specific regulations...As we pointed out in the earlier parts, most high-risk sectors are heavily regulated in ways that apply regardless of whether or not AI is used."
    },
    {
      "rec_id": "rec_55",
      "action": "Emphasize proactive evidence gathering and transparency in emerging AI-driven sectors",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent arms races by enabling attribution and accountability",
      "conditions": "unconditional",
      "rationale_summary": "Arms races are more likely when harms are hard to attribute to product failures, as with social media algorithms. Proactive evidence gathering and transparency requirements help make attribution clearer, enabling market and regulatory forces to work effectively.",
      "quote": "This shows the importance of proactive evidence gathering and transparency in emerging AI-driven sectors and applications. We address this in Part IV. It also shows the importance of 'anticipatory AI ethics'—identifying ethical issues as early as possible in the lifecycle of emerging technologies, developing norms and standards."
    },
    {
      "rec_id": "rec_56",
      "action": "Develop and promote alternative AI system designs less susceptible to misalignment",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "reduce specification gaming and misalignment risks",
      "conditions": "unconditional",
      "rationale_summary": "Some design decisions are more prone to misalignment, particularly reinforcement learning optimizing single objectives over long horizons. Research on alternative paradigms that are less susceptible to specification gaming is important, though misalignment is more an engineering problem than existential threat.",
      "quote": "One setting that is notorious for this is the use of reinforcement learning to optimize a single objective function (which might be accidentally underspecified or misspecified) over a long time horizon...In any case, research on alternative design paradigms that are less susceptible to specification gaming is an important research direction."
    },
    {
      "rec_id": "rec_57",
      "action": "Adopt resilience as the overarching approach to catastrophic AI risks",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "minimize severity and duration of harm when it occurs",
      "conditions": "unconditional",
      "rationale_summary": "Ex ante approaches like risk analysis and precaution are poorly suited to AI due to difficulty of ascertaining risks in advance. Resilience—taking actions now to minimize harm when shocks occur—is better suited to AI's uncertainty and combines ex ante and ex post elements.",
      "quote": "We advocate for reducing uncertainty as a first-rate policy goal and resilience as the overarching approach to catastrophic risks...Marchant and Stevens argued (and we agree) that ex ante approaches are poorly suited to AI because of the difficulty of ascertaining risks in advance of deployment."
    },
    {
      "rec_id": "rec_58",
      "action": "Track AI adoption rate in consequential tasks as a key metric for governance",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "monitor whether adoption is outpacing governance capacity",
      "conditions": "unconditional",
      "rationale_summary": "One concern is that rapid AI adoption could prevent regulators from intervening in time. So far, adoption in consequential tasks has been slow even without regulation, but this could change. The adoption rate is a crucial metric to track for determining if the governance approach needs adjustment.",
      "quote": "One reason why safety regulation might be harder in the case of AI is if adoption is so rapid that regulators will not be able to intervene until it is too late. So far, we have not seen examples of rapid AI adoption in consequential tasks, even in the absence of regulation...The adoption rate of new AI applications will remain a key metric to track."
    }
  ]
}