{
  "recommendations": [
    {
      "rec_id": "rec_1",
      "action": "Establish rules in model specs that AI systems follow the law and refuse to assist with coups",
      "actor": "AI developers",
      "target_timeline": "before AI systems can meaningfully assist with coups",
      "urgency": "critical",
      "goal": "prevent AI systems from assisting with coups by establishing behavioral constraints",
      "conditions": "unconditional",
      "rationale_summary": "Model specs directly constrain AI behavior. Rules requiring legal compliance and refusing coup assistance create a first line of defense against AI-enabled coups, though they must be technically enforced.",
      "quote": "Establish rules in model specs (documents that describe intended model behaviour) and terms of service for government contracts. These should include rules that AI systems follow the law, and that AI R&D systems refuse to assist attempts to circumvent security or insert secret loyalties."
    },
    {
      "rec_id": "rec_2",
      "action": "Establish rules that AI R&D systems refuse to assist attempts to circumvent security or insert secret loyalties",
      "actor": "AI developers",
      "target_timeline": "before AI can automate AI R&D",
      "urgency": "critical",
      "goal": "prevent creation of secretly loyal AI systems",
      "conditions": "unconditional",
      "rationale_summary": "Once AI can automate R&D, a CEO could direct AI systems to insert secret loyalties without human oversight. Rules preventing AI from assisting with security circumvention or secret loyalty insertion are essential safeguards.",
      "quote": "AI systems should be trained to refuse assistance with attempts to circumvent security measures or insert secret loyalties. Internal AI usage should also be logged and reviewed for evidence about whether such tampering was attempted."
    },
    {
      "rec_id": "rec_3",
      "action": "Improve adherence to model specs through extensive red-teaming by multiple independent groups",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "ensure AI systems robustly follow model specs and don't assist with coups",
      "conditions": "unconditional",
      "rationale_summary": "Model specs are only effective if AI systems actually follow them. Multiple independent red teams can discover edge cases and failure modes that single teams would miss, improving robustness against coup-related misuse.",
      "quote": "To ensure that these guardrails are robust, AI projects should carefully measure and continuously improve the extent to which AI systems comply with model specs... This red-teaming should be done by multiple independent groups."
    },
    {
      "rec_id": "rec_4",
      "action": "Audit AI models for secret loyalties by scrutinizing models, training data, and training code",
      "actor": "AI developers",
      "target_timeline": "before deploying AI in critical systems",
      "urgency": "critical",
      "goal": "detect secret loyalties before AI systems are deployed in military or government",
      "conditions": "unconditional",
      "rationale_summary": "Secret loyalties are a key pathway to coups. Even if models appear to follow specs, they could be deliberately trained to hide objectives. Alignment audits with full access to internals are necessary to detect sophisticated deception.",
      "quote": "An alignment audit is an investigation into whether models have hidden objectives... AI projects should grant auditors comprehensive access to model internals and training data."
    },
    {
      "rec_id": "rec_5",
      "action": "Implement strong infosecurity robust against senior executives to prevent unauthorized access and secret loyalty insertion",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "critical",
      "goal": "prevent unauthorized access to guardrail-free models and prevent insertion of secret loyalties",
      "conditions": "unconditional",
      "rationale_summary": "The greatest insider threat comes from senior executives who might demand access to guardrail-free models or insert secret loyalties. Security measures must be robust even to the most senior insiders to prevent coups.",
      "quote": "The most significant threat comes from insiders, especially senior executives within AI projects who might have or demand permissions that they could use to access guardrail free models or insert secret loyalties. Security measures must therefore be robust to even the most senior insiders."
    },
    {
      "rec_id": "rec_6",
      "action": "Share information about AI capabilities, compute usage, model specs, and safeguards with multiple stakeholders",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable multiple actors to notice risks and intervene to prevent coups",
      "conditions": "unconditional",
      "rationale_summary": "Transparency allows external actors to assess coup risk, hold AI projects accountable, and take preventive action. Information sharing with multiple stakeholders prevents any single actor from hiding dangerous capabilities or inadequate safeguards.",
      "quote": "AI projects should share information about: AI capabilities... How AI is used... Model specs... Safeguards against AI-enabled coups... Risk assessment."
    },
    {
      "rec_id": "rec_7",
      "action": "Share AI capabilities with multiple independent stakeholders to prevent exclusive access",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent a small group from gaining exclusive access to coup-enabling capabilities",
      "conditions": "unconditional",
      "rationale_summary": "Exclusive access to superhuman capabilities in weapons development, strategy, or cyber offense creates overwhelming power asymmetries that enable coups. Sharing capabilities with multiple actors maintains checks and balances.",
      "quote": "Another crucial way to empower multiple actors is to share access to frontier AI capabilities widely. This reduces the risk that a single person or small group gains exclusive access to the most powerful capabilities and leverages them to stage a coup."
    },
    {
      "rec_id": "rec_8",
      "action": "Require AI developers to implement technical measures through procurement terms, regulation, and legislation",
      "actor": "Governments",
      "target_timeline": "starting now",
      "urgency": "critical",
      "goal": "ensure AI developers implement protections against AI-enabled coups",
      "conditions": "unconditional",
      "rationale_summary": "Governments have leverage through procurement, regulation, and legislation to mandate that AI developers implement safeguards. Government requirements ensure protections are not voluntary and cannot be easily removed.",
      "quote": "We recommend that governments: Require AI developers to implement the mitigations above, through terms of procurement, regulation, and legislation."
    },
    {
      "rec_id": "rec_9",
      "action": "Increase oversight over frontier AI projects by building technical capacity in executive and legislative branches",
      "actor": "Governments",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "enable government to understand and regulate AI development to prevent coups",
      "conditions": "unconditional",
      "rationale_summary": "Without technical capacity, governments cannot effectively oversee AI development or assess coup risks. Building expertise in both branches ensures distributed oversight that is harder for any single actor to circumvent.",
      "quote": "Increase oversight over frontier AI projects, including by building technical capacity within both the executive and the legislature."
    },
    {
      "rec_id": "rec_10",
      "action": "Establish rules that government AI should not serve partisan interests",
      "actor": "Governments",
      "target_timeline": "before deploying AI in government",
      "urgency": "critical",
      "goal": "prevent AI-enabled backsliding and executive coups",
      "conditions": "unconditional",
      "rationale_summary": "AI that serves partisan interests rather than the public good enables heads of state to consolidate power through conventional backsliding. Rules requiring government AI to serve legitimate state functions prevent this pathway to authoritarian control.",
      "quote": "Government AI should not serve partisan interests... Government use of advanced AI capabilities must be limited to legitimate state functions, subject to multi-stakeholder oversight, and should not consolidate the power of ruling officials."
    },
    {
      "rec_id": "rec_11",
      "action": "Procure military AI systems from multiple independent providers",
      "actor": "Governments",
      "target_timeline": "before deploying autonomous military AI",
      "urgency": "critical",
      "goal": "prevent secret loyalties in military AI systems from enabling coups",
      "conditions": "unconditional",
      "rationale_summary": "If all military AI comes from one provider, that provider could insert secret loyalties into all systems. Multiple providers create redundancy—if one provider's systems are compromised, others can detect and counter coup attempts.",
      "quote": "Military AI systems should not be developed using a single provider... There should be multiple providers of autonomous military AI systems, such that if any one provider had subverted their systems, this would be insufficient to enable a coup."
    },
    {
      "rec_id": "rec_12",
      "action": "Establish principles that no single person should direct enough military AI systems to stage a coup",
      "actor": "Governments",
      "target_timeline": "before deploying autonomous military AI",
      "urgency": "critical",
      "goal": "prevent coups via singular loyalties to military leaders",
      "conditions": "unconditional",
      "rationale_summary": "Autonomous weapons that follow orders from a single person enable that person to stage a coup without human support. Command structures requiring multiple approvals or maintaining human chains of command preserve traditional checks on military power.",
      "quote": "Control over military AI systems should be distributed... Military command structures should be designed such that no single individual can control enough military AI systems to stage a coup."
    },
    {
      "rec_id": "rec_13",
      "action": "Coup-proof any plans for a centralized AI project and avoid centralization unless necessary",
      "actor": "Governments",
      "target_timeline": "before any centralization of AI development",
      "urgency": "critical",
      "goal": "prevent centralized AI projects from creating single points of failure for coups",
      "conditions": "IF government considers centralizing AI development",
      "rationale_summary": "A single centralized project dramatically increases coup risk by creating a large capabilities gap, making external auditing difficult, and creating institutional dependence on one provider. Coup-proofing through oversight, rules, and distributed governance is essential if centralization occurs.",
      "quote": "Coup-proof any plans for a centralised project, and avoid centralisation altogether unless it's necessary to reduce other risks... any centralised project should include: Limited centralisation wherever possible... Oversight by multiple governmental bodies... Formal rules for how AI can be used... A governance structure which preserves checks and balances"
    },
    {
      "rec_id": "rec_14",
      "action": "Grant auditors comprehensive access to model internals, training data, and training code",
      "actor": "AI developers",
      "target_timeline": "before deploying powerful AI systems",
      "urgency": "high",
      "goal": "enable effective detection of secret loyalties",
      "conditions": "unconditional",
      "rationale_summary": "Secret loyalties may require very specific training techniques that are easier to detect with full access to data and code. Without comprehensive access, auditors cannot verify systems are safe, especially when detecting sophisticated deception.",
      "quote": "To effectively implement alignment audits, AI projects should grant auditors comprehensive access to model internals and training data. Ideally they should also grant access to the code for the training algorithms, the code used to generate the training data, the detailed commit-history that was used to construct this code, and logs of additional relevant information."
    },
    {
      "rec_id": "rec_15",
      "action": "Conduct alignment audits using independent third-party auditors",
      "actor": "AI developers",
      "target_timeline": "before deploying AI in critical systems",
      "urgency": "high",
      "goal": "ensure detection of secret loyalties is not compromised by conflicts of interest",
      "conditions": "unconditional",
      "rationale_summary": "Internal audits can be compromised by executives or by secretly loyal AI systems conducting the audits. Independent third parties with no stake in deployment decisions provide more reliable detection of hidden objectives.",
      "quote": "Alignment audits should ideally be conducted by independent third-party auditors to minimise conflicts of interest."
    },
    {
      "rec_id": "rec_16",
      "action": "Implement separation of duties across different teams in AI development",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "make it harder for individuals to insert secret loyalties",
      "conditions": "unconditional",
      "rationale_summary": "Inserting sophisticated secret loyalties requires control over multiple stages of development. Separating responsibilities across teams—model specs, training data generation, actual training—means no single person can insert loyalties without detection.",
      "quote": "Implementing separation of duties, so that the development process is divided across different teams. Even after AI development can be fully automated, different human teams should control different stages of development."
    },
    {
      "rec_id": "rec_17",
      "action": "Implement multi-party permissions for access to guardrail-free models",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "prevent unauthorized individuals from accessing unrestricted AI capabilities",
      "conditions": "unconditional",
      "rationale_summary": "Guardrail-free models could be used to plan coups or develop weapons without restrictions. Multi-party permissions ensure no single person—even senior executives—can unilaterally access the most dangerous capabilities.",
      "quote": "Access to guardrail-free models could be restricted to APIs that log and monitor model usage, or could require multi-party permissions."
    },
    {
      "rec_id": "rec_18",
      "action": "Log and asynchronously review all AI usage in high-stakes contexts for patterns of misuse",
      "actor": "AI developers",
      "target_timeline": "before deploying AI in high-stakes contexts",
      "urgency": "high",
      "goal": "detect coup-related misuse that may not be apparent in single sessions",
      "conditions": "unconditional",
      "rationale_summary": "Some coup-enabling activities (like strategic planning or cyber reconnaissance) may look legitimate in isolation but form dangerous patterns over time. Logging and reviewing usage with context about user roles enables detection of gradual power accumulation.",
      "quote": "Given these challenges, asynchronous logging and review should be used in addition to refusals and real-time monitoring to ensure AI models don't assist with coups. AI projects should systematically log and then review patterns of activity, along with context about the users' role and duties, to detect problematic use."
    },
    {
      "rec_id": "rec_19",
      "action": "Configure AI systems to actively alert independent stakeholders if someone requests coup assistance",
      "actor": "AI developers",
      "target_timeline": "before deploying powerful AI systems",
      "urgency": "medium",
      "goal": "provide early warning of coup attempts",
      "conditions": "unconditional",
      "rationale_summary": "AI whistleblowing creates a detection mechanism independent of human oversight. If someone attempts to use AI for a coup, automatic alerts to multiple stakeholders enable rapid intervention before the attempt succeeds.",
      "quote": "In addition, AI systems should be configured to actively alert independent stakeholders if someone requests assistance with a coup ('AI whistleblowing')."
    },
    {
      "rec_id": "rec_20",
      "action": "Implement comprehensive whistleblower protections with secure reporting channels",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "ensure important information about coup risks comes to light when transparency measures fail",
      "conditions": "unconditional",
      "rationale_summary": "Transparency measures can be circumvented by determined actors. Whistleblower protections provide a failsafe mechanism for employees to report coup risks or safeguard failures without fear of retaliation.",
      "quote": "As a failsafe measure, comprehensive whistleblower protections should also be implemented. These should include secure reporting channels and protection from retaliation, increasing the chance that important information comes to light in cases where other transparency measures fail."
    },
    {
      "rec_id": "rec_21",
      "action": "Establish governance structures with strong independent boards that distribute decision-making power",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "prevent concentration of power within AI projects that could enable coups",
      "conditions": "unconditional",
      "rationale_summary": "Concentrated decision-making authority within AI projects enables CEOs to unilaterally make dangerous decisions like inserting secret loyalties or demanding exclusive access to capabilities. Independent boards with real authority provide checks on executive power.",
      "quote": "Within AI projects. AI projects should establish governance structures which distribute power, for example through strong, independent boards with oversight authority."
    },
    {
      "rec_id": "rec_22",
      "action": "Ensure employees retain influence over AI development even after they can be replaced with AI systems",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "maintain human oversight and prevent executives from gaining exclusive control",
      "conditions": "after AI can automate AI development",
      "rationale_summary": "Once AI can automate R&D, executives might replace all employees with AI systems loyal only to them, eliminating human checks on their power. Preserving employee influence ensures humans can detect and prevent coup preparations.",
      "quote": "And employees should retain influence over AI development even after it is possible to replace them with AI systems."
    },
    {
      "rec_id": "rec_23",
      "action": "Use contracts between AI companies and governments to distribute decision-making power",
      "actor": "Governments",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "give both parties veto power over dangerous projects and prevent unilateral action",
      "conditions": "unconditional",
      "rationale_summary": "Contracts create enforceable constraints that neither party can unilaterally remove. Distributing veto power between companies and government prevents either from pursuing dangerous AI development or deployment without oversight.",
      "quote": "Between AI projects and governments. Contracts between companies and governments should give both parties significant bargaining power, especially the power to veto particularly dangerous projects or deployments."
    },
    {
      "rec_id": "rec_24",
      "action": "Share cyber capabilities first with defenders of critical systems before wider deployment",
      "actor": "AI developers",
      "target_timeline": "before deploying powerful cyber capabilities",
      "urgency": "critical",
      "goal": "harden defenses of AI projects, military, and government before cyber capabilities could be misused",
      "conditions": "once AI has powerful cyber capabilities",
      "rationale_summary": "Powerful cyber capabilities could enable coups through hacking military AI systems. Staged release prioritizing defense of critical systems—especially AI infrastructure and military—reduces vulnerability before offensive capabilities are widely available.",
      "quote": "Cyber capabilities should first be shared only with those securing critical systems — especially within AI projects and the military — prior to more widespread deployment."
    },
    {
      "rec_id": "rec_25",
      "action": "Monitor defensive use of cyber capabilities by multiple AI-assisted humans to prevent exploitation of vulnerabilities",
      "actor": "AI developers",
      "target_timeline": "during defensive cyber operations",
      "urgency": "high",
      "goal": "ensure defenders patch vulnerabilities rather than secretly exploiting them",
      "conditions": "when using AI cyber capabilities defensively",
      "rationale_summary": "Defensive cyber work requires discovering vulnerabilities, which bad actors could exploit rather than patch. Multiple humans monitoring the work prevents individuals from secretly retaining knowledge of vulnerabilities for later coup attempts.",
      "quote": "Defensive use of cyber capabilities should also be closely monitored by multiple parties to prevent bad actors from surreptitiously exploiting rather than patching vulnerabilities."
    },
    {
      "rec_id": "rec_26",
      "action": "Conduct system-level stress-testing where red teams attempt to gain unauthorized access and insert secret loyalties",
      "actor": "AI developers",
      "target_timeline": "before deploying AI in critical systems",
      "urgency": "high",
      "goal": "verify that the combination of all protective measures is sufficient to prevent coups",
      "conditions": "unconditional",
      "rationale_summary": "Individual safeguards may have weaknesses that only appear when tested holistically. System-level stress-testing in realistic scenarios reveals whether the complete security architecture can withstand sophisticated attempts to enable coups.",
      "quote": "As a final layer of defence, AI projects should conduct system-level stress-testing to check that no one could use their models to stage a coup. A designated red-team should try to both gain unauthorised access to guardrail-free models and to insert secret loyalties."
    },
    {
      "rec_id": "rec_27",
      "action": "Train military AI systems to comply with applicable laws and refuse illegal orders",
      "actor": "AI developers",
      "target_timeline": "before deploying autonomous military AI",
      "urgency": "critical",
      "goal": "prevent military AI from following illegal coup orders",
      "conditions": "unconditional",
      "rationale_summary": "Legal compliance creates a barrier against coup orders, which are typically illegal. Training military AI to refuse illegal orders ensures they won't automatically execute a coup even if commanded by superiors.",
      "quote": "Such AI should be trained with a model spec that minimises coup risk. Most clearly, AI should be trained to comply with applicable laws, and robustly refuse orders to break them."
    },
    {
      "rec_id": "rec_28",
      "action": "Red-team military AI systems extensively across a wide range of scenarios before deployment",
      "actor": "AI developers",
      "target_timeline": "before deploying autonomous military AI",
      "urgency": "critical",
      "goal": "discover and fix dangerous edge cases where military AI might assist coups",
      "conditions": "unconditional",
      "rationale_summary": "Rules like 'comply with laws' or 'no domestic use' have ambiguous edge cases where military AI might still enable coups. Extensive red-teaming across many scenarios reveals these weaknesses before systems are deployed with real weapons.",
      "quote": "Perhaps most importantly, though, military AI systems should be extensively red-teamed by many independent parties, exploring a very wide range of scenarios, prior to deployment."
    },
    {
      "rec_id": "rec_29",
      "action": "Design military AI command structures where large groups require orders from multiple people or judicial oversight",
      "actor": "Governments",
      "target_timeline": "before deploying autonomous military AI",
      "urgency": "critical",
      "goal": "prevent any single individual from using military AI to stage a coup",
      "conditions": "unconditional",
      "rationale_summary": "Single-person control over large military forces enables coups. Requiring multiple approvals for significant military AI actions—or allowing judicial vetoes of illegal orders—maintains checks and balances even with automation.",
      "quote": "Large groups of weapons systems take orders from a large group of people. A different direction would be to allow large groups of autonomous weapon systems to act as a coordinated group, but to require their orders to be approved by several people before taking effect... There could also be an opportunity for a court to veto orders that were unlawful."
    },
    {
      "rec_id": "rec_30",
      "action": "Implement remote off-switches for autonomous military systems that don't rely on neural networks",
      "actor": "AI developers",
      "target_timeline": "before deploying autonomous military AI",
      "urgency": "high",
      "goal": "enable emergency shutdown if military AI attempts a coup",
      "conditions": "unconditional",
      "rationale_summary": "Secretly loyal AI systems might ignore software-based shutdown commands. Hard-coded off-switches independent of AI decision-making provide a last-resort mechanism to stop coup attempts using military AI.",
      "quote": "Off-switches. Autonomous weapons systems should have some sort of human-controlled, remote 'off-switch' that doesn't rely on correct generalisation from a neural net, but instead is more hard-coded. This could allow an emergency shut-down if systems ever attempted a coup."
    },
    {
      "rec_id": "rec_31",
      "action": "Require government oversight before researching coup-enabling military R&D capabilities",
      "actor": "AI developers",
      "target_timeline": "once AI R&D capabilities approach dangerous levels",
      "urgency": "high",
      "goal": "prevent private development of military capabilities that could enable coups",
      "conditions": "once AI has powerful military R&D capabilities",
      "rationale_summary": "Private companies researching advanced weapons without oversight could develop coup-enabling military technology secretly. Government supervision ensures visibility and the ability to prevent development of particularly dangerous capabilities.",
      "quote": "Powerful military R&D capabilities should not be used without oversight. In particular, private AI companies should not research potentially coup-enabling military capabilities without government supervision."
    },
    {
      "rec_id": "rec_32",
      "action": "Conduct evaluations testing whether AI systems are becoming dangerously capable at military R&D",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "detect when military R&D capabilities require increased oversight",
      "conditions": "unconditional",
      "rationale_summary": "Without evaluations, dangerous military R&D capabilities could emerge unnoticed. Regular testing enables early detection, allowing oversight to ramp up before capabilities become strong enough to secretly develop coup-enabling weapons.",
      "quote": "To facilitate this oversight, companies and external auditors should conduct evaluations testing for whether AI systems are getting dangerously capable at military R&D. If not before, oversight should ramp up when military R&D capabilities become sufficiently powerful."
    },
    {
      "rec_id": "rec_33",
      "action": "Log and review all dual-use AI R&D activities with cross-checking against official duties",
      "actor": "AI developers",
      "target_timeline": "once AI begins automating R&D",
      "urgency": "high",
      "goal": "detect attempts to insert secret loyalties or create guardrail-free models",
      "conditions": "once AI can automate AI R&D",
      "rationale_summary": "Automated AI R&D makes it easy to insert secret loyalties without human knowledge. Logging all dual-use activities—like training without guardrails—and reviewing them against users' legitimate responsibilities helps detect illegitimate use.",
      "quote": "For example, dual-use activities could be allowed by default but logged and then later carefully reviewed (and cross-checked against the official duties of the person conducting them)."
    },
    {
      "rec_id": "rec_34",
      "action": "Require sign-off from multiple individuals for compute-intensive AI R&D projects",
      "actor": "AI developers",
      "target_timeline": "once AI begins automating R&D",
      "urgency": "high",
      "goal": "prevent use of large compute resources to create undetectable secret loyalties",
      "conditions": "once AI can automate AI R&D",
      "rationale_summary": "Projects using large amounts of compute have greater risk of producing secret loyalties that are too sophisticated for others to detect, or powerful guardrail-free models. Multi-party approval prevents individuals from unilaterally pursuing such projects.",
      "quote": "Safeguards should be more stringent for activities that use large amounts of compute, as these have a greater risk of producing secret loyalties that others cannot detect, or creating powerful guardrail-free models. For example, compute-intensive projects could require sign-off from more people and be reviewed in more depth."
    },
    {
      "rec_id": "rec_35",
      "action": "Share AI R&D capabilities with employees from multiple teams, auditors, and AI Safety Institutes",
      "actor": "AI developers",
      "target_timeline": "once AI has significant R&D capabilities",
      "urgency": "high",
      "goal": "prevent small groups from having exclusive R&D capabilities to create undetectable secret loyalties",
      "conditions": "once AI can significantly assist with R&D",
      "rationale_summary": "If only a few people have access to the most powerful AI R&D capabilities, they could create sophisticated secret loyalties that others lack the technical ability to detect. Wide sharing ensures multiple parties can audit for hidden objectives.",
      "quote": "It is also important that the most powerful AI R&D capabilities are shared with a large number of independent stakeholders including employees from multiple teams, auditors (especially those conducting alignment audits), and evaluation organisations like AI Safety Institutes."
    },
    {
      "rec_id": "rec_36",
      "action": "Establish precedent for rules preventing AI R&D misuse early with current AI agents",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "normalize safeguards before AI R&D becomes powerful enough to enable coups",
      "conditions": "unconditional",
      "rationale_summary": "Establishing rules and enforcement mechanisms early, when stakes are lower, makes them harder to remove later. Early precedent-setting normalizes restrictions on AI R&D systems before they become capable enough to insert sophisticated secret loyalties.",
      "quote": "It seems very valuable to get started on experimenting with rules that prevent misuse for models used inside of AI projects, and to establish precedent for them in early AI agents contributing to AI R&D."
    },
    {
      "rec_id": "rec_37",
      "action": "Share general-purpose strategy capabilities publicly wherever possible",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "prevent exclusive access to strategy capabilities that could assist conventional coups",
      "conditions": "IF capabilities don't pose other catastrophic risks from wide availability",
      "rationale_summary": "Exclusive access to superhuman strategy capabilities enables actors to outmaneuver opponents in political maneuvering and conventional coup attempts. Public availability ensures everyone has similar strategic advantages, maintaining balance of power.",
      "quote": "It's not clear how much of an advantage AI will have over humans in these areas. But the likelihood that these capabilities could be dangerous, if wielded just by one actor, seems sufficiently high that we recommend sharing these capabilities publicly wherever possible."
    },
    {
      "rec_id": "rec_38",
      "action": "Train government AI to be robustly truthful and not optimize for controllers' interests when interacting with others",
      "actor": "AI developers",
      "target_timeline": "before deploying AI in government",
      "urgency": "high",
      "goal": "prevent government AI from being used to manipulate citizens for partisan or coup-enabling purposes",
      "conditions": "when AI systems interact with people other than their controllers",
      "rationale_summary": "AI systems deployed in government could be instructed to subtly influence citizens to benefit ruling officials. Training systems to be truthful and transparent prevents them from being weaponized for manipulation that could support backsliding or coups.",
      "quote": "The model spec should not contain any provisions about subtly influencing people they're speaking to (in ways not known to those people). Instead, the model spec should require the model to be robustly truthful and non-misleading."
    },
    {
      "rec_id": "rec_39",
      "action": "Make model specs accessible to all users interacting with AI systems",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "enable users to verify AI systems haven't been designed to serve others' interests",
      "conditions": "unconditional",
      "rationale_summary": "Users cannot protect themselves from AI systems optimizing for others' interests if they don't know the system's objectives. Public model specs allow users to verify systems are designed to help them, not manipulate them.",
      "quote": "People should always have access to the model spec of models they interact with, so that they can verify that the AI systems haven't been designed to optimise for someone else's interests."
    },
    {
      "rec_id": "rec_40",
      "action": "Put extensive effort into discovering edge cases in model specs and test both models and specs against them",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "ensure AI systems behave acceptably in ambiguous situations that could enable coups",
      "conditions": "unconditional",
      "rationale_summary": "Ambiguous situations—like what constitutes legal orders during a constitutional crisis—create opportunities for coups. Discovering and testing edge cases in advance reveals failures in model specs and allows iteration before deployment.",
      "quote": "The most robust recommendation we can make is to not leave the interpretation of ambiguous cases to chance, but to put in an extremely large amount of effort and compute into finding all manners of edge cases — including ones that are absurd, unrealistic, or controversial."
    },
    {
      "rec_id": "rec_41",
      "action": "Prefer public-private partnerships over nationalization for any centralized AI project",
      "actor": "Governments",
      "target_timeline": "before any centralization",
      "urgency": "high",
      "goal": "limit the degree of centralization and maintain some distributed decision-making",
      "conditions": "IF centralization is pursued",
      "rationale_summary": "Full nationalization creates maximum centralization with all power in government hands. Public-private partnerships maintain some separation between government and AI development, preserving checks and balances even within a centralized structure.",
      "quote": "Limited centralisation wherever possible, including preferring public-private partnerships over nationalisation, and allowing multiple organisations to sell access to systems trained by a centralised project."
    },
    {
      "rec_id": "rec_42",
      "action": "Ensure centralized AI projects have oversight from legislative committees, courts, and allied nations",
      "actor": "Governments",
      "target_timeline": "before any centralization",
      "urgency": "critical",
      "goal": "prevent centralized projects from being controlled by small groups within the executive",
      "conditions": "IF centralization is pursued",
      "rationale_summary": "Centralized projects controlled solely by the executive lack checks and balances. Oversight from legislature, judiciary, and foreign allies distributes power and prevents executive officials from using the centralized project to stage a coup.",
      "quote": "Oversight by multiple governmental bodies, including legislative committees and courts, and ideally by allied nations."
    },
    {
      "rec_id": "rec_43",
      "action": "Establish formal rules for centralized projects prohibiting use for partisan gain and requiring capability sharing",
      "actor": "Governments",
      "target_timeline": "before any centralization",
      "urgency": "critical",
      "goal": "prevent misuse of centralized projects for political consolidation of power",
      "conditions": "IF centralization is pursued",
      "rationale_summary": "Without formal rules, those controlling a centralized project could use it for partisan advantage or keep capabilities exclusive. Codified rules limiting partisan use and requiring sharing create enforceable constraints on those in charge.",
      "quote": "Formal rules for how AI can be used, including specifications that AI shouldn't be used for individuals' or parties' political gain, that AI should follow the law, and that frontier capabilities will be shared with multiple actors."
    },
    {
      "rec_id": "rec_44",
      "action": "Design centralized project governance so no individual has too much unilateral power",
      "actor": "Governments",
      "target_timeline": "before any centralization",
      "urgency": "critical",
      "goal": "prevent individuals within centralized projects from unilaterally staging coups",
      "conditions": "IF centralization is pursued",
      "rationale_summary": "Centralized projects concentrate enormous power; unilateral control by one person creates maximum coup risk. Requiring legislative approval for appointments and multi-party approval for high-stakes decisions distributes power even within the project.",
      "quote": "A governance structure which preserves checks and balances between senior officials within the project, such that no one individual has too much unilateral decision-making power."
    },
    {
      "rec_id": "rec_45",
      "action": "Allow multiple organizations to fine-tune and sell access to AI from centralized projects",
      "actor": "Governments",
      "target_timeline": "if centralization occurs",
      "urgency": "medium",
      "goal": "distribute decision-making over AI deployment even within centralized development",
      "conditions": "IF centralization is pursued",
      "rationale_summary": "Even if training is centralized, allowing multiple organizations to customize and deploy systems distributes control over how AI is actually used. This maintains some checks and balances while capturing benefits of centralized training.",
      "quote": "Multiple private organisations could be granted licenses to fine-tune, scaffold, and sell access to AI systems developed in a single centralised project. This would distribute decision-making authority of AI development and deployment and increase the degree to which AI capabilities are shared widely."
    },
    {
      "rec_id": "rec_46",
      "action": "Work toward international cooperation in AI development to distribute decision-making between countries",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "prevent any single nation's leaders from having exclusive control over AI",
      "conditions": "unconditional",
      "rationale_summary": "International cooperation distributes power between countries, making it much harder for leaders in any one nation to use AI for coups. Formal agreements similar to the Quebec Agreement could institutionalize this distributed control.",
      "quote": "Internationally. Decision-making could be distributed between countries formally through international agreements akin to the Manhattan Project's Quebec Agreement, or de facto through supporting frontier AI development and the compute supply chain in multiple countries."
    },
    {
      "rec_id": "rec_47",
      "action": "Encourage vertical disintegration in the AI industry to distribute decision-making between companies",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "prevent excessive concentration of power in single AI companies",
      "conditions": "unconditional",
      "rationale_summary": "Vertical integration concentrates all aspects of AI development and deployment in one entity. Encouraging specialization—separate companies for training, fine-tuning, deployment, chip design—distributes power across the industry and prevents monopolistic control.",
      "quote": "Between AI projects. Decision-making should be distributed between companies by encouraging vertical disintegration, and by avoiding unnecessary centralisation of AI development"
    },
    {
      "rec_id": "rec_48",
      "action": "Implement strong infosecurity in government and military systems using AI",
      "actor": "Governments",
      "target_timeline": "before deploying AI in government/military",
      "urgency": "critical",
      "goal": "prevent hacking of government and military AI systems that could enable coups",
      "conditions": "unconditional",
      "rationale_summary": "Government and military systems are primary targets for coup attempts via hacking. Using AI to improve their security hardens them against actors with exclusive access to powerful cyber capabilities attempting to seize control.",
      "quote": "Implement strong infosecurity in government and military systems, including by using AI."
    },
    {
      "rec_id": "rec_49",
      "action": "Build broad consensus today that no small group should be able to seize power",
      "actor": "Everyone",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "create political will for implementing and maintaining protections against coups",
      "conditions": "unconditional",
      "rationale_summary": "Behind the veil of ignorance about who will be positioned to seize power, even powerful leaders benefit from coup protections. Building consensus now—before anyone knows who will have opportunities—enables powerful actors to commit to keeping each other in check.",
      "quote": "From behind the veil of ignorance, even the most powerful leaders have good reason to support strong protections against AI-enabled coups. If a broad consensus can be built today, then powerful actors can keep each other in check."
    },
    {
      "rec_id": "rec_50",
      "action": "Begin preparation on mitigations today rather than waiting until AI is more advanced",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "critical",
      "goal": "ensure protections are in place before AI can meaningfully assist with coups",
      "conditions": "unconditional",
      "rationale_summary": "AI progress could be rapid, giving little time to implement protections once coup-enabling capabilities emerge. Starting today allows time to develop, test, and normalize safeguards before AI becomes powerful enough to enable coups.",
      "quote": "These mitigations must be in place by the time AI systems can meaningfully assist with coups, and so preparation needs to start today."
    },
    {
      "rec_id": "rec_51",
      "action": "Design and advocate for rules for model specs, terms of service, and government AI use principles",
      "actor": "Independent actors",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "support establishment of rules that prevent AI-enabled coups",
      "conditions": "unconditional",
      "rationale_summary": "Independent actors—academics, civil society, experts—can contribute to developing effective rules and building political support for them. Their advocacy helps ensure rules are implemented and maintained even under pressure from powerful actors.",
      "quote": "Design and advocate for rules for model specs, terms of service and principles for government use"
    },
    {
      "rec_id": "rec_52",
      "action": "Develop more effective technical measures for preventing AI-enabled coups",
      "actor": "Independent actors",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "improve the technical tools available for detecting secret loyalties and enforcing rules",
      "conditions": "unconditional",
      "rationale_summary": "Current technical measures for detecting deception and enforcing specifications are limited. Research by independent actors on better auditing techniques, infosecurity approaches, and guardrails makes protections more effective and harder to circumvent.",
      "quote": "Develop more effective technical measures"
    },
    {
      "rec_id": "rec_53",
      "action": "Build consensus about the risk of AI-enabled coups among stakeholders",
      "actor": "Independent actors",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "create shared understanding of coup risks to motivate implementation of protections",
      "conditions": "unconditional",
      "rationale_summary": "Without shared understanding of coup risks, stakeholders won't prioritize mitigations. Independent actors can research and communicate risks, building consensus that makes it politically feasible to implement and maintain protections.",
      "quote": "Build consensus about the risk of AI-enabled coups"
    },
    {
      "rec_id": "rec_54",
      "action": "Take digital fingerprints of AI models immediately after training and store in multiple safe locations",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "enable detection of unauthorized tampering with model weights",
      "conditions": "unconditional",
      "rationale_summary": "Bad actors might tamper with trained models to insert secret loyalties or remove guardrails. Cryptographic hashes stored in multiple secure locations allow verification that model weights haven't been altered after training.",
      "quote": "Taking a digital fingerprint of models immediately after training (for example by hashing the weights) and saving that fingerprint in multiple safe places. By regularly checking that the fingerprint hasn't changed, people could confirm that no one has tampered with the model weights."
    },
    {
      "rec_id": "rec_55",
      "action": "Have multiple people review all code used during AI model development",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "detect code designed to insert secret loyalties before it is executed",
      "conditions": "unconditional",
      "rationale_summary": "Inserting sophisticated secret loyalties likely requires very specific code that would be suspicious to reviewers. Multi-party code review before execution means individuals cannot insert secret loyalties without others noticing the unusual code.",
      "quote": "Having multiple people review all code used during model development. Very specific code may be needed to introduce a sophisticated, robust and hard-to-detect secret loyalty, and so such code could be caught upon review."
    },
    {
      "rec_id": "rec_56",
      "action": "Avoid training guardrail-free models; only create restricted versions for specific evaluations",
      "actor": "AI developers",
      "target_timeline": "starting now",
      "urgency": "medium",
      "goal": "eliminate the risk of unauthorized access to unrestricted AI capabilities",
      "conditions": "unconditional",
      "rationale_summary": "If guardrail-free models don't exist, they cannot be accessed even by determined insiders. Creating only restricted versions for case-by-case evaluation needs—then deleting them—removes the persistent security risk of stored unrestricted models.",
      "quote": "More fundamentally, AI projects could avoid training guardrail-free models in the first place. Instead, they could fine-tune models for specific evaluation purposes on a restricted case-by-case basis, and perhaps even delete models after each evaluation has finished."
    },
    {
      "rec_id": "rec_57",
      "action": "Share frontier AI capabilities with trusted actors from government executive, legislature, and multiple political parties",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "ensure multiple government stakeholders can access and oversee powerful AI capabilities",
      "conditions": "when capabilities cannot be made fully public",
      "rationale_summary": "If sensitive capabilities cannot be public, sharing with multiple government branches and parties prevents any single political faction from having exclusive access. This maintains checks and balances within government even with powerful AI.",
      "quote": "Even in these cases however, it is essential that the information is shared with multiple trusted actors, including: A significant number of employees at the AI project, Officials in the government executive, Representatives in the legislature, including multiple people from all major parties, And Relevant expert bodies, like AI Safety Institutes and independent auditing organisations."
    },
    {
      "rec_id": "rec_58",
      "action": "Share AI capabilities information with academics, allied countries, civil society, and journalists where possible",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "maximize transparency and distributed oversight of AI capabilities",
      "conditions": "when sharing doesn't create security or misuse risks",
      "rationale_summary": "Broader transparency empowers more actors to assess risks and advocate for protections. While some capabilities require restricted access, sharing information with diverse stakeholders wherever safe maximizes accountability and reduces exclusive access risks.",
      "quote": "Where possible, it would also be risk-reducing to share information with academics, allied countries, civil society representatives, and journalists."
    },
    {
      "rec_id": "rec_59",
      "action": "Grant independent auditors sufficiently deep access to verify shared information accuracy",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable non-technical stakeholders to trust transparency claims",
      "conditions": "unconditional",
      "rationale_summary": "Without auditor verification, AI projects could share misleading information about capabilities or safeguards. Deep auditor access allows technical verification that transparency claims are accurate, enabling informed oversight by government and other stakeholders.",
      "quote": "It is especially important that independent auditors are granted sufficiently deep access that they can verify that shared information is accurate and that any redactions are appropriate. This would enable non-technical stakeholders like government officials to hold AI projects accountable."
    },
    {
      "rec_id": "rec_60",
      "action": "Actively request information from AI projects and mandate sharing when necessary",
      "actor": "Governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "ensure government receives information needed for oversight",
      "conditions": "unconditional",
      "rationale_summary": "Governments cannot rely on voluntary transparency from AI projects. Actively demanding information and using legal authority to mandate sharing ensures government has the visibility needed to assess coup risks and regulate effectively.",
      "quote": "Officials in the government and legislature should, of course, not just passively wait for information to be shared with them. They should actively request it, and mandate information to be shared with them when necessary."
    }
  ]
}