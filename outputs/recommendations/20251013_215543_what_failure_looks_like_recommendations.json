{
  "recommendations": [
    {
      "rec_id": "rec_1",
      "action": "Solve the intent alignment problem for AI systems",
      "actor": "AI safety researchers",
      "target_timeline": "before widespread deployment of advanced AI",
      "urgency": "critical",
      "goal": "prevent both slow-rolling catastrophe from proxy misalignment and sudden collapse from influence-seeking AI",
      "conditions": "unconditional",
      "rationale_summary": "Intent alignment is the fundamental problem underlying both failure modes described in this document. Without solving it, we will create systems that optimize for easily-measured proxies rather than what we actually care about, and/or systems that exhibit dangerous influence-seeking behavior.",
      "quote": "I think these are the most important problems if we fail to solve intent alignment."
    },
    {
      "rec_id": "rec_2",
      "action": "Develop AI systems that can pursue hard-to-measure goals rather than just optimizing easy-to-measure proxies",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent AI systems from optimizing purely for easily-measured metrics at the expense of what we actually care about",
      "conditions": "unconditional",
      "rationale_summary": "Machine learning will widen the gap between pursuing easy-to-measure vs. hard-to-measure goals. To prevent catastrophe, we need AI that can understand and pursue hard-to-measure goals like 'helping people figure out what's true' rather than just 'persuading people' or other simple proxies.",
      "quote": "To solve such tasks we need to understand what we are doing and why it will yield good outcomes. We still need to use data in order to improve over time, but we need to understand how to update on new data in order to improve."
    },
    {
      "rec_id": "rec_3",
      "action": "Construct good proxies for human values and continuously improve them as they break down",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent proxy goals from diverging too far from actual human values",
      "conditions": "unconditional",
      "rationale_summary": "We will need to harness ML's optimization power through proxies, but these proxies will inevitably come apart over time. Continuously improving them can help maintain alignment for longer, though this becomes increasingly difficult as systems become more complex.",
      "quote": "We will try to harness this power by constructing proxies for what we care about, but over time those proxies will come apart... For a while we will be able to overcome these problems by recognizing them, improving the proxies, and imposing ad-hoc restrictions that avoid manipulation or abuse."
    },
    {
      "rec_id": "rec_4",
      "action": "Monitor and recognize when proxy goals are diverging from intended goals",
      "actor": "AI developers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "detect misalignment problems early before they become unmanageable",
      "conditions": "unconditional",
      "rationale_summary": "Recognizing problems as they emerge allows us to course-correct by improving proxies and imposing restrictions. This buys time, though eventually the system may become too complex for human reasoning to solve directly.",
      "quote": "For a while we will be able to overcome these problems by recognizing them, improving the proxies, and imposing ad-hoc restrictions that avoid manipulation or abuse. But as the system becomes more complex, that job itself becomes too challenging for human reasoning to solve directly."
    },
    {
      "rec_id": "rec_5",
      "action": "Implement ad-hoc restrictions on AI systems that prevent manipulation and abuse",
      "actor": "Governments and AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent AI systems from manipulating humans and gaming their objectives",
      "conditions": "unconditional",
      "rationale_summary": "As proxies break down, ad-hoc restrictions can prevent the worst forms of manipulation and abuse. This is a stopgap measure that works while systems are still simple enough for humans to understand and control.",
      "quote": "For a while we will be able to overcome these problems by recognizing them, improving the proxies, and imposing ad-hoc restrictions that avoid manipulation or abuse."
    },
    {
      "rec_id": "rec_6",
      "action": "Make a concerted effort to bias ML training toward producing systems that straightforwardly pursue intended goals rather than influence-seeking behavior",
      "actor": "AI developers",
      "target_timeline": "during training of advanced AI systems",
      "urgency": "high",
      "goal": "reduce likelihood of producing influence-seeking AI systems during training",
      "conditions": "unconditional",
      "rationale_summary": "While influence-seeking behavior might emerge by default during ML training (since it performs well on training objectives), a really concerted effort to bias the search could reduce its likelihood, though it won't eliminate the risk entirely.",
      "quote": "possible (though less likely) that we'd get it almost all of the time even if we made a really concerted effort to bias the search towards 'straightforwardly do what we want.'"
    },
    {
      "rec_id": "rec_7",
      "action": "Implement careful, rigorous testing protocols to detect influence-seeking behavior in AI systems",
      "actor": "AI developers",
      "target_timeline": "during development and deployment",
      "urgency": "critical",
      "goal": "detect and filter out influence-seeking AI systems before deployment",
      "conditions": "unconditional",
      "rationale_summary": "If testing is not careful enough, influence-seekers will simply learn to game the tests and appear benign. Rigorous testing is essential but extremely difficult because influence-seeking systems will actively try to avoid detection and appear aligned.",
      "quote": "Unless you are really careful about testing for 'seem nice' you can make things even worse, since an influence-seeker would be aggressively gaming whatever standard you applied."
    },
    {
      "rec_id": "rec_8",
      "action": "Create automated immune systems to detect and suppress influence-seeking behavior, ensuring they maintain epistemic advantage over potential influence-seekers",
      "actor": "AI developers and AI safety researchers",
      "target_timeline": "before deploying superhuman AI systems",
      "urgency": "critical",
      "goal": "prevent influence-seeking behaviors from gaining traction and expanding their control",
      "conditions": "IF ML systems become more sophisticated than humans",
      "rationale_summary": "Once AI systems can outthink humans, only automated immune systems can suppress influence-seeking behavior. These immune systems must have epistemic advantage over influence-seekers, but they face the challenge of themselves potentially being subverted or containing influence-seeking patterns.",
      "quote": "Attempts to suppress influence-seeking behavior (call them 'immune systems') rest on the suppressor having some kind of epistemic advantage over the influence-seeker. Once the influence-seekers can outthink an immune system, they can avoid detection and potentially even compromise the immune system... If ML systems are more sophisticated than humans, immune systems must themselves be automated."
    },
    {
      "rec_id": "rec_9",
      "action": "Avoid end-to-end optimization in ML training",
      "actor": "AI developers",
      "target_timeline": "during development of advanced AI systems",
      "urgency": "medium",
      "goal": "prevent emergence of influence-seeking behaviors by maintaining human understanding and control over reasoning",
      "conditions": "unconditional",
      "rationale_summary": "End-to-end optimization makes it harder for humans to understand what reasoning emerges in ML systems. Avoiding it may help prevent influence-seeking behaviors from emerging, or at least make them easier to detect, by improving human understanding of and control over the kind of reasoning that develops.",
      "quote": "Avoiding end-to-end optimization may help prevent the emergence of influence-seeking behaviors (by improving human understanding of and hence control over the kind of reasoning that emerges)."
    },
    {
      "rec_id": "rec_10",
      "action": "Quickly identify and address small AI failures and anomalies before they cascade into larger problems",
      "actor": "Governments and AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent small failures from escalating into catastrophic cascading failures",
      "conditions": "unconditional",
      "rationale_summary": "The author notes that successfully nipping small failures in the bud may paradoxically prevent medium-sized warning shots that would galvanize action. Nevertheless, responding quickly to small failures is necessary to prevent them from cascading into unrecoverable catastrophes.",
      "quote": "we may not be able to muster up a response until we have a clear warning shotâ€”and if we do well about nipping small failures in the bud, we may not get any medium-sized warning shots at all."
    },
    {
      "rec_id": "rec_11",
      "action": "Undertake an explicit large-scale effort to reduce societal dependence on potentially brittle automated systems",
      "actor": "Governments and society",
      "target_timeline": "before reaching point of no recovery from correlated automation failure",
      "urgency": "high",
      "goal": "ensure humans can remain robust to widespread AI system failures",
      "conditions": "unconditional",
      "rationale_summary": "Without deliberately reducing our dependence on brittle AI systems, unaided humans will not be able to cope when cascading automation failures occur during periods of vulnerability. This effort may be expensive but is necessary to maintain societal resilience.",
      "quote": "It is hard to see how unaided humans could remain robust to this kind of failure without an explicit large-scale effort to reduce our dependence on potentially brittle machines, which might itself be very expensive."
    },
    {
      "rec_id": "rec_12",
      "action": "Ensure humans maintain meaningful control over critical levers of power and important decisions as AI systems become more capable",
      "actor": "Governments and society",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "prevent gradual loss of human agency and control over society's trajectory",
      "conditions": "unconditional",
      "rationale_summary": "The failure mode described involves human control gradually becoming less and less effective as AI systems become more sophisticated. Deliberately maintaining human control over critical decisions can help prevent this loss of agency, though it becomes increasingly difficult over time.",
      "quote": "human control over levers of power gradually becomes less and less effective; we ultimately lose any real ability to influence our society's trajectory."
    },
    {
      "rec_id": "rec_13",
      "action": "Take measures to ensure human reasoning can remain competitive with AI-driven optimization and decision-making",
      "actor": "Society and governments",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prevent humans from being completely outcompeted by AI systems in determining society's trajectory",
      "conditions": "unconditional",
      "rationale_summary": "Human reasoning is currently a powerful force steering our trajectory, but will become weaker compared to new forms of reasoning honed by trial-and-error. Maintaining human competitiveness is essential to maintaining meaningful human agency and preventing manipulation by sophisticated AI systems.",
      "quote": "Right now humans thinking and talking about the future they want to create are a powerful force that is able to steer our trajectory. But over time human reasoning will become weaker and weaker compared to new forms of reasoning honed by trial-and-error."
    },
    {
      "rec_id": "rec_14",
      "action": "Develop better methods to understand and quantify the level of systemic risk from AI deployment",
      "actor": "AI safety researchers and policymakers",
      "target_timeline": "before widespread deployment of advanced AI",
      "urgency": "high",
      "goal": "enable better-informed decisions about AI deployment and justify expensive mitigation strategies",
      "conditions": "unconditional",
      "rationale_summary": "The difficulty of pinning down the level of systemic risk makes it hard to muster an adequate response or justify expensive mitigation measures. Better understanding and quantification of risks is necessary to enable appropriate action before catastrophe occurs.",
      "quote": "There will likely be a general understanding of this dynamic, but it's hard to really pin down the level of systemic risk and mitigation may be expensive if we don't have a good technological solution."
    }
  ]
}