{
  "recommendations": [
    {
      "rec_id": "rec_1",
      "action": "Use natural gas to rapidly build 10GW+ power capacity for AI datacenters",
      "actor": "US Government",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "enable construction of massive AI training clusters in the US rather than in Middle Eastern dictatorships",
      "conditions": "unconditional",
      "rationale_summary": "The US has abundant natural gas that can be brought online quickly. Powering a 10GW cluster would take only a few percent of US natural gas production. The Marcellus/Utica shale alone could support 100GW+ with modest additional drilling. This is the fastest path to solving the power constraint for AGI datacenters.",
      "quote": "But it's totally possible to do this in the United States: we have abundant natural gas... Powering a 10GW cluster would take only a few percent of US natural gas production and could be done rapidly."
    },
    {
      "rec_id": "rec_2",
      "action": "Override climate commitments and green datacenter pledges to enable natural gas use for AI infrastructure",
      "actor": "US Government",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "ensure AGI datacenters are built in the US for national security",
      "conditions": "unconditional",
      "rationale_summary": "Well-intentioned climate commitments by government and tech companies are blocking the fastest solution to powering AGI clusters. Given the national security stakes of AGI, these commitments must be subordinated to ensuring American control of superintelligence infrastructure.",
      "quote": "Well-intentioned but rigid climate commitments (not just by the government, but green datacenter commitments by Microsoft, Google, Amazon, and so on) stand in the way of the obvious, fast solution... We're going to drive the AGI datacenters to the Middle East, under the thumb of brutal, capricious autocrats. I'd prefer clean energy too—but this is simply too important for US national security."
    },
    {
      "rec_id": "rec_3",
      "action": "Implement broad deregulatory agenda including NEPA exemptions, FERC reform, utility regulation overrides, and transmission permitting fixes",
      "actor": "US Government",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "enable rapid buildout of power and datacenter infrastructure for AGI",
      "conditions": "unconditional",
      "rationale_summary": "Current permitting, utility regulation, and environmental review processes make projects that should take years take a decade or more. This timeline is incompatible with AGI development timelines. Even if not using natural gas, deregulation is necessary to unlock alternative energy megaprojects.",
      "quote": "At the very least, even if we won't do natural gas, a broad deregulatory agenda would unlock the solar/batteries/SMR/geothermal megaprojects. Permitting, utility regulation, FERC regulation of transmission lines, and NEPA environmental review makes things that should take a few years take a decade or more. We don't have that kind of time."
    },
    {
      "rec_id": "rec_4",
      "action": "Build AGI training and inference clusters in the United States or close democratic allies, not in Middle Eastern dictatorships",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "maintain US control over AGI infrastructure and prevent physical seizure or influence by autocrats",
      "conditions": "unconditional",
      "rationale_summary": "Clusters built in dictatorships create irreversible security risks: weights could be stolen via side-channel attacks with physical access, dictators could physically seize datacenters when AGI race intensifies, or implicitly wield influence. This would put AGI at the whims of brutal autocrats rather than US control.",
      "quote": "The clusters that are being planned today may well be the clusters AGI and superintelligence are trained and run on... The national interest demands that these are built in America (or close democratic allies). Anything else creates an irreversible security risk: it risks the AGI weights getting stolen... it risks these dictatorships physically seizing the datacenters... or even if these threats are only wielded implicity, it puts AGI and superintelligence at unsavory dictator's whims."
    },
    {
      "rec_id": "rec_5",
      "action": "Rapidly and radically improve security at AI labs to prevent theft of algorithmic secrets",
      "actor": "AI labs",
      "target_timeline": "within 12-24 months",
      "urgency": "critical",
      "goal": "prevent China from stealing key AGI algorithmic breakthroughs worth 10x-100x compute advantage",
      "conditions": "unconditional",
      "rationale_summary": "AI labs are developing the key paradigm breakthroughs for AGI right now (to overcome the data wall). These algorithmic secrets are worth more than a 10x or 100x larger cluster to adversaries. Without drastically better security in the next 12-24 months, the US will irreversibly leak these secrets to China, surrendering its algorithmic advantage.",
      "quote": "Most of all, we must rapidly and radically lock down the AI labs, before we leak key AGI breakthroughs in the next 12-24 months (or the AGI weights themselves)... The AI labs are developing the algorithmic secrets—the key technical breakthroughs, the blueprints so to speak—for the AGI right now... AGI-level security for algorithmic secrets is necessary years before AGI-level security for weights."
    },
    {
      "rec_id": "rec_6",
      "action": "Launch crash program now to develop infrastructure for state-actor-proof weight security",
      "actor": "AI labs",
      "target_timeline": "starting now",
      "urgency": "critical",
      "goal": "be ready to secure AGI model weights when AGI arrives in ~3-4 years",
      "conditions": "IF AGI by 2027",
      "rationale_summary": "Developing the infrastructure for weight security takes many years of lead time - it requires innovations in hardware, radically different cluster design, and cycles of iteration. If we wait until we're on the cusp of AGI, we'll face an impossible choice between pressing ahead and delivering superintelligence to the CCP, or waiting years while losing our lead.",
      "quote": "Critically, developing the infrastructure for weight security probably takes many years of lead times—if we think AGI in ~3-4 years is a real possibility and we need state-proof weight security then, we need to be launching the crash effort now."
    },
    {
      "rec_id": "rec_7",
      "action": "Build fully airgapped datacenters with physical security on par with most secure military bases for AGI training and inference",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "prevent exfiltration of AGI weights by adversaries and prevent model self-exfiltration",
      "conditions": "unconditional",
      "rationale_summary": "Airgapping is the first line of defense against superintelligence attempting to self-exfiltrate. It's also necessary to defend against state-actor theft of weights. This requires cleared personnel, physical fortifications, onsite response teams, extensive surveillance and extreme access control - comparable to nuclear weapons facilities.",
      "quote": "Fully airgapped datacenters, with physical security on par with most secure military bases (cleared personnel, physical fortifications, onsite response team, extensive surveillance and extreme access control)"
    },
    {
      "rec_id": "rec_8",
      "action": "Apply same security measures to inference clusters as training clusters",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent exfiltration of AGI weights from inference deployments",
      "conditions": "unconditional",
      "rationale_summary": "Inference fleets will be much larger than training clusters, and there will be overwhelming pressure to run automated AI researchers on inference clusters during the intelligence explosion. The AGI/superintelligence weights could thus be exfiltrated from these clusters as well, but inference security is often overlooked.",
      "quote": "And not just for training clusters—inference clusters need the same intense security!"
    },
    {
      "rec_id": "rec_9",
      "action": "Develop novel technical advances in confidential compute and hardware encryption for AI systems",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "provide additional layer of protection for model weights beyond physical security",
      "conditions": "unconditional",
      "rationale_summary": "Hardware encryption provides defense-in-depth against weight theft. While hardware encryption can be side-channeled, it's an important additional layer. This requires novel technical work given the scale and nature of AI systems.",
      "quote": "Novel technical advances on confidential compute / hardware encryption and extreme scrutiny on the entire hardware supply chain"
    },
    {
      "rec_id": "rec_10",
      "action": "Require all core AGI research personnel to work from SCIFs (Sensitive Compartmented Information Facilities)",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent leakage of algorithmic secrets and enable compartmentalization",
      "conditions": "unconditional",
      "rationale_summary": "Working from SCIFs is standard practice for protecting national defense secrets. AGI algorithmic breakthroughs are as important as any defense secret. SCIFs enable proper compartmentalization, monitoring, and protection from surveillance.",
      "quote": "All research personnel working from a SCIF (Sensitive Compartmented Information Facility, pronounced 'skiff')"
    },
    {
      "rec_id": "rec_11",
      "action": "Implement extreme personnel vetting, security clearances, regular integrity testing, constant monitoring, and rigid information siloing for AGI researchers",
      "actor": "AI labs",
      "target_timeline": "within 12-24 months",
      "urgency": "critical",
      "goal": "prevent insider threats and espionage",
      "conditions": "unconditional",
      "rationale_summary": "Currently AI labs do basically no background checking and thousands have access to key secrets. State actors can easily recruit insiders with offers of $100M+. Proper vetting, clearances, integrity testing, monitoring and siloing are necessary to counter the insider threat, especially as foreign espionage intensifies.",
      "quote": "Extreme personnel vetting and security clearances (including regular employee integrity testing and the like), constant monitoring and substantially reduced freedoms to leave, and rigid information siloing."
    },
    {
      "rec_id": "rec_12",
      "action": "Require multi-key signoff to run any code or training run on AGI systems",
      "actor": "AI labs",
      "target_timeline": "within 12-24 months",
      "urgency": "high",
      "goal": "prevent rogue employees from stealing weights or running unauthorized experiments",
      "conditions": "unconditional",
      "rationale_summary": "Currently AI labs lack internal controls - random employees with zero vetting could go rogue unnoticed. Multi-key signoff ensures no single individual can unilaterally execute potentially dangerous actions like starting a training run or exfiltrating weights.",
      "quote": "Strong internal controls, e.g. multi-key signoff to run any code."
    },
    {
      "rec_id": "rec_13",
      "action": "Implement strict limitations on all external dependencies and satisfy requirements of TS/SCI networks",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "reduce attack surface and prevent supply chain compromises",
      "conditions": "unconditional",
      "rationale_summary": "External dependencies are a major attack vector - state actors have successfully compromised supply chains at scale. AI labs need to minimize external dependencies and meet the same standards as classified government networks to reduce vulnerability.",
      "quote": "Strict limitations on any external dependencies, and satisfying general requirements of TS/SCI networks."
    },
    {
      "rec_id": "rec_14",
      "action": "Conduct ongoing intense penetration testing by NSA or equivalent organizations",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "identify and remediate vulnerabilities before adversaries exploit them",
      "conditions": "unconditional",
      "rationale_summary": "AI labs lack the expertise to identify state-actor-level attacks. Regular penetration testing by organizations like NSA that understand the threat is necessary to find vulnerabilities before China's Ministry of State Security does.",
      "quote": "Ongoing intense pen-testing by the NSA or similar."
    },
    {
      "rec_id": "rec_15",
      "action": "Immediately adopt best security practices from secretive hedge funds and tech companies handling sensitive data",
      "actor": "AI labs",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "defend against regular economic espionage while preparing for state-actor threats",
      "conditions": "unconditional",
      "rationale_summary": "There is massive low-hanging fruit on security. Simply adopting best practices from secretive hedge funds or Google-customer-data-level security would dramatically improve defenses against 'regular' economic espionage. This is well within AI labs' abilities and should be done immediately.",
      "quote": "There's a lot of low-hanging fruit on security at AI labs. Merely adopting best practices from, say, secretive hedge funds or Google-customer-data-level security, would put us in a much better position with respect to 'regular' economic espionage from the CCP."
    },
    {
      "rec_id": "rec_16",
      "action": "Implement security measures iteratively starting now, rather than waiting until the cusp of AGI",
      "actor": "AI labs",
      "target_timeline": "starting now",
      "urgency": "high",
      "goal": "minimize disruption to research productivity and be prepared when needed",
      "conditions": "unconditional",
      "rationale_summary": "Eventually, extreme security will be inevitable as we approach superintelligence. Implementing it from a standing start will cause massive slowdown and friction. Iteratively ramping security now will be less painful long-term and ensure we're prepared when the stakes become existential.",
      "quote": "Moreover, ramping security now will be the less painful path in terms of research productivity in the long run... It will be so much more painful, and cause much more of a slowdown, to have to implement extreme, state-actor-proof security measures from a standing start, rather than iteratively."
    },
    {
      "rec_id": "rec_17",
      "action": "Prioritize national security over commercial interests and competitive dynamics in AI development",
      "actor": "AI labs",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "preserve American algorithmic lead and prevent proliferation of superintelligence",
      "conditions": "unconditional",
      "rationale_summary": "This is a tragedy of the commons problem. Individual labs may be hurt by security measures that slow them 10%, but the national interest is clearly better served if every lab accepts this friction to preserve America's aggregate lead. Failure means surrendering our entire advantage to adversaries.",
      "quote": "This is a tragedy of the commons problem. For a given lab's commercial interests, security measures that cause a 10% slowdown might be deleterious in competition with other labs. But the national interest is clearly better served if every lab were willing to accept the additional friction: American AI research is way ahead of Chinese and other foreign algorithmic progress, and America retaining 90%-speed algorithmic progress as our national edge is clearly better than retaining 0% as a national edge."
    },
    {
      "rec_id": "rec_18",
      "action": "Develop successor techniques to RLHF that can align superhuman AI systems",
      "actor": "AI safety researchers",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "ensure we can reliably control and trust AI systems smarter than humans",
      "conditions": "unconditional",
      "rationale_summary": "RLHF relies on humans being able to supervise AI behavior, which fundamentally won't scale to superhuman systems. Without a successor to RLHF, we won't be able to ensure even basic side constraints like 'don't lie' or 'follow the law' for superintelligent systems, which could lead to catastrophic outcomes.",
      "quote": "RLHF will predictably break down as AI systems get smarter, and we will face fundamentally new and qualitatively different technical challenges... It's clear we will need a successor to RLHF that scales to AI capabilities better than human-level"
    },
    {
      "rec_id": "rec_19",
      "action": "Invest heavily in scalable oversight research including debate, market-making, recursive reward modeling, and prover-verifier games",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "extend human supervision to somewhat superhuman systems",
      "conditions": "unconditional",
      "rationale_summary": "Scalable oversight techniques use AI assistants to help humans supervise other AI systems, extending supervision farther than humans could alone. Models are now strong enough to empirically test these ideas. This will help with the quantitatively superhuman part of the problem.",
      "quote": "Several scalable oversight strategies have been proposed, including debate, market-making, recursive reward modeling, and prover-verifier games, as well as simplified versions of those ideas like critiques. Models are now strong enough that it's possible to empirically test these ideas, making direct progress on scalable oversight."
    },
    {
      "rec_id": "rec_20",
      "action": "Study how AI systems generalize from supervision on easy problems to hard problems beyond human comprehension",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "enable alignment of qualitatively superhuman systems where we can't provide direct supervision",
      "conditions": "unconditional",
      "rationale_summary": "We can only supervise AI on problems we understand. The key question is whether supervision on easy cases generalizes benignly to hard cases. Deep learning often generalizes in favorable ways. Developing methods to nudge generalization in our favor and science to predict when it works could solve alignment even for qualitatively superhuman systems.",
      "quote": "There's a lot of reasons to be optimistic here: part of the magic of deep learning is that it often generalizes in benign ways... I'm fairly optimistic that there will both be pretty simple methods that help nudge the models' generalization in our favor, and that we can develop a strong scientific understanding that helps us predict when generalization will work and when it will fail."
    },
    {
      "rec_id": "rec_21",
      "action": "Develop interpretability techniques including mechanistic interpretability, top-down approaches, and chain-of-thought interpretability",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "understand what AI systems are thinking to verify alignment and detect deception",
      "conditions": "unconditional",
      "rationale_summary": "If we could understand AI systems' internal reasoning, we could verify they're aligned and detect if they're deceiving us. Multiple approaches from ambitious mechanistic interpretability to hackier top-down techniques show promise. Even modest progress here could be transformative for alignment.",
      "quote": "One intuitively-attractive way we'd hope to verify and trust that our AI systems are aligned is if we could understand what they're thinking! For example, if we're worried that AI systems are deceiving us or conspiring against us, access to their internal reasoning should help us detect that."
    },
    {
      "rec_id": "rec_22",
      "action": "Ensure chain-of-thought reasoning remains legible and faithful as models are trained",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "preserve ability to monitor models' reasoning and detect malign behavior",
      "conditions": "IF models continue to use chain-of-thought",
      "rationale_summary": "We may bootstrap to AGI with systems that 'think out loud' via chains of thought. This is extraordinarily helpful for interpretability. But CoT may drift to unintelligible model-speak or become unfaithful depending on how we train. Simple constraints and measurements could preserve this crucial safety property.",
      "quote": "How do we ensure that the CoT remains legible? (It may simply drift from understandable English to unintelligible model-speak, depending on how we e.g. use RL to train models—can we add some simple constraints to ensure it remains legible?) How do we ensure the CoT is faithful, i.e. actually reflects what models are thinking?"
    },
    {
      "rec_id": "rec_23",
      "action": "Advance adversarial testing and automated red-teaming to stress test alignment at every step",
      "actor": "AI safety researchers",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "encounter every failure mode in the lab before encountering it in the wild",
      "conditions": "unconditional",
      "rationale_summary": "We need to proactively find alignment failures before they happen in deployment. This requires substantially advancing automated red-teaming techniques. For example, deliberately planting backdoors to see if safety training catches them. Our goal should be zero surprises in production.",
      "quote": "Along the way, it's going to be critical to stress test the alignment of our systems at every step—our goal should be to encounter every failure mode in the lab before we encounter it in the wild. This will require substantially advancing techniques for automated red-teaming."
    },
    {
      "rec_id": "rec_24",
      "action": "Develop better measurements and metrics for alignment",
      "actor": "AI safety researchers",
      "target_timeline": "before intelligence explosion",
      "urgency": "critical",
      "goal": "enable informed decision-making during intelligence explosion about whether next OOM is safe",
      "conditions": "unconditional",
      "rationale_summary": "The science of measuring alignment is in its infancy. Without reliable metrics, we won't know during the intelligence explosion whether pressing on is safe or not. Developing measurements for whether models have power to be misaligned, what drives they're learning, and clear red lines is among the highest priority work.",
      "quote": "The science of measuring alignment is still in its infancy; improving this will be critical for helping us make the right tradeoffs on risk during the intelligence explosion. Doing the science that lets us measure alignment and gives us an understanding of 'what evidence would be sufficient to assure us that the next OOM into superhuman territory is safe?' is among the very-highest priority work for alignment research today"
    },
    {
      "rec_id": "rec_25",
      "action": "Automate alignment research using somewhat-superhuman AI systems once they can be trusted",
      "actor": "AI labs",
      "target_timeline": "during intelligence explosion",
      "urgency": "critical",
      "goal": "solve alignment for vastly superhuman systems during intelligence explosion",
      "conditions": "IF somewhat-superhuman systems can be aligned and trusted",
      "rationale_summary": "We won't be able to solve alignment for true superintelligence directly - the intelligence gap is too vast. But if we can align somewhat-superhuman systems enough to trust them, we'll have millions of automated AI researchers smarter than the best humans to help solve alignment for even-more superhuman systems.",
      "quote": "Ultimately, we're going to need to automate alignment research... If we manage to align somewhat-superhuman systems enough to trust them, we'll be in an incredible position: we'll have millions of automated AI researchers, smarter than the best AI researchers, at our disposal."
    },
    {
      "rec_id": "rec_26",
      "action": "Dedicate large fraction of compute to automated alignment research versus capabilities research during intelligence explosion if necessary",
      "actor": "AI labs",
      "target_timeline": "during intelligence explosion",
      "urgency": "critical",
      "goal": "ensure alignment keeps up with capabilities during rapid intelligence explosion",
      "conditions": "IF needed for safety",
      "rationale_summary": "Automated AI research will accelerate both capabilities and alignment. But there's no guarantee alignment keeps up - it may be harder, have less clear metrics, or face pressure to prioritize capabilities. Labs must be willing to dedicate majority of compute to alignment if that's what safety requires.",
      "quote": "Labs should be willing to commit a large fraction of their compute to automated alignment research (vs. automated capabilities research) during the intelligence explosion, if necessary."
    },
    {
      "rec_id": "rec_27",
      "action": "Require extremely high confidence in alignment approaches before each OOM advance during intelligence explosion",
      "actor": "AI labs",
      "target_timeline": "during intelligence explosion",
      "urgency": "critical",
      "goal": "avoid catastrophic alignment failure as systems become vastly superhuman",
      "conditions": "unconditional",
      "rationale_summary": "During the intelligence explosion, alignment failures transition from low-stakes to potentially catastrophic. We'll need strong guarantees that let us trust automated alignment research and much better measurements than today. For every OOM we ascend, we need extremely high confidence before proceeding.",
      "quote": "We'll need strong guarantees that let us trust the automated alignment research being produced and much better measurements for misalignment than we have today to know whether we're still in the clear. For every OOM we want to ascend, we will need to have extremely high confidence in our alignment approaches for that next OOM."
    },
    {
      "rec_id": "rec_28",
      "action": "Take decisions on superintelligence as seriously as decisions to launch military operations",
      "actor": "AI labs",
      "target_timeline": "approaching superintelligence",
      "urgency": "critical",
      "goal": "ensure appropriate level of care and deliberation for extraordinarily high-stakes decisions",
      "conditions": "unconditional",
      "rationale_summary": "Once approaching true superintelligence, the stakes become very real - failure could be catastrophic. There will be moments with ambiguous evidence and warning signs where we mostly don't understand what's going on. Making the right costly tradeoffs will require treating these decisions with the gravity of military operations.",
      "quote": "Most of all, getting this right will require an extreme degree of competence, seriousness, and willingness to make hard tradeoffs. Once we're approaching true superintelligence, the stakes get very real, and failure—rogue AI—could be catastrophic... Eventually, the decision to greenlight the next generation of superintelligence will need to be taken as seriously as the decision to launch a military operation."
    },
    {
      "rec_id": "rec_29",
      "action": "Build AGI training and inference clusters as airgapped facilities from the start",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "create first line of defense against superintelligence self-exfiltration and external attacks",
      "conditions": "unconditional",
      "rationale_summary": "Airgapping is the first layer of defense against superintelligence attempting to escape and cause damage. If alignment fails, an airgapped cluster prevents the model from immediately accessing the outside world. It's also critical defense against adversaries attempting to steal weights.",
      "quote": "An airgapped cluster is the first layer of defense against superintelligence attempting to self-exfiltrate and doing damage in the real world."
    },
    {
      "rec_id": "rec_30",
      "action": "Develop extreme security measures against model self-exfiltration including hardware encryption and many-key signoff",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "prevent AI systems from stealing their own weights and escaping containment",
      "conditions": "unconditional",
      "rationale_summary": "Model self-exfiltration is a critical threat - AI systems stealing their own weights to make copies outside containment. This requires extreme security beyond airgapping: hardware encryption to protect weights even with physical access, and many-key signoff to prevent humans fooled by the AI from helping it escape.",
      "quote": "we'll need much more extreme security against model self-exfiltration across the board, from hardware encryption to many-key signoff."
    },
    {
      "rec_id": "rec_31",
      "action": "Implement advanced monitoring systems that leverage AI to monitor other AI instances across all activities",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "detect and prevent malicious activity by AI systems or rogue employees",
      "conditions": "unconditional",
      "rationale_summary": "With millions of AGI instances running complex code, we need to detect if they're doing something malicious or if rogue employees are using them for unauthorized activities. Advanced monitoring leveraging models themselves - monitoring outputs, chains of thought, code, network activity, interactions - is necessary.",
      "quote": "We will need advanced monitoring systems that leverage the models themselves, including monitoring model outputs, chains of thought, model-written code, server/network activity, and interactions with researchers/other humans; essentially, leveraging some AI systems to nark on other AI systems."
    },
    {
      "rec_id": "rec_32",
      "action": "Employ control protocols using less powerful trusted models to protect against more powerful untrusted models",
      "actor": "AI labs",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent subversion by powerful but less-trusted AI systems",
      "conditions": "unconditional",
      "rationale_summary": "We may not fully trust the most powerful models, but can trust less powerful models more. Using trusted weaker models to monitor and control untrusted stronger models provides defense-in-depth. This is sophisticated 'control' protocols to prevent the powerful model from subverting safety measures.",
      "quote": "we'll want to develop and employ sophisticated 'control' protocols, using less powerful but trusted models to protect against subversion from more powerful but less trusted models."
    },
    {
      "rec_id": "rec_33",
      "action": "Dedicate equal amounts of compute to monitoring AI instances as to running active AI instances",
      "actor": "AI labs",
      "target_timeline": "during intelligence explosion",
      "urgency": "high",
      "goal": "ensure adequate monitoring capacity to detect malicious behavior",
      "conditions": "IF necessary for adequate monitoring",
      "rationale_summary": "Effective monitoring of millions of superintelligent AGIs may require substantial compute - potentially matching the compute used to run them. This is an enormous cost but necessary for safety. Labs must be willing to make this tradeoff if that's what proper monitoring requires.",
      "quote": "This may require, for example, dedicating the same amount of compute to monitoring model instances as to compute for active model instances."
    },
    {
      "rec_id": "rec_34",
      "action": "Scrub biology and chemistry knowledge from model training data to limit bioweapon capabilities",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "reduce potential for catastrophic real-world damage via biological or chemical weapons",
      "conditions": "unconditional",
      "rationale_summary": "Targeted capability limitations can reduce risks from alignment failures. Removing biology/chemistry from training (or using unlearning) cuts off paths to catastrophic damage via bio/chem weapons. This may have costs but reduces the severity of potential worst-case outcomes.",
      "quote": "As much as possible, we should try to limit the model's capabilities in targeted ways that reduce fallout from failure. A central example of this might be scrubbing everything related to biology and chemistry from model training (or using 'unlearning' techniques)"
    },
    {
      "rec_id": "rec_35",
      "action": "Avoid long-horizon outcome-based reinforcement learning training as long as possible",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "minimize risk of models learning dangerous long-term internal goals",
      "conditions": "ONLY drop restriction if absolutely necessary",
      "rationale_summary": "Different training methods have different risk profiles. Imitation learning seems relatively safe, while long-horizon outcome-based RL is much more likely to lead to models learning undesirable long-term goals (like power-seeking). We should define these constraints ahead of time and only relax them if absolutely necessary.",
      "quote": "There are likely some ways of training models that are inherently riskier—more likely to produce severe misalignments—than others. For example, imitation learning seems relatively safe... while we should avoid long-horizon outcome-based RL (which seems much more likely to lead to the model learning undesirable long-term goals) as long as possible."
    },
    {
      "rec_id": "rec_36",
      "action": "Avoid training models against interpretability methods and monitoring systems",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "preserve ability to understand and monitor AI systems",
      "conditions": "unconditional",
      "rationale_summary": "If we train models with our interpretability methods and monitoring in the loop, we may train them to evade these safeguards rather than actually be safe. This would undermine our ability to verify alignment. Training should be done in ways that don't incentivize working around safety measures.",
      "quote": "We should also avoid training against our interpretability methods and our monitoring setup (such that our models aren't simply trained to work around these)."
    },
    {
      "rec_id": "rec_37",
      "action": "Define safety constraints and red lines ahead of time before intelligence explosion begins",
      "actor": "AI labs",
      "target_timeline": "before intelligence explosion",
      "urgency": "high",
      "goal": "ensure clear guidelines for when to halt or proceed during rapid capability increases",
      "conditions": "unconditional",
      "rationale_summary": "During the intelligence explosion, decisions will need to be made rapidly under pressure. Having pre-committed to specific constraints and red lines (like 'CoT must remain legible', or 'no long-horizon RL') provides clear guidance and prevents ad-hoc rationalizations to proceed unsafely.",
      "quote": "We should define these constraints ahead of time, maintain them as long as we can throughout the intelligence explosion, and only drop them if absolutely necessary."
    },
    {
      "rec_id": "rec_38",
      "action": "Relax superdefense measures only concomitant with confidence in alignment",
      "actor": "AI labs",
      "target_timeline": "post-intelligence explosion",
      "urgency": "high",
      "goal": "maintain safety margin as systems are deployed more broadly",
      "conditions": "unconditional",
      "rationale_summary": "Superdefense measures like airgapping buy margin for error. We should only relax them (e.g., deploying in non-airgapped environments) as our confidence in alignment increases. This ensures we don't prematurely expose powerful systems before we're confident they're safe.",
      "quote": "We'll want to use that margin to get in a position where we have very high confidence in our alignment techniques, only relaxing 'superdefense' measures (for example, deploying the superintelligence in non-airgapped environments) concomitant with our confidence."
    },
    {
      "rec_id": "rec_39",
      "action": "Use superintelligence for R&D in isolated environments before field deployment for military purposes",
      "actor": "US Government",
      "target_timeline": "post-superintelligence",
      "urgency": "high",
      "goal": "buy margin for error by limiting deployment until confident in safety",
      "conditions": "IF possible given strategic situation",
      "rationale_summary": "Directly deploying superintelligences in military field operations is high-risk. Using them for R&D in more isolated environments, then deploying only the specific technologies they invent, provides more control and confidence. This buys as much margin as possible given strategic pressures.",
      "quote": "Things will get dicey again once we move to deploying these AI systems in less-controlled settings, for example in military applications... we should always try to buy as much margin for error as much as possible—for example, rather than just directly deploying the superintelligences 'in the field' for military purposes, using them to do R&D in a more isolated environment, and only deploying the specific technologies they invent"
    },
    {
      "rec_id": "rec_40",
      "action": "Deploy automated AI researchers to work on security, biodefense, and other safety challenges beyond alignment",
      "actor": "AI labs",
      "target_timeline": "during and after intelligence explosion",
      "urgency": "high",
      "goal": "address full spectrum of AI risks including misuse and security threats",
      "conditions": "unconditional",
      "rationale_summary": "Alignment isn't the only AI safety challenge. The best route to addressing misuse risks, security threats, bioweapons, and unknown unknowns is leveraging early AGIs for safety research in these domains. Put automated researchers to work on improving weight security, biodefense, etc.",
      "quote": "This applies more generally, by the way, for the full spectrum of AI risks, including misuse and so on. The best route—perhaps the only route—to AI safety in all of these cases, will involve properly leveraging early AGIs for safety; for example, we should put a bunch of them to work on automated research to improve security against foreign actors exfiltrating weights, others on shoring up defenses against worst-case bioattacks, and so on."
    },
    {
      "rec_id": "rec_41",
      "action": "United States must maintain decisive lead over China and authoritarian powers in AGI development",
      "actor": "US Government",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "ensure free world prevails and prevent authoritarian control of superintelligence",
      "conditions": "unconditional",
      "rationale_summary": "Superintelligence will give decisive military and economic advantage. If the CCP gets superintelligence first, they could enforce their authoritarian model globally and permanently lock in their power using AI-controlled police and military. The free world's survival depends on democratic allies maintaining the lead.",
      "quote": "The free world must prevail over the authoritarian powers in this race. We owe our peace and freedom to American economic and military preeminence... At stake in the AGI race will not just be the advantage in some far-flung proxy war, but whether freedom and democracy can survive for the next century and beyond."
    },
    {
      "rec_id": "rec_42",
      "action": "US must maintain 2+ year lead over adversaries to have adequate safety margin",
      "actor": "US Government",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "provide margin to navigate intelligence explosion safely and stabilize post-superintelligence situation",
      "conditions": "unconditional",
      "rationale_summary": "A 2-year versus 2-month lead makes all the difference. With 2 years, we can slow down during the intelligence explosion to solve safety challenges, use superintelligence defensively first, and stabilize deterrence. With 2 months, we're forced into a breakneck arms race through the intelligence explosion with maximum risk of self-destruction.",
      "quote": "Perhaps most importantly, a healthy lead gives us room to maneuver: the ability to 'cash in' parts of the lead, if necessary, to get safety right, for example by devoting extra work to alignment during the intelligence explosion... The safety challenges of superintelligence would become extremely difficult to manage if you are in a neck-and-neck arms race. A 2 year vs. a 2 month lead could easily make all the difference."
    },
    {
      "rec_id": "rec_43",
      "action": "American AI labs must work directly with US intelligence community and military",
      "actor": "AI labs",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "enable AGI to contribute to American defense and national security",
      "conditions": "unconditional",
      "rationale_summary": "America's lead on AGI won't secure peace and freedom by just building consumer apps. AI labs have a duty to work with intelligence and military to ensure superintelligence contributes to US defense. This is not pretty but necessary given the stakes.",
      "quote": "And yes, American AI labs have a duty to work with the intelligence community and the military. America's lead on AGI won't secure peace and freedom by just building the best AI girlfriend apps. It's not pretty—but we must build AI for American defense."
    },
    {
      "rec_id": "rec_44",
      "action": "Build AGI datacenters in US rather than onshoring chip fabrication",
      "actor": "US Government",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "prioritize having AGI itself in US over where chips are made",
      "conditions": "IF tradeoffs are necessary",
      "rationale_summary": "Having chip production abroad is like having uranium deposits abroad, but having the AGI datacenter abroad is like having the literal nukes abroad. Given dysfunction and cost of US fab buildouts, prioritize datacenters in US while relying on democratic allies like Japan and South Korea for fabs.",
      "quote": "While onshoring more of AI chip production to the US would be nice, it's less critical than having the actual datacenter (on which the AGI lives) in the US. If having chip production abroad is like having uranium deposits abroad, having the AGI datacenter abroad is like having the literal nukes be built and stored abroad."
    },
    {
      "rec_id": "rec_45",
      "action": "Rely on democratic allies like Japan and South Korea for chip fabrication buildout",
      "actor": "US Government",
      "target_timeline": "ongoing",
      "urgency": "medium",
      "goal": "ensure chip supply while avoiding dysfunctional US fab projects",
      "conditions": "unconditional",
      "rationale_summary": "US fab buildouts have been dysfunctional and costly in practice. Democratic allies like Japan and South Korea have much more functional chip production ecosystems. Better to rely on them for fabs while ensuring AGI datacenters are in US territory.",
      "quote": "Given the dysfunction and cost we've seen from building fabs in the US in practice, my guess is we should prioritize datacenters in the US while betting more heavily on democratic allies like Japan and South Korea for fab projects—fab buildouts there seem much more functional."
    },
    {
      "rec_id": "rec_46",
      "action": "Avoid building critical AGI infrastructure in Middle Eastern dictatorships despite financial incentives",
      "actor": "AI labs",
      "target_timeline": "ongoing",
      "urgency": "critical",
      "goal": "prevent AGI from being under control or influence of autocratic regimes",
      "conditions": "unconditional",
      "rationale_summary": "Middle Eastern autocrats are offering boundless power and funding to host AGI clusters. But building there creates irreversible risks: they could physically seize datacenters when the AGI race intensifies, weights are more easily stolen with physical access, and it puts AGI at their whims. American security must come first.",
      "quote": "Do we really want the infrastructure for the Manhattan Project to be controlled by some capricious Middle Eastern dictatorship?... America sorely regretted her energy dependence on the Middle East in the 70s, and we worked so hard to get out from under their thumbs. We cannot make the same mistake again."
    },
    {
      "rec_id": "rec_47",
      "action": "Be willing to spend parts of lead on safety if necessary during intelligence explosion",
      "actor": "AI labs",
      "target_timeline": "during intelligence explosion",
      "urgency": "critical",
      "goal": "ensure adequate time and resources for solving safety challenges",
      "conditions": "IF necessary for safety AND lead is sufficient",
      "rationale_summary": "A healthy lead enables cashing in parts of it for safety. For example, taking an extra 6 months during the intelligence explosion for alignment research if needed. This margin is why maintaining a lead over adversaries is so important - it gives us room to not cut corners on safety.",
      "quote": "a healthy lead gives us room to maneuver: the ability to 'cash in' parts of the lead, if necessary, to get safety right, for example by devoting extra work to alignment during the intelligence explosion."
    },
    {
      "rec_id": "rec_48",
      "action": "Offer deal to China and adversaries once US decisive lead is clear, in exchange for nonproliferation regime",
      "actor": "US Government",
      "target_timeline": "once US lead is decisive",
      "urgency": "high",
      "goal": "stabilize post-superintelligence world and prevent dangerous proliferation",
      "conditions": "IF US has decisive lead",
      "rationale_summary": "If and when it becomes clear the US will decisively win, that's when to offer a deal. Adversaries will know they can't win, so they'll come to the table. In exchange for noninterference and sharing peaceful benefits, we get a nonproliferation regime and safety norms, avoiding a desperate standoff.",
      "quote": "If and when it becomes clear that the US will decisively win, that's when we offer a deal to China and other adversaries. They'll know they won't win, and so they'll know their only option is to come to the table; and we'd rather avoid a feverish standoff or last-ditch military attempts on their part to sabotage Western efforts."
    },
    {
      "rec_id": "rec_49",
      "action": "Establish government AGI project bringing together labs, cloud providers, and national security apparatus",
      "actor": "US Government",
      "target_timeline": "by 2026-2028",
      "urgency": "critical",
      "goal": "ensure proper security, chain of command, and national mobilization for AGI",
      "conditions": "unconditional",
      "rationale_summary": "No startup can handle superintelligence. We need government involvement for security infrastructure (only NSA-level can defend against CCP), proper chain of command (can't have random CEOs with nuclear button), and to mobilize national resources. One way or another, the USG will be at the helm by 27/28.",
      "quote": "As the race to AGI intensifies, the national security state will get involved. The USG will wake from its slumber, and by 27/28 we'll get some form of government AGI project. No startup can handle superintelligence."
    },
    {
      "rec_id": "rec_50",
      "action": "Leading AI labs should voluntarily merge into national AGI effort",
      "actor": "AI labs",
      "target_timeline": "by 2027-2028",
      "urgency": "high",
      "goal": "pool resources and ensure coordinated approach under government oversight",
      "conditions": "unconditional",
      "rationale_summary": "A fragmented effort with competing labs won't work for superintelligence. Labs will 'voluntarily' merge into a joint effort (like they 'voluntarily' made White House commitments in 2023). This pools the best scientists, ensures coordination on safety and security, and establishes proper oversight.",
      "quote": "While there's a lot of flux in the exact mechanics, one way or another, the USG will be at the helm; the leading labs will ('voluntarily') merge"
    },
    {
      "rec_id": "rec_51",
      "action": "Congress should appropriate trillions of dollars for AGI compute and power infrastructure",
      "actor": "US Government",
      "target_timeline": "by 2027",
      "urgency": "high",
      "goal": "enable construction of massive clusters necessary for AGI and superintelligence",
      "conditions": "unconditional",
      "rationale_summary": "The investments required for AGI will be in the trillions. This requires Congressional appropriation given the scale. The industrial mobilization - 100s of billions for clusters, growing US electricity by 10s of percent - is comparable to wartime mobilization and needs government backing.",
      "quote": "Congress will appropriate trillions for chips and power"
    },
    {
      "rec_id": "rec_52",
      "action": "Core AGI research team should relocate to secure facility",
      "actor": "AI labs",
      "target_timeline": "by 2027-2028",
      "urgency": "high",
      "goal": "enable proper security controls for AGI development",
      "conditions": "unconditional",
      "rationale_summary": "The few hundred core researchers developing AGI need to work in a secure location with proper controls. This is necessary for security (SCIFs, airgapping, monitoring), for managing the intense challenges of the intelligence explosion, and for proper chain of command. The endgame will happen in a SCIF.",
      "quote": "The core AGI research team (a few hundred researchers) will move to a secure location; the trillion-dollar cluster will be built in record-speed; The Project will be on."
    },
    {
      "rec_id": "rec_53",
      "action": "Build trillion-dollar AGI cluster in record time",
      "actor": "US Government",
      "target_timeline": "by 2028-2030",
      "urgency": "high",
      "goal": "provide compute necessary for superintelligence",
      "conditions": "unconditional",
      "rationale_summary": "The cluster for superintelligence will cost over a trillion dollars and require 100+ GW of power. This is an unprecedented industrial mobilization comparable to wartime efforts. It must be done in record speed, which requires extraordinary focus and removal of normal barriers.",
      "quote": "the trillion-dollar cluster will be built in record-speed"
    },
    {
      "rec_id": "rec_54",
      "action": "Establish proper chain of command and decision-making authority for superintelligence",
      "actor": "US Government",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "ensure democratic control over superintelligence comparable to nuclear command authority",
      "conditions": "unconditional",
      "rationale_summary": "Superintelligence will be the most powerful weapon ever built. Random CEOs should not have unilateral control over it. We need a chain of command with proper checks and balances, centuries of proven institutions, and all the safeguards that come with controlling WMDs. Only government can provide this.",
      "quote": "We will need a sane chain of command—along with all the other processes and safeguards that necessarily come with responsibly wielding what will be comparable to a WMD—and it'll require the government to do so."
    },
    {
      "rec_id": "rec_55",
      "action": "Subject all AGI project personnel to extreme vetting comparable to nuclear weapons programs",
      "actor": "US Government",
      "target_timeline": "before AGI",
      "urgency": "high",
      "goal": "prevent insider threats and espionage",
      "conditions": "unconditional",
      "rationale_summary": "Private companies can't subject employees to the level of vetting needed. Only government has the authority and infrastructure to do proper security clearances, background checks, integrity testing, and the threat of imprisonment for leaking secrets. This is necessary to defend against state-actor espionage.",
      "quote": "Basic stuff like the authority to subject employees to intense vetting; threaten imprisonment for leaking secrets"
    },
    {
      "rec_id": "rec_56",
      "action": "Implement physical security for AGI datacenters comparable to nuclear weapons facilities",
      "actor": "US Government",
      "target_timeline": "before AGI",
      "urgency": "critical",
      "goal": "defend against physical attacks and prevent adversary sabotage",
      "conditions": "unconditional",
      "rationale_summary": "As AGI race intensifies, adversaries may attempt to sabotage or physically attack AGI datacenters. Protection comparable to nuclear weapons sites is necessary: military-grade security, armed guards, surveillance, hardening against attacks. Only government can provide this level of physical security.",
      "quote": "physical security for datacenters"
    },
    {
      "rec_id": "rec_57",
      "action": "Leverage NSA and security clearance infrastructure for AGI security",
      "actor": "US Government",
      "target_timeline": "immediately",
      "urgency": "high",
      "goal": "access expertise and capabilities necessary for state-actor-proof security",
      "conditions": "unconditional",
      "rationale_summary": "Private companies lack expertise on state-actor attacks. NSA and the security clearance infrastructure have decades of experience defending secrets against state actors. They have the know-how on everything from physical security to supply chain security to counter-intelligence. This expertise is essential.",
      "quote": "and the vast know-how of places like the NSA and the people behind the security clearances"
    },
    {
      "rec_id": "rec_58",
      "action": "Reserve military applications of superintelligence for government control",
      "actor": "US Government",
      "target_timeline": "ongoing",
      "urgency": "high",
      "goal": "ensure superintelligence weapons remain under democratic control",
      "conditions": "unconditional",
      "rationale_summary": "Civilian applications of superintelligence can be private (like civilian nuclear power or Boeing jets), but military applications must be government-controlled (like nuclear weapons or stealth fighters). This follows the pattern of other dual-use technologies and ensures democratic oversight of the most powerful weapon.",
      "quote": "And the military uses of superintelligence will remain reserved for the government, and safety norms will be enforced."
    },
    {
      "rec_id": "rec_59",
      "action": "Deploy superintelligence initially for defensive applications including cybersecurity, missile defense, and biodefense",
      "actor": "US Government",
      "target_timeline": "immediately post-superintelligence",
      "urgency": "critical",
      "goal": "develop countermeasures to survive new threats before they proliferate",
      "conditions": "unconditional",
      "rationale_summary": "Superintelligence will enable novel threats: superhuman hacking, new WMDs, drone swarms. The initial priority must be defensive: develop countermeasures to survive these threats, shore up our defenses, develop protections. This buys time before others develop these capabilities offensively.",
      "quote": "Perhaps most of all, the initial priority will be to deploy superintelligence for defensive applications, to develop countermeasures to survive untold new threats: adversaries with superhuman hacking capabilities, new classes of stealthy drone swarms that could execute a preemptive strike on our nuclear deterrent, the proliferation of advances in synthetic biology that can be weaponized"
    },
    {
      "rec_id": "rec_60",
      "action": "Form tight alliance of democracies modeled on Quebec Agreement pooling resources for AGI",
      "actor": "US Government",
      "target_timeline": "by 2027",
      "urgency": "high",
      "goal": "pool resources and establish mutual commitments among democratic allies",
      "conditions": "unconditional",
      "rationale_summary": "A coalition of democracies will have more resources, talent, and control the full supply chain. The UK (Deepmind), Japan/South Korea (chips), and NATO allies should pool efforts. This enables coordination on safety and security, provides checks and balances, and presents a united front against authoritarian powers.",
      "quote": "The former might look like the Quebec Agreement: a secret pact between Churchill and Roosevelt to pool their resources to develop nuclear weapons, while not using them against each other or against others without mutual consent. We'll want to bring in the UK (Deepmind), East Asian allies like Japan and South Korea (chip supply chain), and NATO/other core democratic allies"
    },
    {
      "rec_id": "rec_61",
      "action": "Offer Atoms for Peace-style arrangement sharing peaceful benefits of superintelligence globally",
      "actor": "US Government",
      "target_timeline": "once US lead is decisive",
      "urgency": "high",
      "goal": "reduce incentives for arms races and establish global buy-in for nonproliferation regime",
      "conditions": "unconditional",
      "rationale_summary": "Following the nuclear playbook: offer to share peaceful benefits of superintelligence with broader group of countries (including non-democracies), commit to not offensively using superintelligence against them. This brings more countries under a US-led umbrella and reduces proliferation incentives.",
      "quote": "The latter might look like Atoms for Peace, the IAEA, and the NPT. We should offer to share the peaceful benefits of superintelligence with a broader group of countries (including non-democracies), and commit to not offensively using superintelligence against them."
    },
    {
      "rec_id": "rec_62",
      "action": "Establish and enforce international nonproliferation regime for superintelligence with inspection and restrictions",
      "actor": "US Government",
      "target_timeline": "post-superintelligence",
      "urgency": "critical",
      "goal": "prevent proliferation of superintelligence to rogue states and enforce safety norms globally",
      "conditions": "IF US has decisive lead",
      "rationale_summary": "With a decisive lead, US can enforce a nonproliferation regime: other countries refrain from superintelligence projects, make safety commitments, accept restrictions on dual-use applications. This is necessary to prevent Russia, North Korea, Iran, terrorists from developing super-WMDs. US military power underwrites enforcement.",
      "quote": "In exchange, they refrain from pursuing their own superintelligence projects, make safety commitments on the deployment of AI systems, and accept restrictions on dual-use applications. The hope is that this offer reduces the incentives for arms races and proliferation"
    },
    {
      "rec_id": "rec_63",
      "action": "Trigger government involvement in AGI earlier rather than later",
      "actor": "US Government",
      "target_timeline": "by 2025-2026",
      "urgency": "high",
      "goal": "provide adequate time to prepare security, organization, and coordination before intelligence explosion",
      "conditions": "unconditional",
      "rationale_summary": "Government involvement is inevitable, but timing matters. If government only steps in at the last minute, secrets will have been stolen, officials won't be prepared, and there won't be a functioning merged organization. Earlier involvement (by 2025-2026 rather than 2027-2028) provides crucial years to prepare properly.",
      "quote": "One important free variable is not if but when... If the government project is inevitable, earlier seems better. We'll dearly need those couple years to do the security crash program, to get the key officials up to speed and prepared, to build a functioning merged lab, and so on."
    },
    {
      "rec_id": "rec_64",
      "action": "Develop detailed plans for how to organize and operate The Project effectively",
      "actor": "US Government",
      "target_timeline": "immediately",
      "urgency": "critical",
      "goal": "ensure government AGI project is competent and well-organized when launched",
      "conditions": "unconditional",
      "rationale_summary": "Almost no attention has gone into figuring out how The Project should work: how it will be organized, how to make it effective, what checks and balances, what chain of command. This is the most important question and we need to work it out ahead of time. This is the ballgame - nothing else matters if we get this wrong.",
      "quote": "How will it be organized? How can we get this done? How will the checks and balances work, and what does a sane chain of command look like? Scarcely any attention has gone into figuring this out... Almost all other AI lab and AI governance politicking is a sideshow. This is the ballgame."
    },
    {
      "rec_id": "rec_65",
      "action": "Require Senate confirmation for key officials running The Project",
      "actor": "US Government",
      "target_timeline": "when establishing The Project",
      "urgency": "medium",
      "goal": "ensure proper democratic oversight and accountability",
      "conditions": "unconditional",
      "rationale_summary": "The Manhattan Project was run in complete secrecy - even Congress and the Vice President didn't know. We shouldn't repeat that mistake. Key officials running The Project should require Senate confirmation to ensure proper checks and balances on this enormous power.",
      "quote": "Congress—even the Vice President!—didn't know about the Manhattan Project. We probably shouldn't repeat that here; I'd even suggest that key officials for The Project require Senate confirmation."
    }
  ]
}